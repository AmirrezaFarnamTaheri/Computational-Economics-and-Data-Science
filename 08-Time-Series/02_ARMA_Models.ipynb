{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import arma_order_select_ic\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*\"=\"}')\n",
    "\n",
    "note(\"Environment initialized for ARMA Models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8.2: Autoregressive and Moving Average (ARMA) Models\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Introduction: Modeling Stationary Time Series**](#intro)\n",
    "2.  [**Autoregressive (AR) Models**](#ar)\n",
    "    - [Code Lab: Simulating and Estimating an AR(2) Process](#code-ar)\n",
    "3.  [**Moving Average (MA) Models**](#ma)\n",
    "    - [Code Lab: Simulating and Estimating an MA(2) Process](#code-ma)\n",
    "4.  [**Autoregressive Moving Average (ARMA) Models**](#arma)\n",
    "    - [Code Lab: Simulating and Estimating an ARMA(1,1) Process](#code-arma)\n",
    "5.  [**The Box-Jenkins Methodology**](#box-jenkins)\n",
    "6.  [**Information Criteria for Model Selection**](#info-criteria)\n",
    "7.  [**Exercises**](#exercises)\n",
    "8.  [**Summary and Key Takeaways**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: Modeling Stationary Time Series\n",
    "\n",
    "This chapter introduces the foundational models for stationary time series: the **Autoregressive (AR)**, **Moving Average (MA)**, and combined **ARMA** models. These models, developed by George Box and Gwilym Jenkins, form the bedrock of classical time series analysis. They provide a parsimonious way to model the conditional mean of a stationary process by relating it to its own past values (the AR part) and to past forecast errors (the MA part).\n",
    "\n",
    "Understanding these models is crucial for forecasting and for providing a baseline against which more complex models can be compared. The core of this chapter revolves around the **Box-Jenkins methodology**, an iterative process for identifying, estimating, and diagnosing these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ar'></a>\n",
    "## 2. Autoregressive (AR) Models\n",
    "\n",
    "An **Autoregressive model of order p**, denoted AR(p), models the current value of the series, $y_t$, as a linear combination of its own $p$ previous values, plus a white noise error term $\\epsilon_t$.\n",
    "\n",
    "$$ y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t $$\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **ACF:** The ACF of an AR(p) process will decay exponentially or in a damped sine-wave pattern.\n",
    "- **PACF:** The PACF of an AR(p) process will have a sharp cutoff after lag $p$. All partial autocorrelations beyond lag $p$ will be statistically insignificant.\n",
    "This signature PACF cutoff is the primary tool for identifying the order of an AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-ar'></a>\n",
    "sec(\"Simulating and Estimating an AR(2) Process\")\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate AR(2) data: y_t = 0.7*y_{t-1} - 0.4*y_{t-2} + e_t\n",
    "ar_params = np.array([0.7, -0.4])\n",
    "ma_params = np.array([])\n",
    "y = sm.tsa.arma_generate_sample(ar=np.r_[1, -ar_params], ma=np.r_[1, ma_params], nsample=n_samples)\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plot_acf(y, ax=ax1, lags=20, title='AR(2) ACF')\n",
    "plot_pacf(y, ax=ax2, lags=20, title='AR(2) PACF')\n",
    "plt.show()\n",
    "note(\"The ACF tails off, while the PACF cuts off after lag 2, correctly identifying it as an AR(2) process.\")\n",
    "\n",
    "# Estimate the model\n",
    "model_ar = ARIMA(y, order=(2, 0, 0))\n",
    "results_ar = model_ar.fit()\n",
    "print(results_ar.summary())"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ma'></a>\n",
    "## 3. Moving Average (MA) Models\n",
    "\n",
    "A **Moving Average model of order q**, denoted MA(q), models the current value of the series, $y_t$, as a linear combination of the current and $q$ previous white noise error terms.\n",
    "\n",
    "$$ y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q} $$\n",
    "\n",
    "An MA model can be thought of as modeling a series that experiences random shocks, where the effect of a shock persists for $q$ periods.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **ACF:** The ACF of an MA(q) process will have a sharp cutoff after lag $q$. All autocorrelations beyond lag $q$ will be zero.\n",
    "- **PACF:** The PACF of an MA(q) process will decay exponentially or in a damped sine-wave pattern.\n",
    "This signature ACF cutoff is the primary tool for identifying the order of an MA model.\n",
    "\n",
    "### A Note on Invertibility\n",
    "For an MA model to be useful, it must be **invertible**. An MA process is invertible if it can be represented as an infinite-order AR process. This property is crucial because it ensures that the model is unique and that the weights placed on past observations decline over time. The condition for invertibility is that the roots of the MA characteristic polynomial ($1 + \\theta_1 z + ... + \\theta_q z^q = 0$) must lie *outside* the unit circle. This is analogous to the stationarity condition for AR models. All modern estimation packages, including `statsmodels`, enforce invertibility by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-ma'></a>\n",
    "sec(\"Simulating and Estimating an MA(2) Process\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate MA(2) data: y_t = e_t + 0.6*e_{t-1} + 0.4*e_{t-2}\n",
    "ar_params = np.array([])\n",
    "ma_params = np.array([0.6, 0.4])\n",
    "y_ma = sm.tsa.arma_generate_sample(ar=np.r_[1, -ar_params], ma=np.r_[1, ma_params], nsample=n_samples)\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plot_acf(y_ma, ax=ax1, lags=20, title='MA(2) ACF')\n",
    "plot_pacf(y_ma, ax=ax2, lags=20, title='MA(2) PACF')\n",
    "plt.show()\n",
    "note(\"The PACF tails off, while the ACF cuts off after lag 2, correctly identifying it as an MA(2) process.\")\n",
    "\n",
    "# Estimate the model\n",
    "model_ma = ARIMA(y_ma, order=(0, 0, 2))\n",
    "results_ma = model_ma.fit()\n",
    "print(results_ma.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arma'></a>\n",
    "## 4. Autoregressive Moving Average (ARMA) Models\n",
    "\n",
    "An **ARMA(p,q)** model combines the AR(p) and MA(q) components:\n",
    "$$ y_t = c + \\phi_1 y_{t-1} + ... + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q} $$\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **ACF:** The ACF of an ARMA(p,q) process will decay exponentially after lag $q$.\n",
    "- **PACF:** The PACF of an ARMA(p,q) process will decay exponentially after lag $p$.\n",
    "Since both ACF and PACF tail off, identifying the orders $p$ and $q$ can be more difficult than for pure AR or MA processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-arma'></a>\n",
    "sec(\"Simulating and Estimating an ARMA(1,1) Process\")\n",
    "np.random.seed(42)\n",
    "\n",
    "ar_params = np.array([0.75])\n",
    "ma_params = np.array([-0.25])\n",
    "y_arma = sm.tsa.arma_generate_sample(ar=np.r_[1, -ar_params], ma=np.r_[1, ma_params], nsample=n_samples)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plot_acf(y_arma, ax=ax1, lags=20, title='ARMA(1,1) ACF')\n",
    "plot_pacf(y_arma, ax=ax2, lags=20, title='ARMA(1,1) PACF')\n",
    "plt.show()\n",
    "note(\"Both the ACF and PACF tail off, which is characteristic of an ARMA process.\")\n",
    "\n",
    "model_arma = ARIMA(y_arma, order=(1, 0, 1))\n",
    "results_arma = model_arma.fit()\n",
    "print(results_arma.summary())"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='box-jenkins'></a>\n",
    "## 5. The Box-Jenkins Methodology\n",
    "\n",
    "The Box-Jenkins methodology is an iterative, three-stage process for identifying and estimating the best ARMA model for a given time series. The process is a cycle that continues until a satisfactory model is found.\n",
    "\n",
    "![Box-Jenkins Methodology](images\png\box_jenkins_methodology.png)\n",
    "\n",
    "1.  **Identification:**\n",
    "    - Test for stationarity. If the series is non-stationary, difference it until it is.\n",
    "    - Examine the ACF and PACF plots of the stationary series to identify potential orders (p,q) for the ARMA model.\n",
    "\n",
    "2.  **Estimation:**\n",
    "    - Estimate the parameters of the candidate model(s) using a numerical method like Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "3.  **Diagnostic Checking:**\n",
    "    - Analyze the residuals of the fitted model. If the model is a good fit, the residuals should behave like white noise (i.e., be uncorrelated with zero mean). We check this by examining the ACF of the residuals and using formal statistical tests.\n",
    "    - The **Ljung-Box Q-test** is a common formal test. Its null hypothesis is that the residuals are independently distributed (i.e., have no remaining autocorrelation). A high p-value on this test is desirable; it means we cannot reject the null and provides evidence that the model has captured the serial correlation in the data.\n",
    "    - If the residuals are not white noise, the model is misspecified, and we return to the identification stage to select a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Code Lab: Ljung-Box Test for Residuals\")\n",
    "\n",
    "# We use the ARMA(1,1) model we estimated earlier\n",
    "residuals = results_arma.resid\n",
    "\n",
    "# Plot ACF of residuals\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_acf(residuals, ax=ax, title='ACF of Model Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test\n",
    "ljung_box_results = sm.stats.acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "display(ljung_box_results)\n",
    "\n",
    "note(\"The ACF plot shows no significant spikes, and the Ljung-Box test p-value is high. This indicates that the residuals are white noise and our model has successfully captured the autocorrelation in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='info-criteria'></a>\n",
    "## 6. Information Criteria for Model Selection\n",
    "\n",
    "When the ACF/PACF plots are ambiguous, we can use **information criteria** to help select the best model among several candidates. These criteria create a trade-off between model fit (likelihood) and model complexity (number of parameters).\n",
    "\n",
    "- **Akaike Information Criterion (AIC):** $ AIC = -2 \\ln(L) + 2k $\n",
    "- **Bayesian Information Criterion (BIC):** $ BIC = -2 \\ln(L) + k \\ln(n) $\n",
    "\n",
    "Where $L$ is the maximized likelihood, $k$ is the number of parameters, and $n$ is the sample size. We choose the model with the **lowest** AIC or BIC value. The BIC penalizes model complexity more heavily than the AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-info-criteria'></a>\n",
    "sec(\"Code Lab: Using Information Criteria for Model Selection\")\n",
    "\n",
    "# We'll use the ARMA(1,1) data we generated earlier\n",
    "np.random.seed(42)\n",
    "ar_params = np.array([0.75])\n",
    "ma_params = np.array([-0.25])\n",
    "y_true_arma = sm.tsa.arma_generate_sample(ar=np.r_[1, -ar_params], ma=np.r_[1, ma_params], nsample=1000)\n",
    "\n",
    "# Use the arma_order_select_ic function to automatically find the best order\n",
    "# It calculates AIC and BIC for many different (p,q) combinations.\n",
    "note(\"Searching for the best model order using arma_order_select_ic...\")\n",
    "order_ic = arma_order_select_ic(y_true_arma, max_ar=4, max_ma=4, ic=['aic', 'bic'])\n",
    "\n",
    "print(\"\\nAIC-based Order Selection:\")\n",
    "print(order_ic.aic_min_order)\n",
    "display(order_ic.aic)\n",
    "\n",
    "print(\"\\nBIC-based Order Selection:\")\n",
    "print(order_ic.bic_min_order)\n",
    "display(order_ic.bic)\n",
    "\n",
    "note(f\"Both AIC and BIC correctly identify the true model order as {order_ic.aic_min_order}. We would select the ARMA(1,1) model.\")"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\\n",
    "## 7. A Practical Example: Modeling US Inflation\n",
    "\n",
    "Let's apply the Box-Jenkins methodology to a real-world economic time series: US inflation, measured by the Consumer Price Index (CPI). We'll use the core CPI (excluding food and energy) to get a clearer signal of underlying inflation dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Case Study: Modeling Core CPI Inflation\")\n",
    "\n",
    "# Load the data from FRED\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    start_date = '1960-01-01'\n",
    "    end_date = '2022-12-31'\n",
    "    # Core CPI (CPIAUCSL)\n",
    "    cpi_data = web.DataReader('CPIAUCSL', 'fred', start_date, end_date)\n",
    "    # Calculate year-over-year inflation rate\n",
    "    inflation = cpi_data.pct_change(12).dropna() * 100\n",
    "    note(f\"Successfully downloaded Core CPI data from {start_date} to {end_date}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download data. Error: {e}\")\n",
    "    # Create a dummy series if download fails\n",
    "    inflation = pd.Series(np.random.randn(200), index=pd.date_range('2000-01-01', periods=200, freq='M'))\n",
    "\n",
    "# --- 1. Identification ---\n",
    "note(\"**Step 1: Identification** - Check for stationarity and examine ACF/PACF.\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(inflation)\n",
    "plt.title('US Core CPI Inflation (Year-over-Year)')\n",
    "plt.ylabel('Percent')\n",
    "plt.show()\n",
    "\n",
    "# Check for stationarity with the Augmented Dickey-Fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf_result = adfuller(inflation['CPIAUCSL'])\n",
    "print(f'ADF Statistic: {adf_result[0]}')\n",
    "print(f'p-value: {adf_result[1]}')\n",
    "note(f\"The ADF test p-value is {adf_result[1]:.3f}. If this is low (e.g., < 0.05), we reject the null hypothesis of a unit root, suggesting the series is stationary. Inflation appears to be a very persistent process.\")\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plot_acf(inflation['CPIAUCSL'], ax=ax1, lags=24, title='Inflation ACF')\n",
    "plot_pacf(inflation['CPIAUCSL'], ax=ax2, lags=24, title='Inflation PACF')\n",
    "plt.show()\n",
    "note(\"The ACF is very slow to decay, and the PACF has a large spike at lag 1, after which other lags cut off. This is a strong indication of a highly autoregressive process, possibly an AR(1) or a more complex ARMA model. Let's try fitting a few candidate models.\")\n",
    "\n",
    "# --- 2. Estimation ---\n",
    "note(\"**Step 2: Estimation** - Fit candidate models and compare using AIC/BIC.\")\n",
    "model_ar1 = ARIMA(inflation, order=(1, 0, 0)).fit()\n",
    "model_ar2 = ARIMA(inflation, order=(2, 0, 0)).fit()\n",
    "model_arma11 = ARIMA(inflation, order=(1, 0, 1)).fit()\n",
    "\n",
    "print(\"--- Model Comparison ---\")\n",
    "print(f\"AR(1)   AIC: {model_ar1.aic:.2f}, BIC: {model_ar1.bic:.2f}\")\n",
    "print(f\"AR(2)   AIC: {model_ar2.aic:.2f}, BIC: {model_ar2.bic:.2f}\")\n",
    "print(f\"ARMA(1,1) AIC: {model_arma11.aic:.2f}, BIC: {model_arma11.bic:.2f}\")\n",
    "\n",
    "# --- 3. Diagnostic Checking ---\n",
    "note(\"**Step 3: Diagnostics** - The AR(1) model appears to be a good, parsimonious choice. Let's check its residuals.\")\n",
    "print(model_ar1.summary())\n",
    "lj_res = sm.stats.acorr_ljungbox(model_ar1.resid, lags=[12], return_df=True)\n",
    "display(lj_res)\n",
    "note(\"The Ljung-Box test on the AR(1) residuals gives a high p-value, suggesting the residuals are white noise. The AR(1) model seems to be a good fit for this data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\\n",
    "## 8. Exercises\\n",
    "\\n",
    "1.  **Model Identification:** You plot the ACF and PACF for a stationary time series. The ACF tails off slowly, while the PACF has significant spikes at lags 1 and 3, and cuts off after lag 3. What model would you tentatively identify?\\n",
    "2.  **Overfitting:** What happens if you fit an AR(5) model to a series that is truly an AR(1) process? What would you expect to see in the summary of the estimated model?\\n",
    "3.  **Residual Diagnostics:** After fitting an ARMA(1,1) model, you plot the ACF of the residuals and find a significant spike at lag 4. What does this suggest about your model? What might you do next?\\n",
    "4.  **AIC vs. BIC:** You are choosing between an ARMA(2,1) model and an ARMA(1,2) model. Both have 3 parameters. Will AIC and BIC always agree on the preferred model in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\\n",
    "## 8. Summary and Key Takeaways\\n",
    "\\n",
    "This chapter introduced the ARMA framework, the classical approach to modeling stationary time series.\\n",
    "\\n",
    "**Key Concepts**:\\n",
    "- **AR(p) Models**: Model the current value as a function of past values. They have a decaying ACF and a sharp cutoff in the PACF at lag $p$.\\n",
    "- **MA(q) Models**: Model the current value as a function of past error terms. They have a sharp cutoff in the ACF at lag $q$ and a decaying PACF.\\n",
    "- **ARMA(p,q) Models**: Combine both components. Both their ACF and PACF decay exponentially.\\n",
    "- **Box-Jenkins Methodology**: A three-stage iterative process (Identify, Estimate, Diagnose) for building ARMA models.\\n",
    "- **Information Criteria**: AIC and BIC are used to select the best model by balancing model fit and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**1. Model Identification:**\\n",
    "The decaying ACF and cutoff in the PACF at lag 3 are the classic signatures of an **AR(3)** process.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**2. Overfitting:**\\n",
    "You would be fitting an over-parameterized model. In the results summary, you would expect the coefficient on the first lag, $\\phi_1$, to be statistically significant and close to the true value. The coefficients on lags 2 through 5, however, would be statistically insignificant (high p-values) and close to zero, as they are capturing noise.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**3. Residual Diagnostics:**\\n",
    "A significant spike in the ACF of the residuals at lag 4 indicates that there is still predictable structure left in the errors that the model has not captured, specifically at a seasonal frequency (e.g., quarterly). This suggests the model is misspecified. You might try adding a seasonal component to the model, leading to a SARIMA model, or adding an AR(4) term.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**4. AIC vs. BIC:**\\n",
    "Yes, they will always agree in this case. Both models have the same number of parameters ($k=3$). The AIC and BIC formulas are $ -2\\ln(L) + C \\cdot k $, where $C=2$ for AIC and $C=\\ln(n)$ for BIC. Since $k$ is the same for both models, the only term that differs between them is the maximized log-likelihood, $\\ln(L)$. Both criteria will therefore select the model with the higher log-likelihood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
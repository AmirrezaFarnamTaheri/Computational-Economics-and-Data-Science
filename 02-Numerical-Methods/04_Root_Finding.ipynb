{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "from typing import Callable\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq, newton, root\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=8)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Core Numerical Methods\n",
    "## Chapter 2.4: Root-Finding and Fixed-Point Algorithms\n",
    "\n",
    "### Table of Contents\n",
    "1.  [Fixed-Point Theory: Existence and Uniqueness](#1.-Fixed-Point-Theory:-Existence-and-Uniqueness)\n",
    "    *   [1.1 Fixed-Point Iteration](#1.1-Fixed-Point-Iteration)\n",
    "    *   [1.2 Contraction Mapping Theorem](#1.2-Contraction-Mapping-Theorem)\n",
    "    *   [1.3 Brouwer's Fixed-Point Theorem](#1.3-Brouwer's-Fixed-Point-Theorem)\n",
    "    *   [1.4 Accelerating Convergence: Aitken's Delta-Squared Process](#1.4-Accelerating-Convergence:-Aitken's-Delta-Squared-Process)\n",
    "2.  [Root-Finding Algorithms for Single Equations](#2.-Root-Finding-Algorithms-for-Single-Equations)\n",
    "    *   [2.1 Bracketing vs. Open Methods](#2.1-Bracketing-vs.-Open-Methods)\n",
    "    *   [2.2 Newton's Method: Basins of Attraction](#2.2-Newton's-Method:-Basins-of-Attraction)\n",
    "3.  [Systems of Non-Linear Equations](#3.-Systems-of-Non-Linear-Equations)\n",
    "    *   [3.1 Homotopy Continuation Methods](#3.1-Homotopy-Continuation-Methods)\n",
    "    *   [3.2 Application: General Equilibrium with CES Utility](#3.2-Application:-General-Equilibrium-with-CES-Utility)\n",
    "4.  [Chapter Summary](#4.-Chapter-Summary)\n",
    "5.  [Exercises](#5.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Solving for Equilibrium\n",
    "A vast number of problems in economics and finance can be reduced to the fundamental tasks of **finding the root of a function** (finding `x` such that `f(x) = 0`) or **finding a fixed point of a function** (finding `x` such that `g(x) = x`). These are the mathematical equivalents of solving for an equilibrium where opposing forces are in balance or where a system is in a state of rest.\n",
    "\n",
    "This chapter uses canonical macroeconomic problems to motivate and explore the most important algorithms for these tasks. We will cover:\n",
    "*   **Fixed-Point Theory:** Examining the conditions that guarantee the existence and uniqueness of solutions, including the **Contraction Mapping** and **Brouwer's** theorems.\n",
    "*   **Root-Finding Algorithms:** Exploring the trade-offs between robust but slow methods (Bisection) and fast but less reliable methods (Newton's), including a look at their convergence properties and failure modes.\n",
    "*   **Advanced Techniques:** Introducing powerful methods for accelerating convergence (**Aitken's Process**) and for robustly solving systems of equations (**Homotopy Continuation**).\n",
    "*   **Economic Applications:** We will solve for the steady-state of the Solow Growth Model and find the equilibrium prices in a more advanced general equilibrium model with CES utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fixed-Point Theory: Existence and Uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Fixed-Point Iteration\n",
    "A **fixed point** of a function $g: X \\to X$ is a point $x^* \\in X$ such that $g(x^*) = x^*$. Many dynamic economic models are expressed as a law of motion $x_{t+1} = g(x_t)$, and the steady state of this system is a fixed point of $g$.\n",
    "\n",
    "The simplest way to find a fixed point is via **fixed-point iteration**: choose an initial guess $x_0$ and then iterate $x_{k+1} = g(x_k)$ until the sequence converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Contraction Mapping Theorem\n",
    "**The Contraction Mapping Theorem** provides *sufficient* conditions for the existence and uniqueness of a fixed point and for the convergence of this iterative process. A function $g$ is a **contraction mapping** on a complete metric space $(X, d)$ if there exists a constant $\\beta \\in [0, 1)$ such that for all $x, y \\in X$:\n",
    "$$ d(g(x), g(y)) \\le \\beta \\cdot d(x, y) $$\n",
    "If $g$ is a contraction, then it has a unique fixed point, and fixed-point iteration will converge to it from any starting point. This is the workhorse theorem for proving convergence in dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Brouwer's Fixed-Point Theorem\n",
    "\\nThis theorem was famously developed by L.E.J. Brouwer in the early 20th century. One of its most intuitive interpretations is the 'stirring coffee' analogy: no matter how you stir a cup of coffee (continuously, without tearing the liquid), some point of the liquid will end up in exactly the same position it started in.\\n",
    "Brouwer's theorem provides a weaker but more general condition for the *existence* of a fixed point. It states that if $S$ is a **compact** (closed and bounded) and **convex** subset of a Euclidean space and $g: S \\to S$ is a **continuous** function, then $g$ must have at least one fixed point.\n",
    "\n",
    "**Key Differences from Contraction Mapping:**\n",
    "- **Existence Only:** Brouwer's theorem does not guarantee uniqueness, nor does it tell us how to find the fixed point. It is non-constructive.\n",
    "- **Weaker Conditions:** It does not require a metric or the contraction property, only continuity and a mapping from a compact, convex set to itself. This makes it applicable to a wider range of problems, such as proving the existence of a Nash Equilibrium in game theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Accelerating Convergence: Aitken's Delta-Squared Process\n",
    "Standard fixed-point iteration can be slow if the convergence is only linear. **Aitken's Delta-Squared Process** is a method for accelerating the convergence of such sequences. Given a sequence $\\{x_k\\}$, it generates a new, faster-converging sequence $\\{x'_k\\}$ using the formula:\n",
    "$$ x'_k = x_k - \\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k} $$\n",
    "This process can often turn a linearly convergent sequence into one that converges quadratically.",
    "\\nIntuitively, the process uses three consecutive points ($x_k, x_{k+1}, x_{k+2}$) to estimate the rate of convergence and then extrapolates to the limit of the sequence. It's closely related to the Secant method for root-finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accelerating Solow Model Convergence with Aitken's Method](../images/02-Numerical-Methods/aitken_acceleration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Root-Finding Algorithms for Single Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Bracketing vs. Open Methods\n",
    "Root-finding algorithms fall into two main categories:\n",
    "- **Bracketing Methods (e.g., Bisection):** These require an initial interval $[a, b]$ where the root is known to exist (i.e., $f(a)$ and $f(b)$ have opposite signs). They are guaranteed to converge (**global convergence**) but are often slow (Bisection is linear).\n",
    "- **Open Methods (e.g., Newton's, Secant):** These require only an initial guess. They use local information (derivatives) to find the next iterate. They are much faster (Newton's is quadratic) but are only guaranteed to converge if the initial guess is sufficiently close to the root (**local convergence**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a comparison of the convergence rates for finding the root of $f(x) = x^2 - 2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison of Root-Finding Algorithm Convergence](../images/02-Numerical-Methods/rootfinding_convergence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Newton's Method: Basins of Attraction\n",
    "The sensitivity of Newton's method to the initial guess can be visualized by plotting its **basins of attraction**. For a function with multiple roots, the basin of attraction for a given root is the set of all starting points from which the method converges to that root. These basins can have intricate, fractal boundaries.\n",
    "\n",
    "A classic example is finding the roots of $f(z) = z^3 - 1$ in the complex plane. The three roots are $1$, $e^{i2\\pi/3}$, and $e^{-i2\\pi/3}$. The plot below shows which root Newton's method converges to for each starting point in the complex plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basins of Attraction for Newton's Method](../images/02-Numerical-Methods/newton_basins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Systems of Non-Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Homotopy Continuation Methods\n",
    "Finding the root of a system of non-linear equations, $F(\\mathbf{x}) = \\mathbf{0}$, can be very difficult. Standard methods like multivariate Newton's method are highly sensitive to the initial guess. **Homotopy continuation** provides a more robust, global approach.\n",
    "\n",
    "The idea is to start with a simple system $G(\\mathbf{x})=\\mathbf{0}$ that we *can* solve (e.g., $G(\\mathbf{x}) = \\mathbf{x} - \\mathbf{x}_0$). We then define a **homotopy**, a function that gradually deforms $G$ into our target function $F$:\n",
    "$$ H(\\mathbf{x}, t) = (1-t)G(\\mathbf{x}) + tF(\\mathbf{x}), \\quad t \\in [0, 1] $$\n",
    "\n",
    "We start with the known solution to $H(\\mathbf{x}, 0) = G(\\mathbf{x}) = \\mathbf{0}$. We then increment $t$ in small steps, and at each step, we use the solution from the previous step as the initial guess to solve $H(\\mathbf{x}, t_{new}) = \\mathbf{0}$. By tracing this path of solutions from $t=0$ to $t=1$, we arrive at the solution to the original problem, $H(\\mathbf{x}, 1) = F(\\mathbf{x}) = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Application: General Equilibrium with CES Utility\n",
    "We can use `scipy.optimize.root` to solve for the equilibrium prices in a more complex general equilibrium model. Consider an Edgeworth box economy with two agents (A, B) and two goods (1, 2). Instead of Cobb-Douglas utility, we use the more general **Constant Elasticity of Substitution (CES)** utility function:\n",
    "$$ U(x_1, x_2) = (\\alpha x_1^\\rho + (1-\\alpha)x_2^\\rho)^{1/\\rho} $$\n",
    "The elasticity of substitution is $\\sigma = 1/(1-\\rho)$. The demand functions derived from CES utility are more non-linear than those from Cobb-Douglas. We will normalize the price of good 2 to be the num√©raire ($p_2=1$) and solve for the relative price $p_1$ that clears the market for good 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Solving for General Equilibrium with CES Utility\")\n",
    "\n",
    "# Agent A: high preference for good 1, low elasticity\n",
    "alpha_A, rho_A = 0.7, -1.0 # sigma = 0.5\n",
    "# Agent B: balanced preference, high elasticity\n",
    "alpha_B, rho_B = 0.5, 0.8 # sigma = 5.0\n",
    "\n",
    "# Endowments [good1, good2]\n",
    "e_A, e_B = np.array([4, 1]), np.array([1, 4])\n",
    "\n",
    "def ces_demand(p1, p2, income, alpha, rho):\n",
    "    \"\"\"Marshallian demand for good 1 from CES utility.\"\"\"\n",
    "    sigma = 1 / (1 - rho)\n",
    "    term1 = alpha**sigma / (alpha**sigma * p1**sigma + (1-alpha)**sigma * p2**sigma)\n",
    "    return term1 * income / p1\n",
    "\n",
    "def excess_demand_good1(p1, p2=1.0):\n",
    "    income_A = p1 * e_A[0] + p2 * e_A[1]\n",
    "    income_B = p1 * e_B[0] + p2 * e_B[1]\n",
    "    demand_A1 = ces_demand(p1, p2, income_A, alpha_A, rho_A)\n",
    "    demand_B1 = ces_demand(p1, p2, income_B, alpha_B, rho_B)\n",
    "    return (demand_A1 + demand_B1) - (e_A[0] + e_B[0])\n",
    "\n",
    "note(\"Solving for the root of the more complex excess demand function:\")\n",
    "p1_star = brentq(excess_demand_good1, a=0.1, b=10.0)\n",
    "p2_star = 1.0\n",
    "\n",
    "note(f\"Equilibrium found! Relative price p1/p2: {p1_star/p2_star:.4f}\")\n",
    "print(f\"Excess demand for good 1 at this price: {excess_demand_good1(p1_star):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chapter Summary\n",
    "- **Fixed Points:** Problems of the form $g(x)=x$ are central to dynamic models. The **Contraction Mapping Theorem** provides a constructive proof of a unique solution, while **Brouwer's Theorem** guarantees existence under weaker conditions.\n",
    "- **Acceleration:** Linearly convergent fixed-point iterations can be significantly sped up using techniques like **Aitken's Delta-Squared Process**.\n",
    "- **Root-Finding:** Algorithms can be **globally convergent** (Bisection) or only **locally convergent** (Newton's). The choice of method involves a trade-off between robustness and speed. The sensitivity of open methods to initial guesses can be visualized with **basins of attraction**.\n",
    "- **Systems of Equations:** For complex systems, standard solvers can fail. **Homotopy Continuation Methods** provide a more robust global strategy by deforming a simple, solvable problem into the target problem.\n",
    "- **Use Professional Tools:** For real-world problems, always rely on the robust, well-tested solvers in libraries like `scipy.optimize` (`brentq`, `newton`, `root`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Exercises\n",
    "\n",
    "1.  **Aitken's Method from Scratch:** The code for `aitken_accelerate` is provided. Explain in words what each part of the formula for $x'_k$ is doing. Why might the denominator become close to zero, and what does this imply about the underlying sequence?\n",
    "\n",
    "2.  **Implement Newton's Method for Systems:** Write a Python function that implements Newton's method for a system of non-linear equations. It should take a function `F` that returns a vector of errors, a function `J` that returns the Jacobian matrix, and an initial guess `x0`. Test it by solving the CES general equilibrium problem from Section 3.2.\n",
    "\n",
    "3.  **Newton's Method Failure:** The function $f(x) = x^3 - 2x + 2$ has a local minimum near $x \\approx 0.8$. Show that if you start Newton's method with an initial guess of $x_0=1$, the next iterate is $x_1=0$, where the derivative is zero, causing the method to fail. If you start at $x_0=0$, what happens?\n",
    "\n",
    "4.  **Implicit Yield Curve:** The price `P` of a bond with face value `F`, coupon rate `c`, `T` periods to maturity, and yield-to-maturity `y` is given by: $ P = \\sum_{t=1}^{T} \\frac{c F}{(1+y)^t} + \\frac{F}{(1+y)^T} $. Finding the yield-to-maturity (YTM) for a given market price `P` is a root-finding problem. Write a function that takes `P`, `F`, `c`, and `T` and uses `brentq` to solve for the YTM, `y`.\n",
    "\n",
    "5.  **Homotopy Continuation:** Consider the difficult root-finding problem $F(x) = \\cos(4\\pi x) - x = 0$. It has many roots. Let your simple, solvable problem be $G(x) = x - 0.5 = 0$. Implement a simple homotopy continuation method to track the solution from $t=0$ to $t=1$. At each step of $t$, use `scipy.optimize.newton` to solve for the root of $H(x,t)$, using the previous step's solution as the initial guess. Plot the path of the root as a function of $t$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\n
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# JAX is the core library for this chapter.\n",
    "try:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    from jax import grad, jacfwd, jacrev, hessian, jit, vmap\n",
    "    from jax.scipy.stats import norm\n",
    "    JAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JAX_AVAILABLE = False\n",
    "    # Define dummy functions if JAX is not available\n",
    "    def jit(f): return f\n",
    "    def grad(f): return f\n",
    "    def hessian(f): return f\n",
    "    def vmap(f): return f\n",
    "\n",
    "# SymPy for symbolic differentiation\n",
    "import sympy as sp\n",
    "\n",
    "# Graphviz for visualizing computational graphs\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    GRAPHVIZ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GRAPHVIZ_AVAILABLE = False\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=8)\n",
    "sp.init_printing(use_unicode=True)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(f\"Environment initialized. JAX available: {JAX_AVAILABLE}, Graphviz available: {GRAPHVIZ_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Core Numerical Methods\n",
    "## Chapter 2.3: Numerical and Automatic Differentiation\n",
    "\n",
    "### Table of Contents\n",
    "1.  [Numerical Differentiation via Finite Differences](#1.-Numerical-Differentiation-via-Finite-Differences)\n",
    "    *   [1.1 Taylor Series Derivations](#1.1-Taylor-Series-Derivations)\n",
    "    *   [1.2 The Error Trade-off](#1.2-The-Error-Trade-off)\n",
    "2.  [Automatic Differentiation (AD)](#2.-Automatic-Differentiation-(AD))\n",
    "    *   [2.1 The Computational Graph](#2.1-The-Computational-Graph)\n",
    "    *   [2.2 Forward-Mode vs. Reverse-Mode AD](#2.2-Forward-Mode-vs.-Reverse-Mode-AD)\n",
    "3.  [The JAX Framework](#3.-The-JAX-Framework)\n",
    "    *   [3.1 Core Transformations: `grad`, `jit`, `vmap`](#3.1-Core-Transformations:-grad,-jit,-vmap)\n",
    "    *   [3.2 The Constraint of Pure Functions](#3.2-The-Constraint-of-Pure-Functions)\n",
    "4.  [Application 1: Utility Maximization](#4.-Application-1:-Utility-Maximization)\n",
    "5.  [Application 2: Maximum Likelihood Estimation](#5.-Application-2:-Maximum-Likelihood-Estimation)\n",
    "6.  [Chapter Summary](#6.-Chapter-Summary)\n",
    "7.  [Exercises](#7.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: The Three Ways to Differentiate Code\n",
    "Differentiation is a cornerstone of economic analysis. When we have a function implemented in code, we face a choice between three fundamental approaches to compute its derivative:\n",
    "\n",
    "1.  **Symbolic Differentiation**: The \"pen-and-paper\" method automated by computer algebra systems (like SymPy). It is exact but can be intractable for complex functions due to \"expression swell.\"\n",
    "\n",
    "2.  **Numerical Differentiation**: Approximating the derivative using function evaluations at discrete points (e.g., finite differences). It is easy to implement but is an approximation and suffers from a difficult trade-off between truncation and rounding error.\n",
    "\n",
    "3.  **Automatic Differentiation (AD)**: A powerful technique that computes exact derivatives (to machine precision) by breaking down a function into elementary operations and applying the chain rule. It is the engine of modern machine learning and computational economics.\n",
    "\n",
    "This notebook provides a deep dive into these methods, with a particular focus on demystifying and applying Automatic Differentiation, which has become the preferred method for most modern applications due to its accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numerical Differentiation via Finite Differences\n",
    "\n",
    "Numerical differentiation approximates derivatives by evaluating a function at nearby points. The basis for these methods is the Taylor series expansion of a function $f(x)$ around a point $x_0$:\n",
    "$$ f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\dots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Taylor Series Derivations\n",
    "\n",
    "By rearranging the Taylor series and truncating higher-order terms, we can derive the common finite difference formulas.\n",
    "\n",
    "- **Forward Difference:** Solve the expansion for $f'(x_0)$:\n",
    "  $$ f'(x_0) = \\frac{f(x_0+h) - f(x_0)}{h} - \\frac{h}{2}f''(x_0) - \\dots = \\frac{f(x_0+h) - f(x_0)}{h} + O(h) $$\n",
    "  This formula is simple but has a truncation error of order $O(h)$, making it relatively inaccurate.\n",
    "\n",
    "- **Central Difference:** Subtract the Taylor expansion for $f(x_0-h)$ from $f(x_0+h)$:\n",
    "  $$ f(x_0+h) - f(x_0-h) = 2hf'(x_0) + \\frac{2h^3}{6}f'''(x_0) + \\dots $$\n",
    "  $$ f'(x_0) = \\frac{f(x_0+h) - f(x_0-h)}{2h} - \\frac{h^2}{6}f'''(x_0) - \\dots = \\frac{f(x_0+h) - f(x_0-h)}{2h} + O(h^2) $$\n",
    "  By canceling the even-powered terms, the central difference formula achieves a much smaller truncation error of order $O(h^2)$, making it significantly more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 The Error Trade-off\n",
    "\n",
    "The central challenge is choosing the step size `h`. A smaller `h` reduces the mathematical truncation error (the $O(h^p)$ term), but a too-small `h` means we are subtracting two very close numbers in the numerator, leading to **catastrophic cancellation** (a form of rounding error). This creates a characteristic U-shaped curve for the total error, where the optimal `h` balances these two opposing forces.",
    "\\nThis trade-off is fundamental. The truncation error is a property of the mathematical formula (the approximation), while the rounding error is a property of the computer hardware (the finite precision). The optimal `h` is typically around the cube root of machine epsilon for forward-difference and the fourth root for central-difference formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Error vs. Step Size for Finite Differences](../images/02-Numerical-Methods/finite_difference_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Automatic Differentiation (AD)\n",
    "\\nAD was first developed in the 1950s and 1960s, but it was the rise of deep learning and the development of frameworks like Theano, TensorFlow, and PyTorch that brought it to the forefront of scientific computing. The backpropagation algorithm, which is a specific instance of reverse-mode AD, is the engine that drives modern neural networks.\\n",
    "**Automatic Differentiation (AD)** is a set of techniques that computes exact derivatives (to machine precision) with a computational cost that is a small, constant multiple of the cost of evaluating the original function. It is not symbolic and it is not numerical. AD works by decomposing a function into a **computational graph** of elementary operations (e.g., `+`, `*`, `sin`, `exp`) whose derivatives are known. It then repeatedly applies the chain rule to accumulate the derivative of the final output with respect to the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The Computational Graph\n",
    "Consider the function $y = f(x_1, x_2) = \\ln(x_1) + x_1 x_2 - \\sin(x_2)$. AD first breaks this down into a sequence of primitive operations, forming a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Computational Graph for a Simple Function](../images/02-Numerical-Methods/computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Forward-Mode vs. Reverse-Mode AD\n",
    "\n",
    "- **Forward-Mode AD** traverses the graph from inputs to outputs. It computes the derivative of each intermediate node with respect to *one input* at a time. This is done by carrying a \"dual number\" $(v, \\dot{v})$ where $v$ is the value and $\\dot{v} = \\frac{\\partial v}{\\partial x_i}$ is the derivative. To get the full gradient, this requires one pass per input variable.\n",
    "  - **Cost:** For $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, the cost is **O(n) * cost(f)**. Efficient for tall, skinny Jacobians ($n \\ll m$).\n",
    "\n",
    "- **Reverse-Mode AD (Backpropagation)** traverses the graph in reverse, from outputs to inputs. It first performs a forward pass to compute the value of every node. Then, it starts from the final output (with derivative $\\frac{\\partial y}{\\partial y} = 1$) and propagates derivatives backward, computing the derivative of the final output with respect to each intermediate node ($\"bar\"v = \\frac{\\partial y}{\\partial v}$). This gives the entire gradient in one backward pass.\n",
    "  - **Cost:** For $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, the cost is **O(m) * cost(f)**. Efficient for short, fat Jacobians ($n \\gg m$).\n",
    "\n",
    "**Rule of Thumb:** For the typical case in economics and machine learning‚Äîfunctions with many inputs (parameters) and a single scalar output (e.g., a loss function, a likelihood, a utility value), where $n \\gg m=1$‚Äî**reverse-mode AD is vastly more efficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The JAX Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Core Transformations: `grad`, `jit`, `vmap`\n",
    "JAX provides a set of function transformations that are the key to its power.\n",
    "\n",
    "- `grad(fun)`: Transforms a Python function `fun` that returns a scalar into a function that computes its gradient using reverse-mode AD.\n",
    "- `jit(fun)`: **Just-in-time compilation.** Compiles a Python function to highly optimized XLA machine code, often resulting in dramatic speedups, especially for functions with loops.\n",
    "- `vmap(fun)`: **Vectorizing map.** Transforms a function to map over batch dimensions automatically, without requiring manual loops. This is extremely powerful for batch processing and data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Vectorization with vmap\")\n",
    "if not JAX_AVAILABLE:\n",
    "    note(\"JAX is not installed. Skipping this section.\")\n",
    "else:\n",
    "    # A function that computes a weighted norm for a single vector\n",
    "    def weighted_norm(weights, vector):\n",
    "        return jnp.sqrt(jnp.sum(weights * vector**2))\n",
    "\n",
    "    # Sample data: one set of weights, 5 different vectors\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    weights = jnp.array([1., 2., 0.5])\n",
    "    vectors = jax.random.normal(key, (5, 3))\n",
    "\n",
    "    # Use vmap to apply the function to each vector in the batch\n",
    "    # in_axes=(None, 0) means: don't map over 'weights', map over the first axis of 'vectors'\n",
    "    batched_norm = vmap(weighted_norm, in_axes=(None, 0))\n",
    "    results = batched_norm(weights, vectors)\n",
    "    \n",
    "    note(\"vmap allows us to process a batch of 5 vectors without writing a loop:\")\n",
    "    print(f\"Input vectors shape: {vectors.shape}\")\n",
    "    print(f\"Output results shape: {results.shape}\")\n",
    "    print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 The Constraint of Pure Functions\n",
    "JAX's transformations (`jit`, `grad`, `vmap`, etc.) require that the functions they operate on be **pure**. A pure function has two properties:\n",
    "1.  It has no side effects (e.g., it doesn't modify global state, print to the screen, or write to a file).\n",
    "2.  Its output depends only on its inputs.\n",
    "\n",
    "This functional programming model allows JAX to safely trace, transform, and compile the code. Trying to `jit` or `grad` an impure function will often lead to errors or unexpected behavior.",
    "\\nFor example, a function that modifies a global variable or prints to the screen is not pure. JAX traces the function to compile it, and if the function's behavior changes between traces due to side effects, the compiled code may be incorrect or unpredictable. This is why a functional programming style is encouraged when working with JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application 1: Utility Maximization\n",
    "A classic economic problem is for a consumer to maximize their utility subject to a budget constraint. We can solve this by setting up the Lagrangian and using JAX to find the first-order conditions.\n",
    "\n",
    "**Problem:** Maximize $U(c_1, c_2) = (c_1^\\rho + c_2^\\rho)^{1/\\rho}$ subject to $p_1 c_1 + p_2 c_2 = M$.\n",
    "The Lagrangian is: $\\mathcal{L}(c_1, c_2, \\lambda) = (c_1^\\rho + c_2^\\rho)^{1/\\rho} - \\lambda(p_1 c_1 + p_2 c_2 - M)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Utility Maximization with JAX and Newton's Method\")\n",
    "if not JAX_AVAILABLE:\n",
    "    note(\"JAX is not installed. Skipping this section.\")\n",
    "else:\n",
    "    def lagrangian(x, p1, p2, M, rho):\n",
    "        c1, c2, lam = x[0], x[1], x[2]\n",
    "        return (c1**rho + c2**rho)**(1/rho) - lam * (p1*c1 + p2*c2 - M)\n",
    "\n",
    "    params = {'p1': 2.0, 'p2': 3.0, 'M': 100.0, 'rho': 0.8}\n",
    "    L = lambda x: lagrangian(x, **params)\n",
    "    FOCs = jit(grad(L))\n",
    "    Jacobian_FOCs = jit(hessian(L))\n",
    "\n",
    "    x_k = jnp.array([10.0, 10.0, 0.1])\n",
    "    for i in range(20):\n",
    "        step = jnp.linalg.solve(Jacobian_FOCs(x_k), -FOCs(x_k))\n",
    "        x_k = x_k + step\n",
    "        if jnp.linalg.norm(step) < 1e-7:\n",
    "            print(f\"Converged in {i+1} iterations.\")\n",
    "            break\n",
    "            \n",
    "    note(f\"Optimal bundle: c1 = {x_k[0]:.2f}, c2 = {x_k[1]:.2f}, shadow price Œª = {x_k[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application 2: Maximum Likelihood Estimation\n",
    "Maximum Likelihood Estimation (MLE) is a cornerstone of econometrics. The goal is to find the parameters $\\theta$ that maximize the likelihood of observing a given dataset. This is equivalent to maximizing the log-likelihood function, $LL(\\theta) = \\sum_i \\log f(y_i | \\theta)$, where $f$ is the probability density function.\n",
    "\n",
    "We can use JAX to find the MLE parameters for a sample drawn from a normal distribution. The parameters are $\\theta = (\\mu, \\sigma)$. The gradient of the log-likelihood is the **score**, and the negative inverse of the Hessian is the **variance-covariance matrix** of the estimates, which gives us the standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Maximum Likelihood Estimation of a Normal Distribution\")\n",
    "if not JAX_AVAILABLE:\n",
    "    note(\"JAX is not installed. Skipping this section.\")\n",
    "else:\n",
    "    # 1. Generate sample data\n",
    "    key = jax.random.PRNGKey(123)\n",
    "    true_mu, true_sigma = 5.0, 2.0\n",
    "    data = jax.random.normal(key, (1000,)) * true_sigma + true_mu\n",
    "\n",
    "    # 2. Define the log-likelihood function\n",
    "    def log_likelihood(params, y):\n",
    "        mu, sigma = params[0], jnp.exp(params[1]) # Use log(sigma) for unconstrained optimization\n",
    "        return jnp.sum(norm.logpdf(y, loc=mu, scale=sigma))\n",
    "\n",
    "    # 3. Use JAX to get the gradient and hessian\n",
    "    # We want to maximize, so we minimize the negative log-likelihood\n",
    "    neg_LL = jit(lambda p, y: -log_likelihood(p, y))\n",
    "    score_fn = jit(grad(neg_LL, argnums=0))\n",
    "    hessian_fn = jit(hessian(neg_LL, argnums=0))\n",
    "\n",
    "    # 4. Solve using a simple optimizer (e.g., BFGS from scipy)\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # Scipy's minimize function needs functions that accept numpy arrays\n",
    "    def objective(params, y): return np.array(neg_LL(params, y))\n",
    "    def jacobian(params, y): return np.array(score_fn(params, y))\n",
    "    \n",
    "    initial_params = np.array([0.0, 0.0]) # Initial guess for (mu, log(sigma))\n",
    "    result = minimize(objective, initial_params, args=(data,), jac=jacobian, method='BFGS')\n",
    "    mle_params = result.x\n",
    "    mle_mu, mle_sigma = mle_params[0], np.exp(mle_params[1])\n",
    "\n",
    "    # 5. Compute standard errors from the Hessian\n",
    "    H = hessian_fn(mle_params, data)\n",
    "    cov_matrix = jnp.linalg.inv(H)\n",
    "    std_errors = jnp.sqrt(jnp.diag(cov_matrix))\n",
    "\n",
    "    note(f\"True parameters:      Œº = {true_mu:.3f}, œÉ = {true_sigma:.3f}\")\n",
    "    note(f\"MLE estimates:        ŒºÃÇ = {mle_mu:.3f}, œÉÃÇ = {mle_sigma:.3f}\")\n",
    "    note(f\"Standard Errors (Œº, log œÉ): {std_errors[0]:.3f}, {std_errors[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Chapter Summary\n",
    "\n",
    "- **Three Methods:** Differentiation can be done **symbolically** (exact but complex), **numerically** (approximate but simple), or **automatically** (exact and efficient).\n",
    "- **Numerical Differentiation:** Relies on Taylor series approximations (finite differences). Its accuracy is limited by a fundamental trade-off between **truncation error** (from the formula) and **rounding error** (from machine precision).\n",
    "- **Automatic Differentiation (AD):** The modern standard. It achieves machine-precision derivatives by applying the chain rule to a computational graph. **Reverse-mode AD** is exceptionally efficient for the common many-to-one functions found in economics and machine learning.\n",
    "- **JAX:** A powerful framework that combines a NumPy-like API with AD and JIT compilation. Its core transformations (`grad`, `hessian`, `jit`, `vmap`) enable high-performance computational science, but require writing **pure functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exercises\n",
    "\n",
    "1.  **Richardson Extrapolation for the Second Derivative:** The central difference formula for the *second* derivative is $D_2(h) = \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$, which has $O(h^2)$ truncation error. Derive and implement the Richardson extrapolation for this formula to create an $O(h^4)$ accurate approximation. Test it on $f(x) = e^x$ at $x=1$.\n",
    "\n",
    "2.  **Symbolic vs. Automatic:** For the utility maximization problem, use `SymPy` to derive the analytical expressions for the gradient and the Hessian of the Lagrangian. Use `sp.lambdify` to turn these into numerical functions and verify that their output matches the results from JAX.\n",
    "\n",
    "3.  **Forward vs. Reverse Mode Jacobian:** In JAX, `jacfwd` computes Jacobians using forward-mode AD and `jacrev` uses reverse-mode AD. For a function $f: \\mathbb{R}^{1000} \\to \\mathbb{R}^2$, which do you expect to be faster? For a function $g: \\mathbb{R}^2 \\to \\mathbb{R}^{1000}$? Write two simple functions with these signatures and use `%timeit` to verify your hypothesis.\n",
    "\n",
    "4.  **Batch MLE with `vmap`:** Extend the MLE example. Suppose you have data from 10 different, independent experiments. Your data array now has shape `(10, 1000)`. Use `jax.vmap` to compute the MLE estimates for all 10 experiments in a single, efficient batch operation. The `in_axes` argument for `log_likelihood` should be `(None, 0)` to map over the data.\n",
    "\n",
    "5.  **Hessian of Utility:** For a CRRA utility function `u(c) = (c**(1-gamma))/(1-gamma)`, the second derivative `u''(c)` is related to the coefficient of absolute risk aversion. Use JAX's `grad` twice (`grad(grad(u))`) to compute the second derivative. Evaluate it for different values of `gamma` and `c` and interpret the sign."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

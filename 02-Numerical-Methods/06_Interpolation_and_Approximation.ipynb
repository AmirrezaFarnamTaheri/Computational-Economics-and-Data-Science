{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from numpy.polynomial import chebyshev\n",
    "from scipy.interpolate import CubicSpline, BarycentricInterpolator, RegularGridInterpolator, PchipInterpolator\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.special import roots_legendre\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Core Numerical Methods\n",
    "## Chapter 2.6: Interpolation and Function Approximation\n",
    "\n",
    "### Table of Contents\n",
    "1.  [Theoretical Foundations](#1.-Theoretical-Foundations)\n",
    "    *   [1.1 The Weierstrass Approximation Theorem](#1.1-The-Weierstrass-Approximation-Theorem)\n",
    "    *   [1.2 The Runge Phenomenon: A Cautionary Tale](#1.2-The-Runge-Phenomenon:-A-Cautionary-Tale)\n",
    "2.  [1D Function Approximation Methods](#2.-1D-Function-Approximation-Methods)\n",
    "    *   [2.1 Orthogonal Polynomials and Chebyshev Approximation](#2.1-Orthogonal-Polynomials-and-Chebyshev-Approximation)\n",
    "    *   [2.2 Spline Interpolation and Shape Preservation](#2.2-Spline-Interpolation-and-Shape-Preservation)\n",
    "3.  [Numerical Integration: Gaussian Quadrature](#3.-Numerical-Integration:-Gaussian-Quadrature)\n",
    "4.  [Application: Solving for Optimal Policy Functions](#4.-Application:-Solving-for-Optimal-Policy-Functions)\n",
    "5.  [The Curse of Dimensionality and Sparse Grids](#5.-The-Curse-of-Dimensionality-and-Sparse-Grids)\n",
    "6.  [Chapter Summary](#6.-Chapter-Summary)\n",
    "7.  [Exercises](#7.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Solving Models by Filling in the Gaps\n",
    "\n",
    "In many economic models, particularly in dynamic programming, we cannot solve for the value function or policy function analytically. Instead, solving the Bellman equation via methods like value function iteration gives us the value of the function only at a discrete set of points on a state-space grid. To find the optimal action at a state that falls *between* these grid points, we need a way to accurately \"fill in the gaps.\" This is the core task of **interpolation** and **function approximation**.\n",
    "\n",
    "This notebook introduces the robust, professional methods that form the backbone of modern computational economics. We will see how naive approaches can fail spectacularly and explore the two workhorse methods for high-quality approximation: **Chebyshev regression** and **cubic spline interpolation**. We will also explore methods for multi-dimensional and unstructured data, grounding our exploration in core economic applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Theoretical Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 The Weierstrass Approximation Theorem\n",
    "\\nDeveloped by Karl Weierstrass in the late 19th century, this theorem is a foundational result in analysis, confirming that polynomials are 'flexible' enough to model any continuous process over a finite domain.\\n",
    "Before attempting to approximate a function, we should ask: is a good approximation even possible? The **Weierstrass Approximation Theorem** provides the fundamental justification. It states that for any continuous function $f(x)$ defined on a closed interval $[a, b]$, there exists a sequence of polynomials that converges uniformly to $f(x)$.\n",
    "\n",
    "In essence, the theorem guarantees that we can get arbitrarily close to any continuous function with a polynomial of a sufficiently high degree. This gives us the theoretical license to pursue polynomial approximation as a valid strategy. The challenge, as we will see, is how to construct this polynomial in a stable and efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 The Runge Phenomenon: A Cautionary Tale\n",
    "\\nFirst discovered by Carl Runge in 1901, this phenomenon was a major surprise. It demonstrated that the intuitive idea of 'more data points should give a better fit' is not always true for polynomial interpolation, and that the *location* of the points is just as important as their number.\\n",
    "A natural first thought for approximating a function is to fit a single, high-degree polynomial that passes through all our known data points. This approach has a fatal flaw, first identified by Carl Runge. He showed that for many simple, well-behaved functions, as you increase the degree of the interpolating polynomial on *evenly-spaced nodes*, the approximation develops wild oscillations near the endpoints of the interval. This is known as the **Runge phenomenon**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Runge's Phenomenon](../images/02-Numerical-Methods/runge_phenomenon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1D Function Approximation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Orthogonal Polynomials and Chebyshev Approximation\n",
    "The Runge phenomenon is caused by using evenly-spaced nodes. The cure is to use nodes that are clustered near the boundaries of the interval. The optimal choice for this is the **Chebyshev nodes**.\n",
    "\n",
    "Chebyshev polynomials are part of a broader class of **orthogonal polynomials**. Two polynomials $P_n(x)$ and $P_m(x)$ are orthogonal with respect to a weight function $w(x)$ on an interval $[a, b]$ if their inner product is zero: $\\int_a^b P_n(x) P_m(x) w(x) dx = 0$ for $n \\ne m$. This orthogonality is analogous to the orthogonality of vectors. When we use the values of orthogonal polynomials as basis functions in a regression, the resulting system of equations is well-conditioned, leading to a stable and accurate fit. This is the foundation of **Chebyshev regression**.",
    "\\n**Intuition:** The Chebyshev nodes are the projections of equally spaced points on a semicircle down to the diameter. This bunching at the endpoints counteracts the tendency of polynomials to oscillate wildly in those regions, effectively 'pinning down' the ends of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chebyshev Approximation of Runge's Function](../images/02-Numerical-Methods/chebyshev_approximation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Spline Interpolation and Shape Preservation\n",
    "An alternative to a single high-degree polynomial is to use many low-degree polynomials to connect points. \n",
    "- **Cubic Spline:** A series of piecewise cubic polynomials connected such that the entire curve is twice continuously differentiable. This provides excellent smoothness but can overshoot the data.\n",
    "- **PCHIP (Piecewise Cubic Hermite Interpolating Polynomial):** A special type of spline that is guaranteed to preserve the monotonicity of the original data points. This is essential when approximating objects like CDFs or monotonic value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is most apparent when interpolating data with sharp changes or flat regions. A standard cubic spline will tend to 'overshoot' the data to maintain smoothness, while PCHIP prioritizes preserving the shape (monotonicity) of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cubic Spline vs. PCHIP for Monotonic Data](../images/02-Numerical-Methods/spline_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Numerical Integration: Gaussian Quadrature\n",
    "A problem closely related to function approximation is **numerical integration** (or quadrature): approximating the value of a definite integral $\\int_a^b f(x) dx$. Simple methods like the Trapezoidal rule can be slow to converge. **Gaussian Quadrature** is a far more powerful and efficient method.\n",
    "\n",
    "The key insight is that by strategically choosing the locations (nodes) where we evaluate the function, we can achieve a much higher degree of accuracy. Gaussian quadrature chooses the $n$ evaluation nodes to be the roots of the $n$-th degree Legendre polynomial (a class of orthogonal polynomials). With this choice of nodes, an $n$-point Gaussian quadrature rule can exactly integrate any polynomial of degree $2n-1$ or less. This is dramatically more accurate than a simple Riemann sum with the same number of function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Gaussian Quadrature vs. Trapezoidal Rule\")\n",
    "f = lambda x: np.exp(np.sin(x))\n",
    "a, b = 0, np.pi\n",
    "true_integral = 3.9774632605\n",
    "n_points = 5\n",
    "\n",
    "# 1. Trapezoidal Rule\n",
    "x_trap = np.linspace(a, b, n_points)\n",
    "integral_trap = np.trapz(f(x_trap), x_trap)\n",
    "\n",
    "# 2. Gaussian Quadrature\n",
    "nodes, weights = roots_legendre(n_points)\n",
    "# Scale nodes and weights from [-1, 1] to [a, b]\n",
    "nodes_scaled = 0.5*(b-a)*nodes + 0.5*(b+a)\n",
    "weights_scaled = 0.5*(b-a)*weights\n",
    "integral_gauss = np.sum(weights_scaled * f(nodes_scaled))\n",
    "\n",
    "note(f\"For n={n_points} points:\")\n",
    "print(f\"Trapezoidal Rule: Integral = {integral_trap:.8f}, Error = {abs(integral_trap - true_integral):.2e}\")\n",
    "print(f\"Gaussian Quadrature: Integral = {integral_gauss:.8f}, Error = {abs(integral_gauss - true_integral):.2e}\")\n",
    "note(\"Gaussian Quadrature is orders of magnitude more accurate for the same number of function evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Application: Solving for Optimal Policy Functions\n",
    "A core application of function approximation is solving dynamic programming problems. Consider a simple consumption-savings model where an agent lives for `T` periods, has utility $u(c) = \\log(c)$, and wants to maximize lifetime utility. The Bellman equation is:\n",
    "$$ V_t(W_t) = \\max_{c_t} \\left\\{ \\log(c_t) + \\beta V_{t+1}(W_{t+1}) \\right\\} $$\n",
    "We solve this backwards. At each period $t$, we need a continuous approximation of the next period's value function, $\\hat{V}_{t+1}$. We use this to solve for the optimal consumption $c_t^*(W)$ for a grid of wealth levels. We then create a continuous approximation of this **policy function**, $\\hat{c}_t^*(W)$, to use in the next step of the backward induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimal Consumption Policy Functions Over Time](../images/02-Numerical-Methods/optimal_consumption_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The Curse of Dimensionality and Sparse Grids\n",
    "The number of grid points required for grid-based interpolation grows exponentially with the number of dimensions. This is the **curse of dimensionality**. A 10-point grid in 1D requires 10 function evaluations. In 5D, a full tensor grid requires $10^5 = 100,000$ points. This is computationally infeasible.\n",
    "\n",
    "**Sparse grids** provide a powerful method for mitigating this curse. A Smolyak sparse grid is constructed by taking a clever linear combination of tensor-product grids at different levels of resolution. The result is a grid that has far fewer points than a full tensor grid but still maintains good accuracy, especially for smooth functions where interactions between dimensions are not too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing a Smolyak Sparse Grid vs. a Tensor Grid\")\n",
    "from itertools import product\n",
    "\n",
    "# Simple implementation for visualization purposes\n",
    "def cheb_nodes_1d(n): return np.cos(np.pi * (2*np.arange(1, n+1) - 1) / (2*n))\n",
    "\n",
    "level = 4\n",
    "tensor_nodes = cheb_nodes_1d(2**level + 1)\n",
    "tx, ty = np.meshgrid(tensor_nodes, tensor_nodes)\n",
    "\n",
    "smolyak_x, smolyak_y = [], []\n",
    "for i in range(1, level + 2):\n",
    "    nodes_i = cheb_nodes_1d(2**(i-1)+1 if i>1 else 1)\n",
    "    for j in range(1, level + 2 - i):\n",
    "        nodes_j = cheb_nodes_1d(2**(j-1)+1 if j>1 else 1)\n",
    "        for p in product(nodes_i, nodes_j):\n",
    "            smolyak_x.append(p[0]); smolyak_y.append(p[1])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.plot(tx, ty, 'bo', ms=4); ax1.set_title(f'Tensor Grid ({(2**level+1)**2} points)')\n",
    "ax2.plot(smolyak_x, smolyak_y, 'ro', ms=4); ax2.set_title(f'Smolyak Grid ({len(smolyak_x)} points)')\n",
    "for ax in [ax1, ax2]: ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Chapter Summary\n",
    "- **Theoretical Backing:** The **Weierstrass Theorem** guarantees that polynomial approximation is possible. However, naive interpolation on evenly-spaced nodes fails due to the **Runge phenomenon**.\n",
    "- **1D Workhorses:** For high-accuracy global approximation, **Chebyshev regression** (using orthogonal polynomials on Chebyshev nodes) is the gold standard. For local, smooth, and shape-preserving interpolation, **cubic splines** and **PCHIP** are preferred.\n",
    "- **Numerical Integration:** **Gaussian Quadrature** provides a highly efficient and accurate method for numerical integration by strategically placing evaluation nodes at the roots of orthogonal polynomials.\n",
    "- **Dynamic Programming:** A primary application of these methods is solving for **value functions** and **policy functions** in dynamic models via backward induction.\n",
    "- **Curse of Dimensionality:** For problems with more than a few state variables, full grid-based methods become infeasible. **Sparse grids** offer a powerful way to mitigate this curse, and even more advanced methods exist for higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exercises\n",
    "\n",
    "1.  **Chebyshev Implementation:** Write a function `chebyshev_nodes(n, a, b)` that computes the `n` Chebyshev nodes on an arbitrary interval `[a, b]`. Use it to find the nodes for approximating a function on `[-5, 5]` with 20 nodes.\n",
    "\n",
    "2.  **Impact of Curvature:** In the dynamic savings problem, the utility function's curvature is governed by risk aversion. Re-solve the model using a CRRA utility function $u(c) = \\frac{c^{1-\\gamma}-1}{1-\\gamma}$ for `gamma=2` and `gamma=5`. Plot the resulting *policy functions*. How does higher risk aversion (more curvature) affect the optimal consumption policy? Provide an economic intuition.\n",
    "\n",
    "3.  **Gaussian Quadrature Error:** Write a script to compute the integral of $f(x) = e^x$ from 0 to 1 using Gaussian Quadrature. Plot the absolute error of the approximation as a function of the number of quadrature points `n` (from 1 to 10). What do you observe about the rate of convergence?\n",
    "\n",
    "4.  **Extrapolation Dangers:** Using the `cheb_approx` and `spline_approx` objects from the notebook, evaluate and plot their approximations on a wider domain, from `x = -1.5` to `x = 1.5`. How do the approximations behave outside the original interpolation range `[-1, 1]`? Which method appears more dangerously wrong in extrapolation?\n",
    "\n",
    "5.  **2D Interpolation:** A firm's profit function is given by $\\pi(p, w) = (100 - 2p)p - 5w(100-2p)^{0.5}$, where `p` is price and `w` is the wage. You have evaluated this function on a coarse 10x10 grid for `p` in `[10, 40]` and `w` in `[5, 20]`. Use `RegularGridInterpolator` to create a smooth approximation of the profit surface and find the price and wage that maximize profits on your interpolated surface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

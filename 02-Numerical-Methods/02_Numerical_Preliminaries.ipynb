{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, struct\n",
    "from decimal import Decimal, getcontext\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.linalg import hilbert\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=8)\n",
    "getcontext().prec = 50 # Set precision for Decimal objects\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Core Numerical Methods\n",
    "## Chapter 2.2: Numerical Preliminaries: The World of Machine Numbers\n",
    "\n",
    "### Table of Contents\n",
    "1.  [How Computers Represent Numbers](#1.-How-Computers-Represent-Numbers)\n",
    "    *   [1.1 IEEE 754 Floating-Point Representation](#1.1-IEEE-754-Floating-Point-Representation)\n",
    "    *   [1.2 Machine Epsilon and the Spacing of Floats](#1.2-Machine-Epsilon-and-the-Spacing-of-Floats)\n",
    "    *   [1.3 Special Values: `inf`, `-inf`, and `NaN`](#1.3-Special-Values:-inf,-inf,-and-NaN)\n",
    "2.  [A Taxonomy of Numerical Error](#2.-A-Taxonomy-of-Numerical-Error)\n",
    "    *   [2.1 Formal Definitions: Absolute vs. Relative Error](#2.1-Formal-Definitions:-Absolute-vs.-Relative-Error)\n",
    "    *   [2.2 Rounding Error vs. Truncation Error](#2.2-Rounding-Error-vs.-Truncation-Error)\n",
    "3.  [Diagnosing Problems: Conditioning and Stability](#3.-Diagnosing-Problems:-Conditioning-and-Stability)\n",
    "    *   [3.1 Conditioning: A Property of the Problem](#3.1-Conditioning:-A-Property-of-the-Problem)\n",
    "    *   [3.2 Stability: A Property of the Algorithm](#3.2-Stability:-A-Property-of-the-Algorithm)\n",
    "4.  [Arbitrary-Precision Arithmetic](#4.-Arbitrary-Precision-Arithmetic)\n",
    "5.  [Rates of Convergence](#5.-Rates-of-Convergence)\n",
    "6.  [Algorithmic Complexity and Efficiency](#6.-Algorithmic-Complexity-and-Efficiency)\n",
    "    *   [6.1 Big-O Notation](#6.1-Big-O-Notation)\n",
    "    *   [6.2 Amortized Analysis](#6.2-Amortized-Analysis)\n",
    "7.  [Chapter Summary](#7.-Chapter-Summary)\n",
    "8.  [Exercises](#8.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: From Mathematical Theory to Machine Implementation\n",
    "\n",
    "Every empirical result, every simulated model, and every computed equilibrium rests upon the system by which computers represent and manipulate numbers. This architecture is not the clean, infinite world of pure mathematics; it is a finite, practical system with inherent limitations and potential pitfalls. Understanding this system is a prerequisite for producing reliable and trustworthy research.\n",
    "\n",
    "This chapter provides the essential foundations, bridging the gap between abstract mathematical theory and its concrete implementation in hardware. We will explore:\n",
    "*   **Floating-Point Representation:** How the IEEE 754 standard encodes real numbers into a finite binary format.\n",
    "*   **A Taxonomy of Error:** Differentiating between **rounding error** (from finite precision) and **truncation error** (from approximating infinite processes).\n",
    "*   **Algorithmic Stability:** Diagnosing pathologies like **catastrophic cancellation** and distinguishing between the **conditioning** of a problem and the **stability** of an algorithm.\n",
    "*   **Rates of Convergence:** A formal language for comparing the speed of iterative algorithms.\n",
    "*   **Algorithmic Complexity:** Using **Big-O notation** to measure and compare the efficiency of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How Computers Represent Numbers\n",
    "\n",
    "The core challenge of numerical computation is that computers use a finite number of bits to represent the infinite continuum of real numbers. This conversion is the source of many peculiarities and potential pitfalls in numerical work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 IEEE 754 Floating-Point Representation\n",
    "The **IEEE 754 standard** is the ubiquitous system for floating-point arithmetic. A standard 64-bit float (a `float` in Python or a `float64` in NumPy) is composed of three parts:\n",
    "1.  **Sign Bit (1 bit):** The first bit determines if the number is positive (0) or negative (1).\n",
    "2.  **Exponent (11 bits):** This component determines the magnitude (the range) of the number.\n",
    "3.  **Mantissa or Significand (52 bits):** This component determines the precision (the number of significant digits).\n",
    "\n",
    "A number is represented in a form of scientific notation: `sign * mantissa * 2^exponent`. The critical consequence of this binary, finite representation is that numbers with a finite decimal representation (like `0.1`) may not have a finite binary representation.",
    "\\n\\nThe standard was a crucial development, led by William Kahan in the 1980s (earning him the Turing Award), that brought much-needed consistency to scientific computing across different hardware platforms. Before IEEE 754, different computer manufacturers had their own proprietary and often incompatible floating-point formats, making scientific code notoriously difficult to port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing the Bits of a Float\")\n",
    "\n",
    "def float_to_bits(f):\n",
    "    \"\"\"Unpacks a 64-bit float into its sign, exponent, and mantissa bits.\"\"\"\n",
    "    packed = struct.pack('!d', f)\n",
    "    integer_representation = struct.unpack('!Q', packed)[0]\n",
    "    sign = (integer_representation >> 63) & 1\n",
    "    exponent = (integer_representation >> 52) & 0x7FF\n",
    "    mantissa = integer_representation & 0xFFFFFFFFFFFFF\n",
    "    return sign, exponent, mantissa, bin(integer_representation).lstrip('0b').zfill(64)\n",
    "\n",
    "s, e, m, b = float_to_bits(19.94)\n",
    "note(f\"The float 19.94 is represented by these components:\")\n",
    "print(f\"  Sign: {s}\")\n",
    "print(f\"  Exponent: {e} (binary: {e:011b})\")\n",
    "print(f\"  Mantissa: {m} (binary: {m:052b})\")\n",
    "print(f\"Full 64-bit representation:\\n  {b[0]} {b[1:12]} {b[12:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Machine Epsilon and the Spacing of Floats\n",
    "\n",
    "**Machine Epsilon** ($\"epsilon\"_{mach}$) is the distance from 1.0 to the next largest representable floating-point number. It quantifies the maximum possible relative error when representing a real number as a float. For 64-bit floats, $\"epsilon\"_{mach}$ is $2^{-52}$, approximately $2.22 \\times 10^{-16}$.\n",
    "\n",
    "A crucial and often misunderstood fact is that the spacing between floating-point numbers is **not uniform**. The absolute gap between representable numbers increases as their magnitude increases. There are as many representable numbers between 1.0 and 2.0 as there are between 1,024.0 and 2,048.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Machine Epsilon and Non-Uniform Spacing\")\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "note(f\"NumPy's reported machine epsilon for 64-bit floats is: {eps:.2e}\")\n",
    "note(f\"The next representable float after 1.0 is: {np.nextafter(1.0, 2.0)}\")\n",
    "\n",
    "# Demonstrate non-uniform spacing\n",
    "gap_at_1 = np.nextafter(1.0, 2.0) - 1.0\n",
    "gap_at_1024 = np.nextafter(1024.0, 1025.0) - 1024.0\n",
    "print(f\"\\nSpacing after 1.0:    {gap_at_1:.2e}\")\n",
    "print(f\"Spacing after 1024.0: {gap_at_1024:.2e} (which is {gap_at_1024/gap_at_1:.0f} times larger)\")\n",
    "\n",
    "# Visualize the spacing\n",
    "x = np.array([0.125, 0.25, 0.5, 1, 2, 4])\n",
    "y = np.nextafter(x, 2*x) - x\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(x, y, 'o-', label='Gap to next float')\n",
    "plt.xlabel('Number (log scale)')\n",
    "plt.ylabel('Gap Size (log scale)')\n",
    "plt.title('Floating-Point Spacing is Proportional to Magnitude')\n",
    "plt.legend(); plt.grid(True, which='both', ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Floating-point spacing increases with magnitude](../images/02-Numerical-Methods/float_spacing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Special Values: `inf`, `-inf`, and `NaN`\n",
    "The IEEE 754 standard defines special bit patterns for non-finite numbers:\n",
    "- **Infinity (`inf`)**: Represents values that have overflowed, e.g., `1 / 0` or `exp(1000)`. It behaves as expected in comparisons (`inf > 1e12`).\n",
    "- **Negative Infinity (`-inf`)**: The negative counterpart.\n",
    "- **Not a Number (`NaN`)**: Represents the result of an undefined operation, such as `0 / 0` or `inf - inf`. `NaN` is unique in that it is **not equal to anything, including itself**. Therefore, checking for `NaN` must be done with `np.isnan()`, not `x == np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A Taxonomy of Numerical Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Formal Definitions: Absolute vs. Relative Error\n",
    "Let $x$ be the true value and $\\hat{x}$ be its approximation.\n",
    "- **Absolute Error:** $E_{abs} = |x - \\hat{x}|$. This is useful when the magnitude of the value is known.\n",
    "- **Relative Error:** $E_{rel} = \\frac{|x - \\hat{x}|}{|x|}$. This is often more meaningful as it is scale-independent. A 1cm error is large for a molecule but tiny for a planet.\n",
    "\n",
    "These concepts directly map to the tolerance parameters used in numerical libraries: `atol` (absolute tolerance) and `rtol` (relative tolerance). A convergence test like `abs(x - x_hat) <= atol + rtol * abs(x_hat)` is robust across different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Rounding Error vs. Truncation Error\n",
    "1.  **Rounding Error:** The error from using finite-precision numbers to represent real numbers. This is an inherent limitation of the hardware.\n",
    "2.  **Truncation Error:** The error from approximating an infinite process with a finite one. This is an error of the algorithm. For example, using a finite number of terms from a Taylor series to approximate a function, or stopping an iterative solver after a finite number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Truncation error in a Taylor series approximation](../images/02-Numerical-Methods/taylor_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Diagnosing Problems: Conditioning and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conditioning: A Property of the Problem\n",
    "**Conditioning** is an inherent property of the *problem* itself, regardless of how it is solved. A problem is **ill-conditioned** if a small relative change in the input can lead to a large relative change in the output. The **Hilbert matrix** is a classic example of a severely ill-conditioned matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing Ill-Conditioning\")\n",
    "A = hilbert(10)\n",
    "b = np.ones(10)\n",
    "x = np.linalg.solve(A, b)\n",
    "b_perturbed = b + 1e-7 * np.random.randn(10)\n",
    "x_perturbed = np.linalg.solve(A, b_perturbed)\n",
    "\n",
    "note(f\"Condition number of A: {np.linalg.cond(A):.2e}\")\n",
    "note(f\"Relative error in input b: {np.linalg.norm(b - b_perturbed) / np.linalg.norm(b):.2e}\")\n",
    "note(f\"Relative error in output x: {np.linalg.norm(x - x_perturbed) / np.linalg.norm(x):.2e}\")\n",
    "note(\"A tiny perturbation in the input data caused a massive, amplified error in the solution vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Stability: A Property of the Algorithm\n",
    "**Stability** is a property of the *algorithm* used to solve the problem. An algorithm is **numerically unstable** if it amplifies the small rounding errors that are inherent in floating-point arithmetic. A stable algorithm produces a result that is the *exact* solution to a problem with slightly perturbed inputs (a property known as **backward stability**).\n",
    "\n",
    "A classic example is **catastrophic cancellation**, which occurs when you subtract two numbers that are very close to each other. The leading, most significant digits cancel out, leaving only the trailing digits which are dominated by rounding errors. This causes a massive loss of relative precision.\n",
    "\n",
    "Another common example is the naive, one-pass algorithm for calculating variance: $Var(X) = E[X^2] - (E[X])^2$. This is unstable when the standard deviation is small relative to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Algorithmic Instability in Variance Calculation\")\n",
    "rng = np.random.default_rng(42)\n",
    "mean = 1e9\n",
    "std_dev = 1.0\n",
    "n_samples = 1_000_000\n",
    "\n",
    "# Data with large mean and small variance\n",
    "x = rng.normal(loc=mean, scale=std_dev, size=n_samples).astype(np.float32) # Use float32 to exacerbate error\n",
    "\n",
    "# Unstable one-pass algorithm\n",
    "var_unstable = np.mean(x**2) - np.mean(x)**2\n",
    "\n",
    "# Stable two-pass algorithm (or built-in)\n",
    "var_stable = np.var(x)\n",
    "\n",
    "note(f\"True variance is approximately: {std_dev**2:.4f}\")\n",
    "note(f\"Stable algorithm result:    {var_stable:.4f}\")\n",
    "note(f\"Unstable algorithm result:  {var_unstable:.4f} (This can even be negative!)\")\n",
    "note(\"The unstable algorithm subtracts two very large, nearly equal numbers, causing catastrophic cancellation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Arbitrary-Precision Arithmetic\n",
    "\n",
    "When standard 64-bit precision is insufficient, Python's `decimal` module provides a solution. It allows you to specify a desired precision and performs calculations in a way that mirrors how humans do arithmetic, avoiding the binary representation issues of floats. This is essential for financial applications where rounding rules must be exact, but it comes at a significant performance cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"The Need for Decimal Arithmetic\")\n",
    "\n",
    "note(\"Standard floats can't represent 0.1 exactly:\")\n",
    "val_float = 0.1 + 0.1 + 0.1 - 0.3\n",
    "print(f\"0.1 + 0.1 + 0.1 - 0.3 (using floats) = {val_float:.20e}\")\n",
    "\n",
    "note(\"The decimal module performs as expected:\")\n",
    "getcontext().prec = 50\n",
    "val_decimal = Decimal('0.1') + Decimal('0.1') + Decimal('0.1') - Decimal('0.3')\n",
    "print(f\"0.1 + 0.1 + 0.1 - 0.3 (using Decimal) = {val_decimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Rates of Convergence\n",
    "When using iterative methods (e.g., for root finding or optimization), it is crucial to understand how quickly the sequence of approximations approaches the true solution. This is measured by the **rate of convergence**.\n",
    "Let $x_k$ be the sequence of iterates and $x^*$ be the true solution. Let $\\epsilon_k = |x_k - x^*|$ be the error at iteration $k$. Then:\n",
    "- **Linear Convergence:** $\\lim_{k \\to \\infty} \\frac{\\epsilon_{k+1}}{\\epsilon_k} = C$, where $0 < C < 1$. (e.g., Bisection method).\n",
    "- **Quadratic Convergence:** $\\lim_{k \\to \\infty} \\frac{\\epsilon_{k+1}}{\\epsilon_k^2} = C$. The number of correct significant digits roughly doubles at each iteration. This is extremely fast. (e.g., Newton's method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison of linear and quadratic convergence rates](../images/02-Numerical-Methods/convergence_rates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Algorithmic Complexity and Efficiency\n",
    "\n",
    "#### 6.1 Big-O Notation\n",
    "**Big-O notation** provides a formal language for discussing how an algorithm's runtime or space requirements scale with the size of the input, $n$. It focuses on the **order of growth** in the worst-case scenario, ignoring constant factors and lower-order terms.",
    "\\nHere are some concrete examples:\\n* **O(1):** Accessing an element in an array by its index.\\n* **O(log n):** Binary search in a sorted array.\\n* **O(n):** Finding the maximum value in an unsorted list.\\n* **O(n log n):** Efficient sorting algorithms like Merge Sort or Quick Sort.\\n* **O(n^2):** Comparing every element to every other element (e.g., a naive nearest-neighbor search).\\n* **O(2^n):** Solving the traveling salesman problem with a brute-force approach. These algorithms become infeasible very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Growth rates of common Big-O complexity classes](../images/02-Numerical-Methods/big_o_complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Amortized Analysis\n",
    "\n",
    "Sometimes, an operation may be very expensive on rare occasions but cheap the rest of the time. **Amortized analysis** averages the cost over a sequence of operations. A classic example is appending to a dynamic array (like a Python `list`).\n",
    "\n",
    "- Most `append` calls are $O(1)$‚Äîthey just add an element to a pre-allocated block of memory.\n",
    "- Occasionally, the list runs out of space. It must then allocate a new, larger block of memory (e.g., 2x the size) and copy all existing elements over. This single operation is $O(n)$.\n",
    "\n",
    "However, because the expensive resizing happens infrequently, the *amortized* cost of each `append` operation averages out to $O(1)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\n
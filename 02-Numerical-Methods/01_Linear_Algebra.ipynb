{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "from scipy.linalg import lu, lu_factor, lu_solve, solve_triangular, cholesky, qr, svd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.datasets import face\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Polygon, Arrow\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=120)\n",
    "sp.init_printing(use_unicode=True)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Core Numerical Methods\n",
    "## Chapter 2.1: Linear Algebra for Economists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: The Language of Economic Systems\n",
    "\n",
    "Linear algebra is the fundamental language used to express, analyze, and solve the interconnected systems that define modern economics. While microeconomics often begins with the study of a single agent or market, macroeconomics and econometrics are fundamentally concerned with systems of equations. From the market-clearing conditions of a general equilibrium model to the vast datasets of empirical work, linear algebra provides the tools to reason about entire systems through the geometry of vectors and matrices, rather than getting lost in a sea of individual equations.\n",
    "\n",
    "This chapter provides a practical guide to the core concepts of linear algebra, focusing on geometric intuition and computational best practices. We will explore:\n",
    "*   **Vector Spaces and Subspaces:** The abstract structures that govern vectors.\n",
    "*   **The Four Fundamental Subspaces:** The cornerstone theorem linking a matrix's key spaces.\n",
    "*   **Matrices as Transformations:** Understanding matrices as functions that transform vector spaces.\n",
    "*   **Solving Systems:** The theory and robust methods for solving `Ax = b`, the workhorse of economic modeling.\n",
    "*   **The Condition Number:** A critical measure of the numerical sensitivity of a system.\n",
    "*   **Matrix Decompositions:** Unpacking matrix structure via LU, QR, Cholesky, and Eigendecomposition to solve problems efficiently and stably.\n",
    "*   **Economic Applications:** Grounding these concepts in classic problems like the Leontief input-output model, OLS, and the stability of dynamic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vector Spaces, Subspaces, and Bases\n",
    "\n",
    "A **vector space** is a collection of objects called vectors, which can be added together and multiplied by scalars. For a set to be a vector space, it must satisfy a specific set of axioms (closure under addition and scalar multiplication, existence of a zero vector, etc.). The familiar n-dimensional real space, $\\mathbb{R}^n$, is the canonical example.\n",
    "\n",
    "A **subspace** is a subset of a vector space that is itself a vector space. Crucially, it must contain the zero vector and be closed under vector addition and scalar multiplication. For example, a plane passing through the origin is a subspace of $\\mathbb{R}^3$.\n",
    "\n",
    "- **Linear Independence:** A set of vectors $\\{v_1, ..., v_k\\}$ is linearly independent if the only solution to $c_1v_1 + ... + c_kv_k = 0$ is $c_1 = ... = c_k = 0$. Intuitively, no vector in the set can be written as a linear combination of the others.\n",
    "- **Span:** The span of a set of vectors is the set of all possible linear combinations of those vectors. The span of a set of vectors always forms a vector space (or a subspace).\n",
    "- **Basis:** A basis for a vector space is a set of linearly independent vectors that span the space. A basis provides a unique coordinate system for the space.\n",
    "- **Dimension:** The dimension of a vector space is the number of vectors in any basis for that space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Four Fundamental Subspaces\n",
    "\n",
    "For any $m \\times n$ matrix $A$, there are four fundamental subspaces that describe its properties. The **Fundamental Theorem of Linear Algebra** connects their dimensions and orthogonality.\n",
    "\n",
    "1.  **Column Space, $C(A)$:** The span of the columns of $A$. It is a subspace of $\\mathbb{R}^m$. Its dimension, $r$, is the **rank** of the matrix. The system $Ax=b$ is solvable if and only if $b$ is in $C(A)$.\n",
    "2.  **Null Space, $N(A)$:** The set of all vectors $x$ such that $Ax = 0$. It is a subspace of $\\mathbb{R}^n$. Its dimension is $n-r$.\n",
    "3.  **Row Space, $C(A^T)$:** The span of the rows of $A$. It is a subspace of $\\mathbb{R}^n$. Its dimension is also $r$.\n",
    "4.  **Left Null Space, $N(A^T)$:** The set of all vectors $y$ such that $A^T y = 0$. It is a subspace of $\\mathbb{R}^m$. Its dimension is $m-r$.\n",
    "\n",
    "**Orthogonality:**\n",
    "- The row space is orthogonal to the null space ($C(A^T) \\perp N(A)$).\n",
    "- The column space is orthogonal to the left null space ($C(A) \\perp N(A^T)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Four Fundamental Subspaces of a Matrix](../images/02-Numerical-Methods/four_fundamental_subspaces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrices as Linear Transformations\n",
    "\n",
    "A matrix $A \\in \\mathbb{R}^{m \\times n}$ is a recipe for a **linear transformation**‚Äîa function that maps vectors from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ while preserving the rules of vector addition and scalar multiplication. Geometrically, this means grid lines remain parallel and evenly spaced, and the origin stays fixed.\n",
    "\n",
    "The **determinant** of a square matrix measures the factor by which the area (in 2D) or volume (in 3D) of a region changes after the transformation. A determinant of 0 means the transformation squashes space into a lower dimension (e.g., a plane into a line), and the matrix is **singular** (not invertible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visualizing a matrix as a linear transformation](../images/02-Numerical-Methods/matrix_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solving Systems of Linear Equations: `Ax = b`\n",
    "The problem of solving `Ax = b` is central to economics. Geometrically, it means asking: **\"What vector `x`, when transformed by `A`, lands on vector `b`?\"**\n",
    "A solution exists if and only if `b` is in the column space of `A`. If `A` is square and non-singular, a unique solution exists for any `b`.\n",
    "\n**Best Practice: `solve` is Better Than `inv`**\n",
    "A common mistake is to solve the system by computing `x = inv(A) @ b`. This is less accurate and slower than `np.linalg.solve(A, b)`, which uses stable and efficient matrix decompositions (like LU factorization) under the hood. The difference is negligible for small matrices but becomes significant for larger systems. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Proof: `solve` is Superior to `inv`\")\n",
    "N = 500\n",
    "A = np.random.default_rng(123).random((N, N))\n",
    "b = np.random.default_rng(123).random(N)\n",
    "solve_time = timeit.timeit(lambda: np.linalg.solve(A, b), number=10)\n",
    "inv_time = timeit.timeit(lambda: np.linalg.inv(A) @ b, number=10)\n",
    "note(f'`np.linalg.solve` took {solve_time:.4f} seconds.\\n`np.linalg.inv` took {inv_time:.4f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application: The Leontief Input-Output Model\n",
    "\\nThe Leontief model, developed by Wassily Leontief (who won a Nobel Prize in 1973 for this work), is a foundational tool for understanding inter-industry relationships. It represents the economy as a system of linear equations, making it a perfect application for the numerical methods we are discussing.\\n",
    "The Leontief model describes the interdependencies between sectors in an economy. If $A$ is the technology matrix where $A_{ij}$ is the value of input from sector $i$ needed to produce one unit of output in sector $j$, and $d$ is the final demand vector, then the gross output $x$ required to meet this demand is given by:\n",
    "$$ x = Ax + d \\implies (I - A)x = d $$\n",
    "We need to solve this system for $x$. Since an economy's parameters might be analyzed under many different final demand scenarios ($d_1, d_2, ...$), this is a perfect case for using an efficient solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Solving the Leontief Input-Output Model\")\n",
    "# A: Technology Matrix (rows: input sector, cols: output sector)\n",
    "# e.g., A[0,1] = 0.2 means Manuf needs 0.2 units from Ag for each unit of output\n",
    "A = np.array([[0.1, 0.2, 0.4],  # Inputs from Agriculture\n",
    "                [0.5, 0.1, 0.3],  # Inputs from Manufacturing\n",
    "                [0.1, 0.6, 0.1]]) # Inputs from Services\n",
    "I = np.eye(3)\n",
    "L = I - A # The Leontief Matrix\n",
    "\n",
    "# Decompose the Leontief matrix ONCE using lu_factor for efficiency.\n",
    "lu_factors, piv = lu_factor(L)\n",
    "\n",
    "note(\"The Leontief matrix (I - A) is LU-decomposed once for efficiency.\")\n",
    "\n",
    "def solve_leontief(d):\n",
    "    \"\"\"Solves the system for a given demand vector using the pre-computed LU factors.\"\"\"\n",
    "    return lu_solve((lu_factors, piv), d)\n",
    "\n",
    "# Scenario 1: High demand for services\n",
    "d1 = np.array([100, 200, 500])\n",
    "x1 = solve_leontief(d1)\n",
    "print(f\"For demand d1 = {d1}, required gross output x1 = {x1}\")\n",
    "\n",
    "# Scenario 2: High demand for manufacturing\n",
    "d2 = np.array([150, 600, 150])\n",
    "x2 = solve_leontief(d2)\n",
    "print(f\"For demand d2 = {d2}, required gross output x2 = {x2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stability and the Condition Number\n",
    "\n",
    "The **condition number** of a matrix `A`, denoted $\\kappa(A)$, measures how sensitive the solution of `Ax=b` is to small changes in `A` or `b`. A low condition number means the matrix is **well-conditioned**, and the solution is stable. A high condition number means the matrix is **ill-conditioned**, and small input errors can lead to large output errors.\n",
    "$$ \\kappa(A) = ||A|| \\cdot ||A^{-1}|| $$\n",
    "For a square matrix, $\\kappa(A) = |\\frac{\\lambda_{max}}{\\lambda_{min}}|$ where $\\lambda$ are the eigenvalues. For OLS, the relevant condition number is that of the matrix $X^T X$. Severe multicollinearity in the design matrix $X$ leads to a very high condition number, making the OLS estimates unstable and unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Condition Number and Multicollinearity\")\n",
    "rng = np.random.default_rng(42)\n",
    "n = 100\n",
    "x1 = rng.standard_normal(n)\n",
    "\n",
    "note(\"Case 1: Low correlation between x1 and x2\")\n",
    "x2_low_corr = 0.1 * x1 + rng.standard_normal(n) # Low correlation\n",
    "X_low = np.c_[np.ones(n), x1, x2_low_corr]\n",
    "cond_low = np.linalg.cond(X_low.T @ X_low)\n",
    "print(f\"Correlation: {np.corrcoef(x1, x2_low_corr)[0,1]:.2f}, Condition Number: {cond_low:.2f}\")\n",
    "\n",
    "note(\"Case 2: High correlation between x1 and x2\")\n",
    "x2_high_corr = 0.95 * x1 + 0.1 * rng.standard_normal(n) # High correlation\n",
    "X_high = np.c_[np.ones(n), x1, x2_high_corr]\n",
    "cond_high = np.linalg.cond(X_high.T @ X_high)\n",
    "print(f\"Correlation: {np.corrcoef(x1, x2_high_corr)[0,1]:.2f}, Condition Number: {cond_high:.2f}\")\n",
    "note(\"The extremely high condition number indicates that the OLS estimates will be very sensitive to small changes in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Special Matrices in Economics\n",
    "Certain types of matrices appear frequently in economic theory and econometrics due to their special properties.\n",
    "\n",
    "- **Symmetric Matrices ($A = A^T$):** Covariance matrices and Hessian matrices of second derivatives are always symmetric. Symmetric matrices have real eigenvalues and a full set of orthogonal eigenvectors.\n",
    "- **Positive Definite Matrices:** A symmetric matrix $A$ is positive definite if $x^T A x > 0$ for all non-zero vectors $x$. This is a crucial property for covariance matrices (variance must be positive) and for ensuring a minimum in optimization problems.\n",
    "- **Idempotent Matrices ($P^2 = P$):** Projection matrices, like the \"hat\" matrix in OLS ($P = X(X^TX)^{-1}X^T$), are idempotent. Applying a projection twice is the same as applying it once. Their eigenvalues are always 0 or 1.\n",
    "- **Orthogonal Matrices ($Q^T Q = I$):** These matrices represent rotations and reflections. They preserve vector lengths and angles. They are fundamental to the QR decomposition and SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Testing for Positive Definiteness\")\n",
    "\n",
    "note(\"A valid covariance matrix must be positive definite.\")\n",
    "valid_cov = np.array([[1, 0.5], [0.5, 1]])\n",
    "try:\n",
    "    # Cholesky decomposition only works for positive definite matrices\n",
    "    cholesky(valid_cov)\n",
    "    print(\"Matrix is positive definite (Cholesky succeeded).\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Matrix is not positive definite.\")\n",
    "\n",
    "note(\"An invalid covariance matrix is not positive definite.\")\n",
    "invalid_cov = np.array([[1, 1.1], [1.1, 1]]) # Correlation > 1\n",
    "try:\n",
    "    cholesky(invalid_cov)\n",
    "    print(\"Matrix is positive definite.\")\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"Matrix is not positive definite (Cholesky failed: {e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Eigenvalues and System Dynamics\n",
    "An **eigenvector** `v` of a square matrix `A` is a special non-zero vector that does not change direction when transformed by `A`. It is only scaled by a factor, its corresponding **eigenvalue** `Œª`: $Av = \\lambda v$.\n",
    "\n",
    "**Eigendecomposition** factors a matrix `A` into `A = PDP‚Åª¬π`, where `P` is the matrix of eigenvectors and `D` is the diagonal matrix of eigenvalues. This is invaluable for understanding dynamic systems. If a system is described by the linear difference equation `x_{t+1} = Ax_t`, its long-term behavior is governed by the eigenvalues of `A`:\n",
    "- If all $|\\lambda_i| < 1$, the system is **stable** and converges to the origin.\n",
    "- If any $|\\lambda_i| > 1$, the system is **unstable** and diverges.\n",
    "- If some $|\\lambda_i| < 1$ and some $|\\lambda_i| > 1$, the system has **saddle-path stability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phase portraits for dynamic systems with different eigenvalues](../images/02-Numerical-Methods/system_dynamics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The Singular Value Decomposition (SVD): The Master Tool\n",
    "\n",
    "The SVD is arguably the most powerful and general matrix decomposition. It states that *any* matrix $A \\in \\mathbb{R}^{m \\times n}$ can be factored as:\n",
    "$$ A = U \\Sigma V^T $$\n",
    "Where:\n",
    "- $U$ is an $m \\times m$ orthogonal matrix whose columns (left singular vectors) are an orthonormal basis for the output space $\\mathbb{R}^m$.\n",
    "- $\\Sigma$ is an $m \\times n$ diagonal matrix containing the **singular values** $\\sigma_i$ (which are non-negative and sorted).\n",
    "- $V^T$ is an $n \\times n$ orthogonal matrix whose rows (right singular vectors) are an orthonormal basis for the input space $\\mathbb{R}^n$.\n",
    "\n",
    "Geometrically, the SVD states that any linear transformation can be broken down into three fundamental operations: a rotation (by $V^T$), a scaling along orthogonal axes (by $\\Sigma$), and another rotation (by $U$). Its power comes from its generality‚Äîit applies to any matrix, square or not‚Äîand its ability to reveal the underlying structure of the data.\n",
    "\n",
    "#### Application: Principal Component Analysis (PCA)\n",
    "PCA is a cornerstone of dimensionality reduction. Given a dataset $X$, PCA finds the directions of maximum variance (the principal components). These components are precisely the right singular vectors ($V$) of the centered data matrix. The SVD provides a direct and numerically stable way to compute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Principal components of a dataset found via SVD](../images/02-Numerical-Methods/pca_svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Sparse Matrices in Economics\n",
    "\n",
    "Many economic models, especially those involving networks, panel data with fixed effects, or large-scale structural models, produce matrices that are very large but **sparse**‚Äîmeaning most of their elements are zero. Storing these matrices as dense NumPy arrays is incredibly inefficient in terms of both memory and computation.\n",
    "\n",
    "The `scipy.sparse` module provides data structures and algorithms optimized for sparse matrices. Instead of storing all $N \\times M$ elements, they only store the non-zero values and their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Memory and Speed of Sparse Matrices\")\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import timeit\n",
    "N = 10000\n",
    "diag = np.ones(N) * -2; off_diag = np.ones(N - 1)\n",
    "A_sparse = csr_matrix((np.concatenate([diag, off_diag, off_diag]), (np.concatenate([np.arange(N), np.arange(N-1), np.arange(1,N)]), np.concatenate([np.arange(N), np.arange(1,N), np.arange(N-1)]))), shape=(N, N))\n",
    "A_dense = A_sparse.toarray()\n",
    "b = np.random.rand(N)\n",
    "note(f'Memory of dense {N}x{N} matrix: {A_dense.nbytes / 1e6:.2f} MB')\n",
    "note(f'Memory of sparse matrix: {A_sparse.data.nbytes / 1e6:.2f} MB (plus overhead)')\n",
    "sparse_time = timeit.timeit(lambda: spsolve(A_sparse, b), number=10)\n",
    "dense_time = timeit.timeit(lambda: np.linalg.solve(A_dense, b), number=10)\n",
    "note(f'Sparse solver took {sparse_time:.4f} seconds.\\nDense solver took {dense_time:.4f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Chapter Summary\n",
    "\n",
    "- **Foundation:** Linear algebra is built on the concepts of vector spaces, subspaces, basis, and dimension. The **Four Fundamental Subspaces** provide a complete picture of a matrix's action.\n",
    "- **Geometric Intuition:** Matrices are linear transformations that rotate, scale, and shear space. The determinant measures the change in volume, and eigenvalues/eigenvectors describe the directions that are unchanged by the transformation.\n",
    "- **Solving Systems:** The workhorse problem `Ax=b` should be solved with decomposition-based methods like `np.linalg.solve` (which uses LU) or `scipy.linalg.lu_solve` for repeated solves. **Never use `inv(A)`**.\n",
    "- **Numerical Stability:** The **condition number** is a vital diagnostic for the sensitivity of `Ax=b`. For OLS, this reveals multicollinearity. The **QR decomposition** provides the most numerically stable method for solving least-squares problems.\n",
    "- **Decompositions are Key:** Understanding LU, QR, Cholesky, Eigendecomposition, and SVD is crucial. Each reveals a different aspect of the matrix and is suited to different problems (solving systems, stability analysis, dimensionality reduction).\n",
    "- **Sparsity:** Real-world economic models often generate huge, sparse matrices. Using `scipy.sparse` is essential for memory and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Exercises\n",
    "\n",
    "1.  **Proof of Eigenvalues:** Prove that the eigenvalues of a real, symmetric, idempotent matrix must be either 0 or 1.\n",
    "    - *Hint: Start with the definition $Px = \\lambda x$. Multiply both sides by $P$. Use the idempotent property $P^2=P$.*\n",
    "\n",
    "2.  **QR Decomposition for OLS:** A more stable way to solve the OLS problem is to use QR decomposition on the design matrix $X$. If $X=QR$, the normal equations $(X^T X)\\beta = X^T y$ become $(R^T Q^T Q R)\\beta = R^T Q^T y$, which simplifies to $R\\beta = Q^T y$ since $Q^T Q = I$. This is an upper-triangular system that can be solved efficiently with back substitution.\n",
    "    - **Task:** Using the data from the condition number example, compute the QR decomposition of `X_high`. Then solve the system $R\\beta = Q^T y$ for $\\beta$ using `scipy.linalg.solve_triangular`. Compare your result to the standard `np.linalg.lstsq` solution.\n",
    "\n",
    "3.  **SVD for Image Compression:** A powerful application of low-rank approximation via SVD is image compression. An image can be represented as a matrix of pixel values. By taking the SVD of the image matrix and reconstructing it using only the top `k` singular values and vectors, we can achieve significant compression.\n",
    "    - **Task:** Load an image (e.g., from `scipy.datasets.face(gray=True)`). Compute its SVD. Write a function that reconstructs the image using only the top `k` singular values. Use Matplotlib to display the original image and several reconstructions with different values of `k` (e.g., 5, 20, 50) to see the trade-off between compression and image quality.\n",
    "\n",
    "4.  **Sparse Fixed Effects:** In panel data econometrics, fixed effects are often implemented by creating a large number of dummy variables, one for each individual or group. This results in a very large, very sparse design matrix. \n",
    "    - **Task:** Create a sparse matrix representing a design matrix with 100,000 observations and 10,000 individual fixed effects (so it's a 100,000 x 10,000 matrix). Each row should have exactly one '1' in one of the 10,000 columns. Compare the memory usage of this matrix when stored as a `scipy.sparse.csr_matrix` versus a dense NumPy array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\n
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a28071",
   "metadata": {},
   "source": [
    "> **License Notice**\n",
    "> \n",
    "> This notebook is part of the *Computational Economics and Data Science* course.\n",
    "> \n",
    "> - Code cells are released under the MIT License. You are free to use and adapt the code with attribution.\n",
    "> - Text, figures, and other non-code content are released under the Creative Commons Attribution 4.0 International License.\n",
    "> \n",
    "> Please retain this notice and credit Amirreza \"Farnam\" Taheri when sharing adaptations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, linalg\n",
    "from scipy.optimize import fsolve\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# --- Custom Display Functions ---\n",
    "def theorem(title, statement):\n",
    "    display(Markdown(f\"\"\"\n",
    "    <div style='background-color:#e7f3e7; padding:15px; border-left:5px solid #4CAF50'>\n",
    "    <strong>Theorem ({title}):</strong><br/>\n",
    "    {statement}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "def example(title, content):\n",
    "    display(Markdown(f\"\"\"\n",
    "    <div style='background-color:#e3f2fd; padding:15px; border-left:5px solid #2196F3'>\n",
    "    <strong>Example ({title}):</strong><br/>\n",
    "    {content}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "def econ_app(title, content):\n",
    "    display(Markdown(f\"\"\"\n",
    "    <div style='background-color:#fff3e0; padding:15px; border-left:5px solid #FF9800'>\n",
    "    <strong>ðŸ’¼ Economic Application ({title}):</strong><br/>\n",
    "    {content}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "def course_connection(content):\n",
    "    display(Markdown(f\"\"\"*ðŸ“š **Course Connection:** {content}*\"\"\"))\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A3: Probability Theory\n",
    "\n",
    "---\n",
    "## Table of Contents\n",
    "\n",
    "- 3.1 Probability Spaces\n",
    "- 3.2 Expectation and Moments\n",
    "- 3.3 Conditional Expectation\n",
    "- 3.4 Convergence of Random Variables\n",
    "- 3.5 Common Distributions\n",
    "- 3.6 Introduction to Martingales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**: This section reviews the core concepts of probability theory that are indispensable for modern economics. We will cover the axiomatic foundations of probability, the properties of random variables and their distributions, and the key limit theorems that underpin all of econometrics. A solid understanding of these concepts is essential for working with data, modeling uncertainty, and understanding stochastic economic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Probability Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Sample Spaces, Events, and Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **probability space** is a mathematical construct that models a random process. It consists of three parts: $(\\\\Omega, \\\\mathcal{F}, P)$.\n",
    "- **Sample Space ($\\Omega$)**: The set of all possible outcomes.\n",
    "- **Sigma-Algebra ($\\mathcal{F}$)**: A collection of subsets of $\\Omega$ called **events**. This collection is closed under complement and countable unions.\n",
    "- **Probability Measure ($P$)**: A function that assigns a probability (a number in $[0, 1]$) to each event in $\\mathcal{F}$.\n",
    "\n",
    "A **random variable** $X$ is a function that maps outcomes from the sample space to real numbers, $X: \\\\Omega \\\\to \\\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 CDF, PDF, and PMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of a random variable is characterized by its distribution, which can be described in several ways:\n",
    "- **Cumulative Distribution Function (CDF)**: $F_X(x) = P(X \\le x)$. This is the most general description.\n",
    "- **Probability Mass Function (PMF)** (for discrete RVs): $p_X(x) = P(X = x)$.\n",
    "- **Probability Density Function (PDF)** (for continuous RVs): A function $f_X(x)$ such that $P(a \\le X \\le b) = \\int_a^b f_X(x)dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical Note:** While the ideas of probability have been around for centuries, the formal axiomatic foundation of probability theory was laid by the Russian mathematician **Andrey Kolmogorov** in his 1933 book, *Foundations of the Theory of Probability*. Kolmogorov's axioms, which are based on measure theory, provide the rigorous mathematical framework for modern probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below shows the Probability Density Function (PDF) and the Cumulative Distribution Function (CDF) for a standard normal distribution (mean=0, standard deviation=1). The area under the PDF curve to the left of a value `x` is equal to the value of the CDF at `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Normal PDF](images\png\normal_pdf.png)\n",
    "*<center>Probability Density Function (PDF)</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Normal CDF](images\png\normal_cdf.png)\n",
    "*<center>Cumulative Distribution Function (CDF)</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Expectation and Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **expected value** (or mean) of a random variable is the long-run average value of repetitions of the experiment it represents. It is a weighted average of all possible values, where the weights are the probabilities.\n",
    "- **Discrete:** $E[X] = \\sum_x x \\cdot p_X(x)$\n",
    "- **Continuous:** $E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical Note:** The Central Limit Theorem has a long history, with early versions being developed by Abraham de Moivre and Pierre-Simon Laplace in the 18th and 19th centuries. However, the first general proof for independent and identically distributed random variables with finite variance was provided by the Russian mathematician **Aleksandr Lyapunov** in 1901. The theorem is a cornerstone of modern statistics and econometrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Law of the Unconscious Statistician (LOTUS)\", \"To find the expectation of a function of a random variable, $g(X)$, one does not need to first find the distribution of $g(X)$. Instead, one can compute it directly: $E[g(X)] = \\\\int g(x) f_X(x)dx$.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Variance, Covariance, and Higher Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Variance**: A measure of the spread or dispersion of a distribution. $Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$.\n",
    "- **Covariance**: A measure of the joint variability of two random variables. $Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$.\n",
    "- **Correlation**: A normalized version of covariance that is always between -1 and 1. $\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$.\n",
    "- **Higher Moments**: Skewness (asymmetry) and Kurtosis (tail thickness) describe other aspects of a distribution's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_app(\"Portfolio Theory\", \"The mean and variance of asset returns are the fundamental inputs into modern portfolio theory. The covariance and correlation between different assets are crucial for understanding the benefits of diversification. A portfolio's variance depends critically on the covariance between its constituent assets.\"),\n",
    "course_connection(\"These concepts are foundational for all of econometrics (Chapter 6) and finance (Chapter 9).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jensen_header",
   "metadata": {},
   "source": [
    "### 3.2.3 Jensen's Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jensen_theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Jensen's Inequality\", \"If $X$ is a random variable and $g$ is a convex function, then $g(E[X]) \\\\le E[g(X)]$. If $g$ is a concave function, the inequality is reversed: $g(E[X]) \\\\ge E[g(X)]$. The inequality is strict if $g$ is strictly convex/concave and $X$ is not a constant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jensen_why_matters",
   "metadata": {},
   "source": [
    "**Motivation**: Jensen's Inequality provides the mathematical foundation for the economic theory of risk aversion. It establishes a direct link between the **curvature** of an agent's utility function and their preference for risk. For a concave (i.e., 'hill-shaped') utility function, the inequality shows that the utility of a certain outcome is always higher than the expected utility of a risky gamble with the same expected value. This formalizes the idea that a risk-averse individual prefers a sure thing over a gamble. The gap between the utility of the expected value and the expected utility of the gamble is a measure of the cost of risk, which gives rise to concepts like the risk premium and insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jensen_econ_app",
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_app(\"Risk Aversion\", \"An agent is risk-averse if they have a concave utility function, $U$. Consider a lottery (a random variable $X$) that pays $x_1$ or $x_2$ with equal probability. The expected wealth is $E[X]$. The utility of the expected wealth is $U(E[X])$. The expected utility of the lottery is $E[U(X)]$. By Jensen's Inequality, because $U$ is concave, we have $U(E[X]) > E[U(X)]$. This means a risk-averse agent prefers the certain outcome (the expected value of the lottery) to the risky lottery itself. The difference between $E[X]$ and the 'certainty equivalent' (the amount of certain wealth that gives the same utility as the lottery) is the risk premium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jensen_image",
   "metadata": {},
   "source": [
    "The plot below illustrates Jensen's Inequality for a concave utility function (like $U(x)=\\\\log(x)$). The expected utility from a gamble over two outcomes, $E[U(X)]$, is lower than the utility of the expected value of that gamble, $U(E[X])$.\n",
    "\n",
    "![Jensen's Inequality](images\png\jensen_inequality.png)\n",
    "*<center>This image was programmatically generated for the course.</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Conditional Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Conditional Expectation and Bayes' Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional expectation**, $E[Y|X]$, is the expected value of a random variable $Y$, given that another random variable $X$ has taken on a specific value. It is the best prediction of $Y$ given the information in $X$. A key tool for understanding this is **Bayes' Rule**, which allows us to update our beliefs (probabilities) in light of new evidence:\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 The Law of Iterated Expectations (LIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Law of Iterated Expectations\", \"The expected value of the conditional expectation of $Y$ given $X$ is the same as the unconditional expectation of $Y$. Formally, $E[E[Y|X]] = E[Y]$. The smaller sigma-algebra always wins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**: The LIE is a powerful tool for breaking down complex expectations into simpler, nested ones. It is used extensively in **rational expectations models**, where agents' expectations of future variables are formed based on today's information. The law allows us to relate expectations across different time periods and information sets in a consistent way. For example, our best guess today about what our best guess will be tomorrow is simply our best guess today. In econometrics, it is the foundation for the exogeneity assumption in OLS. The assumption that the error term has a mean of zero *conditional on the regressors*, $E[u|X] = 0$, is a statement about the conditional expectation. The LIE tells us that if this holds, the unconditional expectation of the error must also be zero: $E[u] = E[E[u|X]] = E[0] = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Demo: Law of Iterated Expectations\n",
    "np.random.seed(42)\n",
    "n = 10000\n",
    "X = np.random.choice([0, 1], p=[0.5, 0.5], size=n)\n",
    "# Y depends on X\n",
    "Y = np.zeros(n)\n",
    "Y[X==0] = np.random.normal(5, 1, size=np.sum(X==0))\n",
    "Y[X==1] = np.random.normal(10, 1, size=np.sum(X==1))\n",
    "\n",
    "E_Y = np.mean(Y)\n",
    "E_Y_given_X0 = np.mean(Y[X==0])\n",
    "E_Y_given_X1 = np.mean(Y[X==1])\n",
    "\n",
    "# E[E[Y|X]] = P(X=0)*E[Y|X=0] + P(X=1)*E[Y|X=1]\n",
    "E_E_Y_X = 0.5 * E_Y_given_X0 + 0.5 * E_Y_given_X1\n",
    "\n",
    "print(f\"E[Y]: {E_Y:.4f}\")\n",
    "print(f\"E[E[Y|X]]: {E_E_Y_X:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_connection(\"Underpins all of econometrics (Chapter 6) and rational expectations models (Chapter 4).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Convergence of Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Modes of Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of convergence for a sequence of random variables is more nuanced than for deterministic sequences. There are several distinct 'modes' of convergence, each with a different strength and interpretation.\n",
    "\n",
    "- **Convergence in Distribution ($X_n \\xrightarrow{d} X$)**: This is the weakest form of convergence. It means that the *shape* of the cumulative distribution function (CDF) of $X_n$ gets closer and closer to the shape of the CDF of $X$. It does not mean that the random variables themselves are getting close. The Central Limit Theorem is the most famous example of convergence in distribution.\n",
    "- **Convergence in Probability ($X_n \\xrightarrow{p} X$)**: This is a stronger concept. It means that the probability of the random variable $X_n$ being 'far away' from the random variable $X$ becomes vanishingly small as $n$ increases. The Law of Large Numbers is a key example.\n",
    "- **Almost Sure Convergence ($X_n \\xrightarrow{a.s.} X$)**: This is the strongest form of convergence typically used in economics. It means that for any possible outcome of the underlying random process ($\\\\omega$), the sequence of realized values $X_n(\\\\omega)$ will converge to the value $X(\\\\omega)$, except for a set of 'unlucky' outcomes that has probability zero. \n",
    "\n",
    "**Hierarchy**: Almost sure convergence implies convergence in probability, which in turn implies convergence in distribution. The reverse implications are not generally true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Law of Large Numbers (LLN) and Central Limit Theorem (CLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Law of Large Numbers (LLN)\", \"For i.i.d. random variables $X_i$ with mean $\\\\mu$, the sample mean $\\\\bar{X}_n = \\\\frac{1}{n}\\\\sum_{i=1}^n X_i$ converges in probability to the true mean $\\\\mu$. The Strong LLN states that this convergence is almost sure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Central Limit Theorem (CLT)\", \"For i.i.d. random variables $X_i$ with mean $\\\\mu$ and variance $\\\\sigma^2$, the standardized sample mean converges in distribution to a standard normal distribution: $ \\\\frac{\\\\bar{X}_n - \\\\mu}{\\\\sigma/\\\\sqrt{n}} \\\\xrightarrow{d} N(0, 1) $. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why they matter**: The LLN justifies using sample averages to estimate population means. The CLT is the reason the normal distribution is ubiquitous in statistics; it allows us to perform hypothesis testing and construct confidence intervals for sample means, even if the underlying data is not normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images below provide a simple illustration of the Central Limit Theorem. We start with a discrete uniform distribution (a single die roll). As we sum more and more independent draws from this distribution, the distribution of the sum begins to look more and more like a normal distribution."
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CLT with 1 die](images\png\clt_1.png)\n",
    "*<center>Distribution for the sum of 1 die.</center>*"
   ]
  },
      {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CLT with 2 dice](images\png\clt_2.png)\n",
    "*<center>Distribution for the sum of 2 dice.</center>*"
   ]
  },
      {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CLT with 3 dice](images\png\clt_3.png)\n",
    "*<center>Distribution for the sum of 3 dice. Note the emerging bell shape.</center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_connection(\"The LLN and CLT are the foundation of all statistical inference in Chapter 6.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Common Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Key Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief overview of the most important distributions for economics and econometrics:\n",
    "\n",
    "**Discrete:**\n",
    "- **Bernoulli**: A single trial with two outcomes (e.g., success/failure).\n",
    "- **Binomial**: The number of successes in a fixed number of independent Bernoulli trials.\n",
    "- **Poisson**: The number of events occurring in a fixed interval of time or space.\n",
    "\n",
    "**Continuous:**\n",
    "- **Uniform**: All outcomes in a range are equally likely.\n",
    "- **Normal (Gaussian)**: The classic bell curve, central to statistics due to the CLT.\n",
    "- **Lognormal**: The logarithm of the random variable is normally distributed. Often used for variables that cannot be negative, like income or stock prices.\n",
    "- **Chi-squared (Ï‡Â²)**: The distribution of a sum of squared standard normal variables. Used in hypothesis testing.\n",
    "- **Student's t**: Similar to the normal but with heavier tails. Used for inference when the sample size is small.\n",
    "- **F-distribution**: The distribution of a ratio of two chi-squared variables. Used for F-tests in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 The Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **multivariate normal distribution** is the generalization of the normal distribution to higher dimensions. A vector $\\\\mathbf{X} \\\\in \\\\mathbb{R}^n$ is multivariate normal, $\\\\mathbf{X} \\\\sim N(\\\\\boldsymbol{\\\\mu}, \\\\Sigma)$, if it is characterized by a mean vector $\\\\boldsymbol{\\\\mu}$ and a covariance matrix $\\\\Sigma$. It has several crucial properties:\n",
    "- Linear combinations of its components are normally distributed.\n",
    "- All marginal distributions are normal.\n",
    "- All conditional distributions are normal.\n",
    "- Zero correlation implies independence (this is a special property not true for most distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below shows samples drawn from three different bivariate normal distributions. The correlation coefficient $\\rho$ determines the orientation and shape of the distribution. When $\\rho$ is positive, the variables are positively related; when $\\rho$ is negative, they are negatively related; and when $\\rho$ is zero, they are uncorrelated and, in the case of the normal distribution, independent.\n",
    "\n",
    "![Bivariate Normal Distributions](images\png\multivariate_normal_distribution.png)\n",
    "*<center>This image was programmatically generated for the course.</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Introduction to Martingales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Definition and Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **martingale** is a stochastic process (a sequence of random variables) for which, at any particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value. Informally, it's a model of a **fair game**; if you know the history of the game up to today, your best bet for the value of your fortune tomorrow is its value today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem(\"Martingale Property\", \"A stochastic process $\\{M_t\\}$ is a martingale with respect to an information set (filtration) $\\\\mathcal{F}_t$ if it satisfies:<br>1. $E[|M_t|] < \\\\infty$ for all $t$.<br>2. $E[M_{t+1} | \\\\mathcal{F}_t] = M_t$.\"),\n",
    "econ_app(\"Efficient Markets Hypothesis\", \"A simple version of the EMH states that asset prices should follow a martingale. If a price change were predictable, investors would trade on that information until the predictable component was eliminated. Therefore, under the EMH, the best forecast of tomorrow's stock price is today's stock price.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows a simulation of several paths of a random walk, which is a classic example of a martingale. At any point in time, the expected next position is the current position.\n",
    "\n",
    "![Martingale Paths](images\png\martingale_paths.png)\n",
    "*<center>This image was programmatically generated for the course.</center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_connection(\"Crucial for asset pricing (Chapter 9), rational expectations models (Chapter 4), and the theory of time series econometrics (Chapter 8).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

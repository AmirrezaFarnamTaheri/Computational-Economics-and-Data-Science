{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, re\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# The core libraries for this chapter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_datareader.data as web\n",
    "from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6), 'figure.dpi': 130})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    print(f\"\\n\ud83d\udcdd {textwrap.fill(msg, width=100, subsequent_indent='   ')}\", **kwargs)\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations\n",
    "## Chapter 1.14: Data Acquisition: APIs and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Beyond the Curated Dataset\n",
    "\n",
    "Modern empirical economics relies on novel and up-to-the-minute data. The ability to acquire data directly from its source is a crucial skill. This process falls into two categories:\n",
    "\n",
    "1.  **Using APIs (Application Programming Interfaces):** The structured, preferred method. An API is a formal contract provided by a data source (like the Federal Reserve or World Bank) that specifies how a programmer can request data in a clean, machine-readable format (usually JSON).\n",
    "\n",
    "2.  **Web Scraping:** The process of extracting information from unstructured websites that do not provide an API. It involves downloading a web page's raw HTML and parsing it to extract specific information.\n",
    "\n",
    "This notebook provides a practical guide to both methods, covering the underlying web protocols, the tools for interacting with them, and the challenges of acquiring data from both static and dynamic web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Language of the Web: HTTP\n",
    "\n",
    "The **Hypertext Transfer Protocol (HTTP)** governs communication on the web. Every API call or page visit involves an HTTP request-response cycle.\n",
    "\n",
    "1.  **Request:** Your client sends a request with a:\n",
    "    - **Method**: The verb indicating the desired action (e.g., `GET` to retrieve data, `POST` to send data).\n",
    "    - **URL**: The unique address of the resource.\n",
    "    - **Headers**: Metadata about the request (e.g., `User-Agent` identifying your client, `Accept` specifying desired content types).\n",
    "    - **Body** (optional): The data being sent in a `POST` request.\n",
    "2.  **Response:** The server sends back a response with a:\n",
    "    - **Status Code**: A three-digit code indicating the outcome (e.g., `200 OK` for success, `404 Not Found` for a client error, `500 Internal Server Error` for a server error).\n",
    "    - **Headers**: Metadata about the response.\n",
    "    - **Body**: The requested content (e.g., HTML, JSON, or an image file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accessing Structured Data with APIs\n",
    "An API is the preferred method for data acquisition. It is robust, structured, and respectful of the data provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 High-Level Convenience: `pandas-datareader`\n",
    "For common economic data sources, `pandas-datareader` provides a convenient interface. However, it can be brittle; if the source API changes, the wrapper may break. For robust work, direct API interaction is often superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Fetching Data from FRED with pandas-datareader\")\n",
    "try:\n",
    "    start_date, end_date = '2020-01-01', '2023-12-31'\n",
    "    fred_data = web.DataReader(['GDP', 'UNRATE'], 'fred', start_date, end_date)\n",
    "    note(f\"Successfully fetched FRED data from {start_date} to {end_date}.\")\n",
    "    print(fred_data.tail())\n",
    "except Exception as e:\n",
    "    note(f\"pandas-datareader failed. The service may be down or the API may have changed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Direct API Interaction with `requests`\n",
    "Using the `requests` library to interact with APIs directly is more robust. This involves constructing the request URL, handling authentication (e.g., with an API key), and parsing the JSON response.\n",
    "\n",
    "> **Security Best Practice:** Never hard-code secrets like API keys in your scripts. Store them as environment variables and access them with `os.environ.get()`. This prevents you from accidentally committing sensitive credentials to a public repository like GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Direct FRED API Interaction with Requests\")\n",
    "\n",
    API_KEY = os.environ.get("FRED_API_KEY")
    if not API_KEY:
        note("Skipping direct API call. Set the FRED_API_KEY environment variable to run.")
    else:
        base_url = 'https://api.stlouisfed.org/fred/series/observations'
    "params = {\n",
    "    'series_id': 'UNRATE', 'api_key': API_KEY, 'file_type': 'json',\n",
    "    'observation_start': '2020-01-01', 'observation_end': '2023-12-31'\n",
    "}\n",
    "try:\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status() # Raise exception for bad status codes (4xx or 5xx)\n",
    "    \n",
    "    raw_data = response.json()\n",
    "    df = pd.DataFrame(raw_data['observations'])[['date', 'value']]\n",
    "    df = df.astype({'date': 'datetime64[ns]', 'value': 'float64'}).set_index('date')\n",
    "    \n",
    "    print(\"--- Parsed DataFrame from direct API call ---\")\n",
    "    print(df.tail())\n",
    "except requests.exceptions.RequestException as e:\n",
    "    note(f\"HTTP Request failed: {e}\")\n",
    "except (KeyError, json.JSONDecodeError) as e:\n",
    "    note(f\"Failed to parse JSON response: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting Unstructured Data with Web Scraping\n",
    "When an API is unavailable, we can resort to web scraping: programmatically downloading and parsing a web page's HTML. This is a powerful but often brittle technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 The Ethics of Scraping: `robots.txt`\n",
    "Before scraping, you must check the site's `robots.txt` file. This file, located at the root of a domain (e.g., `https://en.wikipedia.org/robots.txt`), specifies rules for automated agents. While not legally binding, **disobeying `robots.txt` is a serious ethical breach.** Always be a polite scraper: send requests slowly, identify your bot with a `User-Agent` header, and never overload a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Examining a `robots.txt` file\")\n",
    "try:\n",
    "    robots_url = 'https://en.wikipedia.org/robots.txt'\n",
    "    response = requests.get(robots_url)\n",
    "    print(f\"--- Contents of {robots_url} ---\")\n",
    "    print(response.text[:500] + \"...\") # Print first 500 characters\n",
    "except requests.exceptions.RequestException as e:\n",
    "    note(f\"Could not fetch robots.txt: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Scraping Static Sites with `requests` and `BeautifulSoup`\n",
    "The standard toolkit for simple scraping is:\n",
    "1.  **`requests`**: To download the full HTML content of a URL.\n",
    "2.  **`BeautifulSoup`**: To parse messy HTML into a structured, searchable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Scraping an HTML Table with BeautifulSoup\")\n",
    "\n",
    "def scrape_sp500_table():\n",
    "    \"\"\"Scrapes the S&P 500 components table from Wikipedia.\"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    # It's good practice to identify your bot with a User-Agent header.\n",
    "    headers = {'User-Agent': 'Jules-Economic-Analysis-Bot/1.0'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return note(f\"Failed to fetch URL: {e}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Find the table by its unique CSS ID, which is more robust than finding its caption.\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    if not table:\n",
    "        return note(\"Could not find S&P 500 constituents table. Page structure may have changed.\")\n",
    "    \n",
    "    # Use pandas' built-in HTML table parser, which is highly efficient.\n",
    "    try:\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        note(f\"Successfully parsed {len(df)} companies from the table.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return note(f\"Failed to parse table with pandas: {e}\")\n",
    "\n",
    "sp500_df = scrape_sp500_table()\n",
    "if sp500_df is not None:\n",
    "    print(sp500_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advanced Scraping: Dealing with JavaScript-Rendered Pages\n",
    "Many modern websites load their content dynamically using JavaScript. `requests` only gets the initial HTML; it does not execute JavaScript. To scrape these sites, we need to automate a real web browser.\n",
    "\n",
    "#### `playwright` vs. `selenium`\n",
    "While `selenium` is the classic tool for browser automation, **`playwright`** is a more modern library with a simpler API, better performance, and more reliable auto-waiting mechanisms. For new projects, `playwright` is the recommended choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Scraping a JavaScript-Rendered Page with Playwright\")\n",
    "\n",
    "def scrape_dynamic_quotes_playwright():\n",
    "    \"\"\"Uses Playwright to scrape quotes from a JavaScript-powered website.\"\"\"\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            # headless=True runs the browser in the background without a visible UI.\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            \n",
    "            url = 'http://quotes.toscrape.com/js/'\n",
    "            page.goto(url, wait_until='domcontentloaded')\n",
    "            \n",
    "            # Playwright's locators have auto-waiting built-in. This is crucial for dynamic sites.\n",
    "            # It waits for the element to appear before proceeding, up to a timeout.\n",
    "            page.wait_for_selector('div.quote', timeout=10000)\n",
    "            note(\"Dynamic content has loaded.\")\n",
    "            \n",
    "            soup = BeautifulSoup(page.content(), 'html.parser')\n",
    "            browser.close()\n",
    "            \n",
    "            quotes = []\n",
    "            for quote_div in soup.find_all('div', class_='quote'):\n",
    "                quotes.append({\n",
    "                    'text': quote_div.find('span', class_='text').text,\n",
    "                    'author': quote_div.find('small', class_='author').text\n",
    "                })\n",
    "            \n",
    "            note(f\"Successfully scraped {len(quotes)} quotes.\")\n",
    "            print(pd.DataFrame(quotes))\n",
    "    except PlaywrightTimeoutError:\n",
    "        note(\"Playwright timed out waiting for the page to load.\")\n",
    "    except Exception as e:\n",
    "        note(f\"An error occurred with Playwright. You may need to run 'pip install playwright && playwright install'. Error: {e}\")\n",
    "\n",
    "scrape_dynamic_quotes_playwright()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Best Practices Summary\n",
    "- **Prioritize APIs:** Always prefer a documented API over scraping.\n",
    "- **Be Ethical:** Check `robots.txt`. Scrape slowly. Identify your bot with a `User-Agent`.\n",
    "- **Handle Errors:** Network requests can fail. Use `try...except` blocks and check status codes.\n",
    "- **Separate Acquisition from Analysis:** Write data acquisition code that saves its results to a file (CSV, Parquet). This makes your research reproducible without re-scraping data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Exercises\n",
    "\n",
    "1.  **FRED API:** Register for a FRED API key and set it as an environment variable. Write a function that takes a list of FRED series IDs and returns a single, clean Pandas DataFrame containing all of them. Use it to download data on the 10-Year (`DGS10`) and 2-Year (`DGS2`) Treasury rates and plot the yield curve spread (`10Y - 2Y`).\n",
    "\n",
    "2.  **Static Scraping:** The website `books.toscrape.com` is a sandbox for scraping. Write a script that scrapes the title, price, and star rating of every book on the first page. Store the results in a Pandas DataFrame.\n",
    "\n",
    "3.  **Scraping Challenge (Pagination):** Extend your script from the previous exercise to handle pagination. Your script should click the \"Next\" button and continue scraping until it has collected the data for all books across all pages.\n",
    "\n",
    "4.  **Dynamic Scraping (Advanced):** The website `toscrape.com` also has an \"infinite scroll\" page at `http://quotes.toscrape.com/scroll`. Write a script using `playwright` that automatically scrolls down the page multiple times to load more quotes and then scrapes all the loaded quotes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
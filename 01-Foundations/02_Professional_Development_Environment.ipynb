{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "# This cell initializes the notebook's environment. It is standard practice to place all imports\n",
    "# and configurations at the beginning for clarity, reproducibility, and to avoid clutter later on.\n",
    "\n",
    "# --- Core Python Libraries ---\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Scientific Computing & Data Analysis ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Interactivity & Display ---\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set a professional plot style and update parameters for high-quality figures.\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({ \n",
    "    'figure.dpi': 150,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 'large',\n",
    "    'axes.labelsize': 'medium',\n",
    "    'xtick.labelsize': 'small',\n",
    "    'ytick.labelsize': 'small',\n",
    "    'legend.fontsize': 'medium'\n",
    "})\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=120)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # Suppress minor warnings\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    \"\"\"Prints a formatted message with a notebook icon.\"\"\"\n",
    "    formatted_msg = textwrap.fill(msg, width=100, subsequent_indent='   ')\n",
    "    print(f\"\\n\ud83d\udcdd {formatted_msg}\", **kwargs)\n",
    "\n",
    "def sec(title):\n",
    "    \"\"\"Prints a formatted section title for code blocks.\"\"\"\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "IMAGE_BASE = Path(\"../images\")  # Adjust path based on notebook location\n",
    "\n",
    "def show_img(path, caption=\"\", width=None):\n",
    "    \"\"\"Display image with caption\"\"\"\n",
    "    if width:\n",
    "        display(Image(filename=IMAGE_BASE / path, width=width))\n",
    "    else:\n",
    "        display(Image(filename=IMAGE_BASE / path))\n",
    "    if caption:\n",
    "        display(Markdown(f\"*{caption}*\"))\n",
    "\n",
    "note(\"Environment initialized. Reproducibility seed set to 123.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations\n",
    "## Chapter 1.2: The Professional Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: From Disposable Scripts to Durable Scientific Artifacts\n",
    "\n",
    "Computational research in economics transcends merely writing code that functions. It demands the creation of **durable scientific artifacts**: structured, transparent, and reproducible projects that can withstand rigorous scrutiny. The transition from writing disposable, one-off scripts to engineering auditable research artifacts is the defining characteristic of a professional computational scientist. This chapter details the suite of tools and the associated workflow that form the bedrock of this professional practice.\n",
    "\n",
    "#### The \"Reproducibility Crisis\" as a Call to Action\n",
    "\n",
    "Across many scientific disciplines, including economics, there is a growing awareness of a **\"reproducibility crisis.\"** Numerous high-profile studies have proven difficult or impossible to replicate, not because their theories were necessarily wrong, but because their empirical results could not be regenerated from the original data and code. This has led to a fundamental rethinking of what constitutes a scientific contribution. A published paper is now increasingly seen as an *advertisement* for the research; the research *itself* is the full ecosystem of code, data, and environment that produced the results.\n",
    "\n",
    "This creates a new, higher standard of professional responsibility. The primary objective is to cultivate a workflow where every figure, every table, and every statistical result can be regenerated with push-button simplicity, by anyone, anywhere, at any time. This is not a matter of mere technical fastidiousness; it is a prerequisite for **scientific credibility** in the modern era. When a model *is* the code, the code must be as transparent and auditable as a mathematical proof. \n",
    "\n",
    "The tools discussed here—the command line for automation, Git for version control, and Conda for environment management—are the essential infrastructure for meeting this standard. Mastery of this environment is as fundamental to the discipline as a deep understanding of economic theory or econometric methods. It is the practical means by which we ensure our work is a reliable and lasting contribution to scientific knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Command-Line Interface (CLI): The Economist's Power Tool\n",
    "\n",
    "The command line, or **shell**, is a text-based interface for interacting with a computer's operating system. While graphical user interfaces (GUIs) are intuitive for simple, interactive tasks, the CLI is indispensable for scientific computing due to its power, expressiveness, and capacity for automation.\n",
    "\n",
    "**Core Rationale for Researchers:**\n",
    "- **Automation and Scripting:** The CLI allows the automation of repetitive tasks. A sequence of commands can be saved into a shell script (e.g., a `run_analysis.sh` file) to perform a multi-step process—such as downloading data, executing a cleaning script, running an estimation routine, and compiling a paper—with a single command. This drastically reduces the potential for manual error and explicitly documents the entire research workflow.\n",
    "- **Tool Composability (The Unix Philosophy):** The power of the CLI is rooted in the Unix philosophy of building simple, interoperable tools that do one thing well. The **pipe** operator, `|`, exemplifies this by allowing the output of one command to be seamlessly used as the input for another. This enables the construction of sophisticated data processing pipelines directly in the shell, often far more efficiently than writing a dedicated script.\n",
    "- **Remote Computing:** For accessing high-performance computing (HPC) clusters or cloud-based virtual machines—essential for large-scale economic modeling (e.g., HANK models, structural estimation)—the CLI is the standard, and often the only, mode of interaction.\n",
    "\n",
    "**Essential Commands for Navigating Your Project:**\n",
    "The shell operates from a **working directory**. All commands are executed relative to this location.\n",
    "- `pwd`: **P**rint **W**orking **D**irectory. Shows your current location in the filesystem.\n",
    "- `ls`: **L**i**s**t files and directories. Common flags: `ls -l` for a long listing with details (permissions, owner, size), `ls -a` to show hidden files (like `.git` or `.gitignore`), and `ls -F` to add type indicators (e.g., `/` for directories, `*` for executables).\n",
    "- `cd [directory]`: **C**hange **D**irectory. Navigates the filesystem. `cd ..` moves to the parent directory; `cd ~` navigates to your home directory; `cd -` navigates to the previous directory you were in.\n",
    "- `mkdir [name]`: **M**a**k**e **Dir**ectory. Creates a new directory.\n",
    "- `mv [source] [destination]`: **M**o**v**e or rename a file or directory.\n",
    "- `cp [source] [destination]`: **C**o**p**y a file or directory. Use `cp -r` to copy a directory and its contents recursively.\n",
    "- `rm [file]`: **R**e**m**ove a file. This is permanent and does not use a trash bin; use with caution. Use `rm -r` to remove a directory recursively.\n",
    "- `cat [file]`: Con**cat**enate and print the contents of a file to the screen.\n",
    "- `head`/`tail [file]`: Show the first/last 10 lines of a file. Use `-n 50` to show 50 lines. This is invaluable for peeking at large data files without loading them into memory.\n",
    "\n",
    "**Data-Centric Power Tools:**\n",
    "- `wget [url]` or `curl -O [url]`: Download a file from a URL directly into your current directory. Essential for scripting data acquisition.\n",
    "- `tar -zxvf archive.tar.gz`: Decompress a g-zipped tarball. The flags stand for e**x**tract, **v**erbose, **f**ile, and un-**z**ip.\n",
    "- `unzip archive.zip`: Decompress a zip file.\n",
    "- `find [path] -name \"*.csv\"`: A powerful tool for finding files. This example finds all files ending in `.csv` within the specified path.\n",
    "- `awk -F, '{print $1, $3}' data.csv`: A versatile programming language for text processing. This command treats `data.csv` as a comma-separated file (`-F,`) and prints the first and third columns (`$1`, `$3`) for each line.\n",
    "- `xargs`: Executes a command for each item it receives from standard input. Incredibly powerful for batch processing. For example, to find all `.csv` files and delete them: `find . -name \"*.csv\" | xargs rm`.\n",
    "\n",
    "**Example of a CLI Pipeline:**\n",
    "Imagine you have a large CSV file, `firm_data.csv`, and you want to calculate the average revenue (column 2) for firms in the \"Manufacturing\" sector (column 5) without opening a data analysis program. The CLI allows you to do this efficiently:\n",
    "```bash\n",
    "# This command chain finds manufacturing firms, extracts their revenue, and calculates the average.\n",
    "cat firm_data.csv | grep \"Manufacturing\" | awk -F, '{ total += $2; count++ } END { print total/count }'\n",
    "```\n",
    "This pipeline is a perfect illustration of composability:\n",
    "1.  `cat firm_data.csv`: Reads the file and streams its content to standard output.\n",
    "2.  `|`: The pipe operator redirects the standard output of `cat` to become the standard input of `grep`.\n",
    "3.  `grep \"Manufacturing\"`: Filters the stream, only passing through lines that contain the string \"Manufacturing\".\n",
    "4.  `|`: The filtered lines are piped to the next command.\n",
    "5.  `awk -F, '{...}'`: This is a mini-program. `-F,` sets the field separator to a comma. For each line it receives, it adds the value of the second column (`$2`) to a `total` variable and increments a `count` variable. The `END` block is executed after all lines have been processed, printing the final average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Version Control with Git and GitHub: Your Scientific Logbook\n",
    "\n",
    "**Version Control** is a system that records changes to a file or set of files over time so that you can recall specific versions later. For a researcher, it is the equivalent of a perfect, indelible lab notebook. **Git** is the world's standard distributed version control system.\n",
    "\n",
    "**Why Git is Non-Negotiable for Modern Research:**\n",
    "- **Complete Audit Trail:** Git provides a complete, time-stamped, author-stamped history of every change made to your project. This allows you to answer critical questions like, \"What was the exact version of the code that produced the result in Table 2 of the draft from last Tuesday?\"\n",
    "- **Fearless Experimentation:** Git's branching mechanism allows you to explore new ideas—a different model specification, a new data source, a major refactoring—without any risk to the stable, working version of your project. If an idea doesn't work out, you can simply discard the branch.\n",
    "- **Error Recovery:** If a change introduces an error or breaks existing code, Git makes it trivial to revert to a previous, working state.\n",
    "- **Collaboration:** Git is designed for distributed, asynchronous collaboration. **GitHub** and **GitLab** are web-based platforms that host Git repositories and provide an interface for managing collaborative workflows, making them indispensable for team research.\n",
    "\n",
    "**Core Concepts Explained:**\n",
    "- **Repository (Repo):** The project's entire directory, including all files and their complete revision history. This history is stored in a hidden `.git` sub-directory in the project's root.\n",
    "- **Commit:** A snapshot of the repository at a specific point in time. Each commit is a node in the project's history graph, identified by a unique SHA-1 hash (e.g., `a1b2c3d`). A commit should represent a single, logical unit of work.\n",
    "- **The Three States:** Git manages files in three main sections: the **working directory**, the **staging area**, and the **Git directory**. This separation is fundamental to its flexibility.\n",
    "  - **Working Directory:** Contains the actual files you are currently editing.\n",
    "  - **Staging Area (Index):** A draft of your *next* commit. You use `git add` to move changes from the working directory to the staging area.\n",
    "  - **Git Directory (Repository):** Where Git permanently stores the history of committed snapshots.\n",
    "\n",
    "![The three states of Git. Files move from the working directory to the staging area, and then are permanently recorded in the repository via a commit.](../images/foundations/git/git-three-states.png)\n",
    "\n",
    "- **Branch:** An independent line of development. The main branch, which should always represent a stable, production-ready state, is typically named `main`. New work is always developed in separate branches to avoid destabilizing the main line of the project.\n",
    "\n",
    "#### The Feature Branch Workflow and Code Review\n",
    "\n",
    "A disciplined and robust workflow is **feature branching**. All new work—a new feature, a bug fix, an exploratory analysis—is done in a dedicated branch. This isolates development, ensuring the `main` branch always remains in a stable, working state. \n",
    "\n",
    "When the work on the feature branch is complete, it is not merged directly. Instead, a **Pull Request** (PR) is opened on GitHub. A PR is a formal proposal to merge one branch into another. It serves as a forum for **code review**, where collaborators can examine the changes, ask questions, suggest improvements, and ensure the new code meets project standards before it is integrated. This process is fundamental to maintaining code quality and intellectual rigor in collaborative projects.\n",
    "\n",
    "![The Git Feature Branch Workflow. The main branch remains stable while new work is done in isolated branches. Once a feature is complete, tested, and reviewed, it is merged back into main.](../images/1.2-git-feature-branch-workflow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Anatomy of a Good Commit Message\n",
    "\n",
    "Commit messages are a critical form of documentation for your future self and collaborators. Adhering to a convention makes the project history readable and easy to navigate. The **Conventional Commits** specification is a widely adopted standard that enforces a clear and informative history. A commit message is not just a note; it's a structured piece of information that can be used by tools for automated changelog generation and semantic versioning.\n",
    "\n",
    "```\n",
    "type(scope): Short, imperative-mood description (max 50 chars)\n",
    "\n",
    "Optional longer body explaining the 'what' and 'why' of the change,\n",
    "not the 'how'. The 'how' is in the code itself. Wrap lines at 72\n",
    "characters for readability.\n",
    "\n",
    "Optional footer for referencing issue numbers, e.g., 'Fixes #42'.\n",
    "A BREAKING CHANGE footer indicates a change that is not backward-compatible.\n",
    "```\n",
    "- **Type:** `feat` (a new feature), `fix` (a bug fix), `docs` (documentation changes), `style` (formatting, white-space), `refactor` (a code change that neither adds a feature nor fixes a bug), `perf` (a code change that improves performance), `test` (adding or correcting tests), `chore` (build process or auxiliary tool changes).\n",
    "- **Scope:** An optional noun describing the section of the codebase affected (e.g., `data-cleaning`, `estimation`, `plotting`).\n",
    "\n",
    "**Good vs. Bad Commit Messages: A Comparison**\n",
    "\n",
    "| Bad Commit Message | Why It's Bad | Good Commit Message | Why It's Good |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| `Update code` | Vague, provides no context. | `fix(estimation): Correct off-by-one error in loop` | Specific, follows conventional commit format. |\n",
    "| `Fixed stuff` | Still too vague. What stuff? | `refactor(data): Replace custom parser with pandas.read_csv` | Describes the *what* and implies the *why* (using a standard tool). |\n",
    "| `Added new function and changed some variables` | Describes the *how*, not the *why*. Too long for a subject line. | `feat(plotting): Add function to plot model convergence` | Follows the format and describes the new capability. |\n",
    "| `bug fix` | No detail about the bug. | `fix: Prevent division by zero when assets are zero` | Clearly states the bug and the condition under which it occurs. |\n",
    "\n",
    "A good commit message should allow a collaborator to understand the *purpose* of the change without having to read the code itself. The code explains the implementation details; the commit message explains the intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Git for Researchers\n",
    "\n",
    "Beyond the daily workflow, several advanced Git commands are invaluable for maintaining a clean, professional history and efficiently diagnosing issues.\n",
    "\n",
    "- **`git rebase -i` (Interactive Rebase):** Before you open a pull request, your feature branch might have a messy history (e.g., \"fix typo\", \"try something\", \"oops revert\"). Interactive rebase allows you to clean this up. You can **reorder**, **squash** (combine multiple commits into one), **reword**, or **drop** commits. This turns a noisy history into a clean, logical sequence of changes, making it far easier for collaborators to review.\n",
    "\n",
    "- **`git bisect` (The Bug Detective):** Suppose you discover a bug in your `main` branch, but you know the code worked correctly a month ago. Between then and now, there are hundreds of commits. How do you find the single commit that introduced the bug? `git bisect` automates this search. You tell it a \"good\" commit (where the bug didn't exist) and a \"bad\" commit (where it does). Git then performs a binary search on the commit history, checking out a commit in the middle and asking you if it's good or bad. By repeating this process, it can pinpoint the exact commit that introduced the regression in logarithmic time.\n",
    "\n",
    "Let's make this concrete with a scripted example. Imagine a file `computation.py` that is supposed to calculate `2 + 2`. A bug was introduced at some point that changed this to `2 * 2`. We can use `git bisect` to find the exact commit that introduced this error.\n",
    "```bash\n",
    "# --- Setup a temporary repository for the demo ---\n",
    "mkdir /tmp/git-bisect-demo && cd /tmp/git-bisect-demo\n",
    "git init\n",
    "\n",
    "# --- Create a history of commits ---\n",
    "echo 'print(1 + 1)' > computation.py && git add . && git commit -m \"feat: Initial correct computation\"\n",
    "echo '# Adding comments' >> computation.py && git add . && git commit -m \"docs: Add comments\"\n",
    "echo 'print(2 + 2)' > computation.py && git add . && git commit -m \"feat: Update to a better computation\" # This is the last 'good' commit\n",
    "git tag v1.0 # Tag the good commit for easy reference\n",
    "\n",
    "echo '# More comments' >> computation.py && git add . && git commit -m \"docs: More comments\"\n",
    "echo 'print(2 * 2)' > computation.py && git add . && git commit -m \"feat: A buggy change\" # This is the 'bad' commit\n",
    "echo '# Final comments' >> computation.py && git add . && git commit -m \"docs: Final comments\"\n",
    "\n",
    "# --- Create a test script to detect the bug ---\n",
    "# This script will exit with code 0 (success) if the computation is correct, and 1 (failure) if it's wrong.\n",
    "echo '#!/bin/bash' > test.sh\n",
    "echo 'result=$(python3 computation.py)' >> test.sh\n",
    "echo 'if [ \"$result\" -eq 4 ]; then exit 0; else exit 1; fi' >> test.sh\n",
    "chmod +x test.sh\n",
    "\n",
    "# --- Use git bisect to find the bug ---\n",
    "git bisect start\n",
    "git bisect bad HEAD # The current commit is bad\n",
    "git bisect good v1.0 # We know the commit tagged v1.0 was good\n",
    "git bisect run ./test.sh # Automatically run our test script\n",
    "\n",
    "# After git bisect identifies the bad commit, you must run 'git bisect reset' to return to your original HEAD.\n",
    "git bisect reset\n",
    "\n",
    "# --- Cleanup ---\n",
    "cd .. && rm -rf /tmp/git-bisect-demo\n",
    "```\n",
    "\n",
    "- **`git worktree` (Parallel Universes):** This command allows you to check out multiple branches in different directories simultaneously. This is incredibly useful for comparing two branches side-by-side, or for working on a quick bug fix on `main` without having to stash the complex changes you're currently working on in your feature branch.\n",
    "\n",
    "- **`git blame` (Code Archeology):** When you encounter a confusing or questionable line of code, `git blame` is your tool. It annotates every line in a file with the commit hash and author who last modified it. This provides crucial context, allowing you to look up the original commit message to understand *why* a particular change was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge of Versioning Jupyter Notebooks\n",
    "Jupyter Notebooks (`.ipynb` files) are powerful tools for exploration and presentation, but they present challenges for version control because they are stored as complex JSON files. This structure means:\n",
    "- **Metadata Changes:** Simple actions like re-running a cell can change the notebook's metadata (e.g., execution counts), creating a \"change\" that Git will track, even if the code or text is identical.\n",
    "- **Output Diffing:** The output of code cells, including plots and large dataframes, is stored within the JSON. This makes `git diff` difficult to read, as it will show large, uninformative changes in the base64-encoded output.\n",
    "- **Merging Conflicts:** Merging two notebooks with conflicting changes is nearly impossible to do manually, as it risks corrupting the JSON structure.\n\n",
    "**Best Practices:**\n",
    "- **Keep Notebooks for Narrative:** Use notebooks for the high-level narrative and visualization of your analysis. Move all complex logic, functions, and classes into separate `.py` files in your `src/` directory and import them into the notebook.\n",
    "- **Clear Output Before Committing:** Before staging a notebook, it's often good practice to clear all cell outputs (`Kernel > Restart & Clear Output`) to avoid committing large, noisy diffs. This ensures that commits reflect changes in code and prose, not execution artifacts.\n",
    "- **Use Specialized Diffing Tools:** Tools like `nbdime` can be integrated with Git to provide \"semantic\" diffs of notebooks, showing changes to code and markdown in a clean, human-readable format.\n",
    "- **Use `jupytext`:** A powerful tool that can be configured to save a plain Python (`.py`) or Markdown (`.md`) version of your notebook alongside the `.ipynb` file. You can version control the clear text file, which provides clean diffs and easy merging, while still using the fully interactive notebook. \n",
    "\n",
    "#### The `.gitignore` File\n",
    "This is a crucial configuration file. It's a plain text file that tells Git which files or directories to intentionally ignore. It is essential for keeping your repository clean and secure. It should be used to prevent committing:\n",
    "- **Large data files:** Repositories are not designed for large binary files. Data should be stored elsewhere (e.g., a university server, a cloud bucket) and downloaded via a script.\n",
    "- **Credentials and secrets:** API keys, passwords, and other sensitive information must never be committed to a repository.\n",
    "- **Generated files:** Compiled code, log files, plots, or intermediate outputs that can be recreated from the source code and raw data.\n",
    "- **System and editor files:** Files specific to your operating system (`.DS_Store`, `Thumbs.db`) or editor configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Environment Management with Conda\n",
    "\n",
    "Scientific reproducibility requires that the computational environment—the specific versions of the language, libraries, and tools used—can be precisely recreated. A script that works today may fail a year from now if a core library like `pandas` or `statsmodels` is updated and introduces a breaking change. This is the problem of **software dependency management**.\n",
    "\n",
    "#### Conda vs. Pip: Why Conda is Essential for Science\n",
    "\n",
    "While many Python users are familiar with `pip` and `requirements.txt` files, Conda offers crucial advantages for scientific computing:\n",
    "- **Manages More Than Python:** `pip` is a package manager for Python libraries only. Scientific workflows often depend on non-Python software (e.g., the Intel MKL for linear algebra, compilers like GCC, or geospatial libraries like GDAL). Conda is a language-agnostic manager that can install and manage these complex, compiled dependencies, which `pip` cannot.\n",
    "- **True Environment Isolation:** Conda creates truly isolated environments, including the Python interpreter itself. This means you can have one project running Python 3.9 and another on the same machine running Python 3.11, without any conflict. `pip` with `venv` isolates packages, but it still relies on a system-level Python installation.\n",
    "- **Robust Dependency Resolution:** Conda's solver is designed to find a set of mutually compatible packages, which is critical when dealing with the complex dependency graphs of scientific libraries. While `pip`'s resolver has improved, Conda's is generally more robust for scientific stacks.\n",
    "\n",
    "**Anaconda** is a free, open-source distribution that bundles Python, Conda, and hundreds of the most popular scientific packages, making it an excellent starting point.\n",
    "\n",
    "**The Key to Reproducibility: The `environment.yml` file**\n",
    "The most important feature of Conda for science is the ability to export the exact specification of an environment to a text file, conventionally named `environment.yml`. This file is the blueprint of your computational environment. It lists not just the packages, but also their exact versions and the channels they were downloaded from. By committing this file to your Git repository, you give any other researcher (including your future self) the ability to perfectly recreate your environment with a single command.\n",
    "\n",
    "A well-structured `environment.yml` file should almost always prioritize the community-maintained `conda-forge` channel. It is the most comprehensive and up-to-date source for scientific packages.\n",
    "```yaml\n",
    "# A professional environment.yml file\n",
    "name: my-research-env\n",
    "channels:\n",
    "  - conda-forge # Prioritize this channel\n",
    "  - defaults\n",
    "dependencies:\n",
    "  # --- Core Environment ---\n",
    "  - python=3.11\n",
    "  - pip\n",
    "\n",
    "  # --- Core Scientific Stack ---\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - matplotlib\n",
    "  - scipy\n",
    "  - statsmodels\n",
    "  - scikit-learn\n",
    "\n",
    "  # --- Development & Testing ---\n",
    "  - jupyterlab\n",
    "  - pytest\n",
    "  - pytest-cov # For checking test coverage\n",
    "  - hypothesis # For property-based testing\n",
    "  # - nbdime # Optional: for better notebook diffing\n",
    "  # - jupytext # Optional: for saving notebooks as pure scripts\n",
    "\n",
    "  # --- Packages installed with Pip ---\n",
    "  - pip:\n",
    "    # Install the project's own source code in editable mode\n",
    "    - -e .\n",
    "    # Example of a package only available on PyPI\n",
    "    - some-package-only-on-pypi\n",
    "```\n",
    "The `pip` section with `-e .` is a powerful convention. It tells pip to install the project's own code (from the `src` directory, as defined in `pyproject.toml`) in \"editable\" mode. This makes your own project code importable from anywhere within the project, just like any other library.\n",
    "\n",
    "#### A Faster Implementation: Mamba\n",
    "A key task for Conda is **dependency resolution**. For complex environments, this can be slow. **Mamba** is a parallel, C++-based re-implementation of Conda that is significantly faster. It uses the same commands and syntax and is fully compatible with Conda environments and packages. It is highly recommended for any serious work.\n",
    "\n",
    "**Installation (once per machine):**\n",
    "`conda install -n base -c conda-forge mamba`\n",
    "\n",
    "From then on, you can simply replace `conda` with `mamba` for most operations (`mamba create`, `mamba install`, `mamba env create`) to gain a substantial speed improvement.\n",
    "\n",
    "#### Troubleshooting Dependency Issues\n",
    "A common point of frustration is when `conda` or `mamba` gets \"stuck\" trying to solve an environment, or fails with an \"unsolvable environment\" error. Here are some practical steps to take:\n",
    "1.  **Start Fresh:** It's often easier to create a new, clean environment from your `environment.yml` file than to try and fix a broken one. Use `mamba env create -f environment.yml`.\n",
    "2.  **Be Specific:** If the solver is struggling, try pinning the version of a key package that might be causing the conflict. For example, `python=3.11.5` instead of just `python=3.11`.\n",
    "3.  **Isolate the Problem:** Create a new, minimal `environment.yml` file and add packages one by one until you find the one that causes the conflict. This helps identify the source of the issue.\n",
    "4.  **Check Channel Priority:** Ensure `conda-forge` is your top channel. Mixing channels, especially `defaults` and `conda-forge`, can sometimes lead to conflicts due to different build versions of underlying libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Code Editor and Debugger: Visual Studio Code\n",
    "\n",
    "A modern code editor or Integrated Development Environment (IDE) is a critical component of a productive workflow. **Visual Studio Code (VS Code)** has become a standard in the scientific Python community due to its performance, flexibility, and extensive ecosystem of extensions for languages like Python, R, Julia, and LaTeX.\n",
    "\n",
    "While features like syntax highlighting and intelligent code completion are standard, the most transformative tool for moving beyond simple scripting is the **interactive debugger**.\n",
    "\n",
    "**The Debugger: From Guesswork to Systematic Diagnosis**\n",
    "\n",
    "Debugging with `print()` statements is a common but deeply inefficient practice. It clutters code with temporary lines, provides only a static snapshot of a variable at one point in time, and requires rerunning the entire script to get more information. A debugger is a far more powerful and systematic tool. It allows you to:\n",
    "- **Set Breakpoints:** Pause the execution of your code at any line without modifying the code itself.\n",
    "- **Inspect State:** Once paused at a breakpoint, you can inspect the value of every variable in the current scope. You can see the entire contents of a DataFrame or a NumPy array, not just what you chose to print.\n",
    "- **Step Through Code:** Execute your code line-by-line (`Step Over`), or dive into the execution of a a function (`Step Into`), or run until you exit the current function (`Step Out`). This allows you to observe how the program's state evolves in real-time.\n",
    "- **Examine the Call Stack:** See the chain of function calls that led to the current point of execution. This is invaluable for understanding how you arrived at a particular state, especially in complex code.\n",
    "\n",
    "Learning to use a debugger is a methodological leap. It replaces the haphazard guesswork of `print()` statements with a systematic, scientific process of diagnosis. It enables you to understand and fix complex bugs with an efficiency and precision that is simply not possible otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. A Professional Project Structure\n",
    "\n",
    "A standardized project structure enhances clarity, facilitates collaboration, and makes it easier for others (and your future self) to understand and navigate your work. A professional structure maps directly to the research lifecycle: acquiring raw data, processing it into analytical datasets, running models, and generating outputs like figures and tables for a paper.\n",
    "\n",
    "**A Comprehensive Directory Layout:**\n",
    "```\n",
    "my-research-project/            # The root of your project, a Git repository.\n",
    "├── .gitignore                    # Specifies files for Git to ignore.\n",
    "├── README.md                     # High-level project description, setup, and usage instructions.\n",
    "├── environment.yml               # Conda environment specification for reproducibility.\n",
    "├── pyproject.toml                # Project metadata and build configuration (PEP 518).\n",
    "|\n",
    "├── data/\n",
    "│   ├── raw/                      # Original, immutable data. Never edit files in here.\n",
    "│   ├── interim/                  # Intermediate data that has been transformed.\n",
    "│   └── processed/                # The final, canonical data sets for analysis.\n",
    "|\n",
    "├── notebooks/                    # Jupyter notebooks for exploration, prototyping, and presentation.\n",
    "│   ├── 01-initial-exploration.ipynb\n",
    "│   └── 02-modeling-results.ipynb\n",
    "|\n",
    "├── paper/                        # The manuscript.\n",
    "│   ├── manuscript.tex            # Main LaTeX file for the paper.\n",
    "│   ├── references.bib            # Bibliography file.\n",
    "│   └── figures/                  # Figures and plots generated by the code for the paper.\n",
    "|\n",
    "├── scripts/                      # Any standalone scripts.\n",
    "│   └── download_data.sh          # e.g., a shell script to download raw data.\n",
    "|\n",
    "└── src/                          # Source code for your project.\n",
    "    └── my_project/               # Make your code an importable Python package.\n",
    "        ├── __init__.py           # Makes the directory a package.\n",
    "        ├── data_processing.py    # Scripts for cleaning and preparing data.\n",
    "        ├── estimation.py         # Core estimation routines and model logic.\n",
    "        └── plotting.py           # Functions for creating standard plots.\n",
    "    └── tests/                    # Automated tests for your source code.\n",
    "        ├── __init__.py\n",
    "        └── test_estimation.py\n",
    "```\n",
    "\n",
    "**The Role of `src/` vs. `notebooks/`**\n",
    "This separation is critical. The `src/` directory contains the durable, reusable, and testable logic of your project. It should be structured as an importable Python package, configured via `pyproject.toml`. The `notebooks/` directory contains the narrative of your analysis. A notebook should tell a story, importing functions from your `src/` modules to do the heavy lifting (e.g., `from my_project.estimation import run_model`). This prevents notebooks from becoming thousand-line, unreadable monoliths and promotes code that is modular, reusable, and easier to debug and test.\n",
    "\n",
    "#### The `pyproject.toml` File: Defining Your Package\n",
    "The `pyproject.toml` file is the modern, standardized way to define project metadata and build requirements (as specified in PEP 517 and 518). It replaces older, more fragmented files like `setup.py` and `requirements.txt` for many purposes. Its most critical role in this structure is to tell Python's packaging tools (like `pip`) how to treat your `src` directory as an **installable package**. This is what enables the command `pip install -e .` to work. The `-e` flag stands for \"editable,\" meaning that changes you make to the source code in `src/` are immediately reflected in the installed package without needing to reinstall.\n",
    "\n",
    "A minimal `pyproject.toml` for a project named `my-research-project` might look like this:\n",
    "```toml\n",
    "# pyproject.toml\n",
    "[build-system]\n",
    "requires = [\"setuptools>=61.0\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"my_research_project\"\n",
    "version = \"0.1.0\"\n",
    "authors = [\n",
    "  { name=\"Your Name\", email=\"you@example.com\" },\n",
    "]\n",
    "description = \"A short description of the research project.\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.9\"\n",
    "license = { text=\"MIT\"}\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]  # Look for packages in the 'src' directory\n",
    "```\n",
    "This file makes your research code a first-class, reusable component of the Python ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Static Analysis: Automating Code Quality\n",
    "\n",
    "**Static analysis** tools automatically inspect your code for errors, style violations, and potential bugs *without* running it. Integrating these tools into your workflow is a hallmark of professional development. They act as an automated, tireless reviewer, catching common mistakes early and enforcing a consistent, readable style across your project. This is especially vital for collaborative work.\n",
    "\n",
    "#### Linters and Formatters: The Core Quality Tools\n",
    "\n",
    "- **Formatter (e.g., `black`):** An autoformatter automatically rewrites your code to conform to a strict style guide. This eliminates all arguments about style (e.g., spacing, line length, quote style) because the formatter makes the decision for you. `black` is known as \"the uncompromising code formatter\" and is a widely adopted standard in the Python community. Running `black .` from your project root will format all your `.py` files.\n",
    "\n",
    "- **Linter (e.g., `ruff` or `flake8`):** A linter goes beyond formatting to analyze your code for potential errors, bugs, stylistic errors, and code complexity. `ruff` is a modern, extremely fast linter and formatter written in Rust that can replace many older tools (`flake8`, `isort`, `pyupgrade`). It can identify issues like unused imports, undefined variables, and overly complex functions.\n",
    "\n",
    "#### Pre-Commit Hooks: Your Quality Gatekeeper\n",
    "\n",
    "It's easy to forget to run formatters and linters manually. **Pre-commit hooks** solve this problem. They are scripts that run automatically every time you make a commit. If any of the scripts fail (e.g., the linter finds an error), the commit is aborted, forcing you to fix the issue before committing. This guarantees that no low-quality code ever enters your project's history.\n",
    "\n",
    "The `pre-commit` framework makes managing these hooks simple. You define a `.pre-commit-config.yaml` file in your project root:\n",
    "```yaml\n",
    "# .pre-commit-config.yaml\n",
    "repos:\n",
    "-   repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.4.0\n",
    "    hooks:\n",
    "    -   id: check-yaml\n",
    "    -   id: end-of-file-fixer\n",
    "    -   id: trailing-whitespace\n",
    "-   repo: https://github.com/psf/black\n",
    "    rev: 23.3.0\n",
    "    hooks:\n",
    "    -   id: black\n",
    "-   repo: https://github.com/charliermarsh/ruff-pre-commit\n",
    "    rev: 'v0.0.278'\n",
    "    hooks:\n",
    "    -   id: ruff\n",
    "        args: [--fix, --exit-non-zero-on-fix]\n",
    "```\n",
    "After installing (`pip install pre-commit`) and setting up (`pre-commit install`), this configuration will automatically run `black` and `ruff` on your staged files before every commit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ensuring Correctness with Automated Testing\n",
    "\n",
    "As code becomes more complex and modular, manual testing becomes unreliable. **Automated testing** is the practice of writing code to test your code. It is a cornerstone of professional software development and is equally critical for reliable scientific research.\n",
    "\n",
    "**Why Test?**\n",
    "- **Confidence in Results:** Tests provide a verifiable guarantee that your functions are producing the correct output for known inputs.\n",
    "- **Preventing Regressions:** When you modify or refactor code, a comprehensive test suite ensures that you haven't accidentally broken existing functionality.\n",
    "- **Clarifying Functionality:** Writing a test forces you to think clearly about what a function is supposed to do, including how it should handle edge cases and invalid inputs.\n",
    "\n",
    "#### Core Testing with `pytest`\n",
    "`pytest` is the de facto standard testing framework in the Python community. Beyond simple test functions, it provides powerful features for writing clean, scalable, and maintainable tests.\n",
    "\n",
    "- **The Peril of Floating-Point Comparisons:** A common error is testing for equality between floating-point numbers (e.g., `assert result == 0.333`). Due to the way computers store numbers, `0.1 + 0.2` does not exactly equal `0.3`. Tests must always compare floats using a tolerance. `np.allclose(a, b)` is the standard for NumPy arrays, and `pytest.approx(x)` is the standard for scalars.\n",
    "- **Test-Driven Development (TDD):** A disciplined workflow where you write a *failing* test *before* you write the implementation code. The cycle is: **Red** (write a test that fails because the feature doesn't exist), **Green** (write the simplest code to make the test pass), **Refactor** (clean up the code while keeping the test green). This enforces modular, testable code from the start.\n",
    "- **Code Coverage:** A metric that measures the percentage of your codebase that is executed by your test suite. While 100% coverage doesn't guarantee correctness, low coverage is a clear sign of inadequate testing. The `pytest-cov` plugin can generate coverage reports.\n",
    "\n",
    "#### Beyond Examples: Property-Based Testing with `hypothesis`\n",
    "Standard tests are *example-based*: you provide a specific input and assert a specific output. This is effective but limited by your ability to imagine all possible edge cases. **Property-based testing** is a more advanced technique that flips this logic. Instead of testing for specific outcomes, you state general *properties* that your function must always satisfy, and a library like `hypothesis` generates hundreds of random, often strange and pathological, examples to try and falsify your property.\n",
    "\n",
    "**Example: Testing a `winsorize` function.**\n",
    "A `winsorize` function clamps outliers in data to a specified percentile. For example, winsorizing at the 5th and 95th percentiles means any value below the 5th percentile is set *to* the 5th percentile, and any value above the 95th is set *to* the 95th. This is a common task in empirical microeconomics.\n",
    "\n",
    "Instead of testing with a few hand-picked arrays, we can define properties that must always hold true for a winsorized array:\n",
    "1.  The output array must have the same shape as the input array.\n",
    "2.  The minimum value of the output array must be greater than or equal to the original lower percentile.\n",
    "3.  The maximum value of the output array must be less than or equal to the original upper percentile.\n",
    "4.  The function should not fail on arrays containing `NaN`s.\n",
    "\n",
    "Here is how you would write such a test using `pytest` and `hypothesis`.\n",
    "```python\n",
    "# In src/tests/test_analysis.py\n",
    "import pytest\n",
    "import numpy as np\n",
    "from hypothesis import given, strategies as st\n",
    "from my_project.analysis import winsorize # Assuming this function exists\n",
    "\n",
    "# Define a 'strategy' for hypothesis to generate NumPy arrays.\n",
    "# It will create arrays with float values, allowing NaNs, and of varying shapes.\n",
    "array_strategy = st.builds(np.array, st.lists(st.floats(allow_nan=True)))\n",
    "\n",
    "@given(data=array_strategy)\n",
    "def test_winsorize_properties(data):\n",
    "    \"\"\"Tests general properties of the winsorize function.\"\"\"\n",
    "    if data.size == 0:\n",
    "        # Handle empty array case separately\n",
    "        assert winsorize(data).size == 0\n",
    "        return\n",
    "\n",
    "    # Calculate percentiles, ignoring NaNs for the calculation\n",
    "    p5 = np.nanpercentile(data, 5)\n",
    "    p95 = np.nanpercentile(data, 95)\n",
    "\n",
    "    winsorized_data = winsorize(data)\n",
    "\n",
    "    # Property 1: Shape is preserved\n",
    "    assert winsorized_data.shape == data.shape\n",
    "\n",
    "    # Property 2: Minimum is respected\n",
    "    # We only check this if there are non-NaN values to get a min from\n",
    "    if not np.all(np.isnan(winsorized_data)):\n",
    "        assert np.nanmin(winsorized_data) >= p5\n",
    "\n",
    "    # Property 3: Maximum is respected\n",
    "    if not np.all(np.isnan(winsorized_data)):\n",
    "        assert np.nanmax(winsorized_data) <= p95\n",
    "```\n",
    "When you run `pytest`, `hypothesis` will generate dozens or hundreds of different arrays—empty, full of `NaN`s, with infinities, with strange float values—and feed them to your test. It is far more likely to discover a hidden bug or an unhandled edge case than manual, example-based testing. This technique is a powerful tool for building robust and reliable scientific code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Practical Example: Testing a Simple Function\n",
    "\n",
    "Theory is best understood through practice. Let's walk through a mini-TDD cycle directly within this notebook. We will create a simple (and initially buggy) function, write a test for it, see the test fail, fix the code, and see the test pass. Our function will calculate the Present Value of a single cash flow, a fundamental concept in economics.\n",
    "\n",
    "The formula for present value (PV) is $PV = \\frac{FV}{(1 + r)^n}$, where $FV$ is the future value, $r$ is the discount rate, and $n$ is the number of periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Examine the Function and its Test\n",
    "\n",
    "First, let's examine our function, which is saved in `finance_utils.py`, and its corresponding test in `test_finance_utils.py`. The test checks if the function correctly calculates the PV for a known case: the present value of $110 to be received in 2 years with a 10% discount rate is $110 / (1.1)^2 = $90.91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Step 1: Examining the Python Module and Test File\")\n",
    "!pygmentize finance_utils.py\n",
    "!pygmentize test_finance_utils.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: The Red-Green-Refactor Cycle\n",
    "\n",
    "The Test-Driven Development (TDD) workflow follows a short, repetitive cycle:\n",
    "1.  **Red:** Write a test that fails because the feature or bugfix is not yet implemented.\n",
    "2.  **Green:** Write the simplest possible code to make the test pass.\n",
    "3.  **Refactor:** Clean up the code you just wrote while keeping the test green.\n",
    "\n",
    "We will now demonstrate this cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Step 2a: The 'Red' Phase (Write a Buggy Function and See the Test Fail)\")\n",
    "\n",
    "buggy_code = \"\"\"# finance_utils.py\ndef calculate_pv(fv, r, n):\n    # Bug: Incorrect order of operations\n    return fv / 1 + r**n\n\"\"\"\n",
    "with open(\"finance_utils.py\", \"w\") as f:\n",
    "    f.write(buggy_code)\n",
    "\n",
    "note(\"Overwrote `finance_utils.py` with a buggy version. Now running pytest...\")\n",
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Step 2b: The 'Green' Phase (Fix the Bug and See the Test Pass)\")\n",
    "\n",
    "fixed_code = \"\"\"# finance_utils.py\ndef calculate_pv(fv, r, n):\n    # Fix: Correct parentheses\n    return fv / (1 + r)**n\n\"\"\"\n",
    "with open(\"finance_utils.py\", \"w\") as f:\n",
    "    f.write(fixed_code)\n",
    "\n",
    "note(\"Overwrote `finance_utils.py` with the corrected version. Now running pytest...\")\n",
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Automating Quality with Continuous Integration (CI)\n",
    "\n",
    "**Continuous Integration (CI)** is the practice of automatically running your test suite every time you push a change to your repository. This provides immediate feedback, ensuring that new contributions don't break the project.\n",
    "\n",
    "**GitHub Actions** is a CI/CD platform integrated directly into GitHub. You configure it by adding a YAML file (e.g., `.github/workflows/run-tests.yml`) to your repository. This file instructs GitHub on what to do when code is pushed. A professional workflow ties directly into the environment management practices discussed earlier.\n",
    "\n",
    "**A Robust CI Workflow:**\n",
    "This workflow uses Mamba for speed and installs dependencies directly from the `environment.yml` file, ensuring the CI environment perfectly matches the local development environment. It also installs the local project code itself using `pip install -e .`.\n",
    "\n",
    "```yaml\n",
    "# In .github/workflows/run-tests.yml\n",
    "name: Run Python Tests\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "\n",
    "    - name: Set up Mamba\n",
    "      uses: conda-incubator/setup-miniconda@v2\n",
    "      with:\n",
    "        auto-update-conda: true\n",
    "        python-version: '3.11'\n",
    "        mamba-version: '*'\n",
    "        channels: conda-forge,defaults\n",
    "        channel-priority: strict\n",
    "\n",
    "    - name: Install dependencies from environment file\n",
    "      run: mamba env update -n base -f environment.yml\n",
    "\n",
    "    - name: Test with pytest\n",
    "      run: pytest --cov=my_project src/\n",
    "```\n",
    "With this in place, every pull request on GitHub will automatically have a \"check\" that shows whether the tests passed or failed, providing a critical quality gate before merging new code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Concluding Thoughts: A Workflow for Credibility\n",
    "\n",
    "The tools and practices outlined in this chapter—CLI, Git, Conda, a professional editor, a standard project structure, and principles like discretization, vectorization, modularity, and automated testing—are not merely technical details. They form an integrated system for producing credible, reproducible, and transparent computational research. Adopting this workflow is a foundational step in moving from an amateur coder to a professional computational social scientist. It provides the scaffolding that allows you to focus on the core intellectual tasks of economic research, confident that your empirical results are built on a solid and verifiable foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Challenge Exercise: A Mini-Project Workflow\n",
    "\n",
    "This exercise is designed to be performed on your local machine to integrate the key tools from this chapter into a single, coherent workflow. You will create a new project, download real data, process it, and commit the results using best practices.\n",
    "\n",
    "**Objective:** Download the classic 'Auto MPG' dataset, write a Python script to convert a specific column from miles per gallon to liters per 100 km, and structure the project in a reproducible way.\n",
    "\n",
    "1.  **Project Setup (CLI & Git)**\n",
    "    a. On your GitHub account, create a new, public repository named `mpg-data-pipeline`.\n",
    "    b. Clone this repository to your local machine.\n",
    "    c. Inside the cloned directory, use `mkdir` to create the following structure: `data/raw/`, `data/processed/`, and `src/mpg_pipeline/`.\n",
    "    d. Create a `.gitignore` file. Add `data/` to it to ensure you never commit the data itself, only the code that generates it.\n",
    "\n",
    "2.  **Data Acquisition (CLI)**\n",
    "    a. The Auto MPG dataset is available from the UCI Machine Learning Repository. The direct link is: `http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data`.\n",
    "    b. Use `wget` or `curl` to download this file directly into your `data/raw/` directory.\n",
    "\n",
    "3.  **Scripting and Processing (Python & Project Structure)**\n",
    "    a. Create a Python script named `src/mpg_pipeline/processing.py`.\n",
    "    b. Inside this script, write code using `pandas` to:\n",
    "        i. Load the `auto-mpg.data` file. Note: it's a whitespace-delimited file with no header. You will need to consult the dataset's documentation to find the column names.\n",
    "        ii. Select the 'mpg' column.\n",
    "        iii. Create a new column named 'lp100km' (liters per 100 km). The conversion formula is `235.214 / mpg`.\n",
    "        iv. Save the processed DataFrame (with the new column) to `data/processed/auto-mpg-processed.csv`.\n",
    "    c. Create a `pyproject.toml` file to make your `src/` directory an installable package.\n",
    "\n",
    "4.  **Reproducibility (Conda & Git)**\n",
    "    a. Create an `environment.yml` file that includes `python`, `pandas`, and `pip` with the `-e .` install.\n",
    "    b. Use `mamba` or `conda` to create and activate the environment.\n",
    "    c. Run your processing script from the command line: `python src/mpg_pipeline/processing.py`. Verify that the processed file is created correctly.\n",
    "\n",
    "5.  **Committing Your Work (Git)**\n",
    "    a. Create a new branch called `feat/create-processing-pipeline`.\n",
    "    b. Use `git add` to stage the `.gitignore`, `pyproject.toml`, `environment.yml`, and your `processing.py` script. **Do not stage the `data` directory.**\n",
    "    c. Commit your changes with a clear, conventional commit message (e.g., `feat(pipeline): Create script to process MPG data`).\n",
    "    d. Push your branch to GitHub and open a pull request."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
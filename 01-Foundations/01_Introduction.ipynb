{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations\n",
    "## Chapter 1.1: A History and Introduction to Computational Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Introduction: Computation as Lens and Laboratory](#Introduction:-Computation-as-Lens-and-Laboratory)\n",
    "2. [How to Approach This Course: A Pedagogical Philosophy](#How-to-Approach-This-Course:-A-Pedagogical-Philosophy)\n",
    "3. [What is Computational Economics?](#What-is-Computational-Economics?)\n",
    "    - [The Theoretical Guarantee: The Contraction Mapping Theorem](#The-Theoretical-Guarantee:-The-Contraction-Mapping-Theorem)\n",
    "    - [An Economic Example: The Cobweb Model](#An-Economic-Example:-The-Cobweb-Model)\n",
    "4. [The Modeler's Trilemma: Realism, Generality, and Precision](#The-Modeler's-Trilemma:-Realism,-Generality,-and-Precision)\n",
    "5. [The Pre-Computational Era: The Dawn of Systemic Economic Thought](#The-Pre-Computational-Era:-The-Dawn-of-Systemic-Economic-Thought)\n",
    "6. [The Twin Catalysts of the Modern Era: Formalism and War](#The-Twin-Catalysts-of-the-Modern-Era:-Formalism-and-War)\n",
    "7. [The Four Revolutions of Computational Economics](#The-Four-Revolutions-of-Computational-Economics)\n",
    "    - [The First Revolution: Econometrics and the Mainframe (c. 1940s-1970s)](#The-First-Revolution:-Econometrics-and-the-Mainframe-(c.-1940s-1970s))\n",
    "    - [The Second Revolution: The PC and the Credibility Revolution (c. 1980-2000)](#The-Second-Revolution:-The-PC-and-the-Credibility-Revolution-(c.-1980-2000))\n",
    "    - [The Third Revolution: Computation as a Laboratory (c. 2000-Present)](#The-Third-Revolution:-Computation-as-a-Laboratory-(c.-2000-Present))\n",
    "    - [The (Emerging) Fourth Revolution: AI as a Modeling Partner](#The-(Emerging)-Fourth-Revolution:-AI-as-a-Modeling-Partner)\n",
    "8. [Why Python for Computational Economics?](#Why-Python-for-Computational-Economics?)\n",
    "9. [A Roadmap for This Course](#A-Roadmap-for-This-Course)\n",
    "10. [Core Tools and Professional Norms](#Core-Tools-and-Professional-Norms)\n",
    "11. [Chapter Summary](#Chapter-Summary)\n",
    "12. [Curated References and Further Reading](#Curated-References-and-Further-Reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Computation as Lens and Laboratory\n",
    "\n",
    "The history of modern economics is inseparable from the history of computation. The available computational tools of any given era actively define the frontiers of economic inquiry. They act as both a **lens** through which economists observe, measure, and understand the world, and a **laboratory** in which they construct and test their theories. The tools shape the questions that can be asked, the methods used to answer them, and the very definition of what constitutes a valid economic argument. To understand economics today is to appreciate how it has been molded by the computational technologies of its time.\n",
    "\n",
    "This course examines this co-evolution through the lens of four distinct—though overlapping—revolutions. Each was catalyzed by a technological leap that opened up new analytical possibilities, from the formalization of general equilibrium to the simulation of entire artificial economies. This historical framework is essential for understanding the contemporary landscape of economic research. It explains why certain models and methods dominated specific eras, how standards of evidence and rigor have shifted, and why the economist's role has transformed from that of a logician and statistician to, increasingly, a computational scientist. The central thesis is that the computational paradigm of an era—from the mechanical calculator to the mainframe, the personal computer, and the distributed cloud—is a defining feature of its economic science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Approach This Course: A Pedagogical Philosophy\n",
    "\n",
    "This course is designed to be more than a collection of code snippets and algorithms. It is built on a specific pedagogical philosophy that emphasizes a deep, intuitive understanding of the material. We believe that true mastery comes from seeing the connections between theory, application, and implementation. To that end, we encourage you to adopt the following mindset as you work through the material:\n",
    "\n",
    "1.  **Embrace the \"Scratch Implementation\" Principle:** In many early chapters, we will build fundamental tools (like an OLS regression or a numerical optimizer) from scratch using only basic libraries like NumPy. While professional libraries offer more robust and optimized versions, building them yourself provides an unparalleled look \"under the hood.\" It demystifies the process and solidifies your understanding of the underlying mechanics. Once you have built a tool from scratch, you will be better equipped to use its professional-grade equivalent wisely.\n",
    "\n",
    "2.  **Actively Engage with the Code:** The code cells in these notebooks are not static illustrations; they are interactive experiments. We strongly encourage you to **actively engage** with them. Change the parameters, break the assumptions, and observe the results. What happens if you increase the learning rate in an optimization algorithm? What if you introduce a different type of shock into a macroeconomic model? This kind of hands-on exploration is one of the most effective ways to build intuition.\n",
    "\n",
    "3.  **Connect Theory to Code, and Code to Theory:** We will frequently move between mathematical notation and its corresponding implementation in Python. Your goal should be to see these not as two separate languages, but as two representations of the same underlying logic. When you see a summation sign ($\\sum$) in an equation, you should start to think of a `for` loop or a vectorized `np.sum()` operation in code. When you see a class definition in Python, you should think about how it represents a formal economic object, like a utility function or a production technology.\n",
    "\n",
    "4.  **Cultivate a \"Growth Mindset\":** You will inevitably encounter bugs, errors, and models that do not work on the first try. This is not a sign of failure; it is the essence of the scientific and programming process. Every error message is a learning opportunity. The goal of this course is not just to teach you computational economics, but to teach you *how to learn* computational economics. This means developing the skills of debugging, consulting documentation, and systematically diagnosing problems—skills that will serve you long after this course is over.\n",
    "\n",
    "By adopting this active, inquisitive, and resilient approach, you will not just learn the material in these notebooks; you will develop the foundational skills and intellectual framework of a modern computational economist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Computational Economics?\n",
    "\n",
    "At its core, computational economics is a discipline at the intersection of economics, computer science, and numerical analysis. It involves using computational tools to analyze and solve economic models that are too complex for traditional analytical methods. As economic models grow more realistic to better capture the complexities of the real world—incorporating features like heterogeneous agents, dynamic interactions, and overlapping generations—analytical (pen-and-paper) solutions become increasingly rare.\n",
    "\n",
    "A more formal, graduate-level definition frames computational economics as the study of **finding and analyzing fixed points of theoretical operators**. An economic model can often be expressed as a mapping $T$ (an \"operator\") from a space of objects $X$ (e.g., functions, vectors) to itself. The model's solution, or *equilibrium*, is an object $x \\in X$ that is a **fixed point** of this operator, satisfying the condition:\n",
    "\n",
    "$$ x = T(x; \\theta) $$\n",
    "\n",
    "Here, $x$ is the endogenous object we want to solve for (e.g., a function, a vector of prices), and $\\theta$ represents the model's parameters (e.g., preferences, technology). The intellectual exercise of modeling is to define the space $X$ and the operator $T$ such that this fixed point equation meaningfully represents an economic equilibrium. For example:\n",
    "\n",
    "- In **dynamic programming**, the foundational equation is the **Bellman Equation**. Here, $X$ is a space of continuous, bounded functions, $x$ is the value function $V(k)$, and $T$ is the Bellman operator, which maps a candidate value function into an updated one. The solution $V^*$ satisfies the fixed point equation $V^* = T(V^*)$.\n",
    "$$ V(k) = \\max_{c, k'} \\{ u(c) + \\beta V(k') \\} \\quad \\text{s.t. constraint} $$\n",
    "\n",
    "- In a **general equilibrium** model, $X$ is the space of non-negative price vectors, $x$ is a vector of prices $\\mathbf{p}$, and $T(\\mathbf{p})$ is an operator representing the aggregate excess demand function. The fixed point $\\mathbf{p}^* = T(\\mathbf{p}^*)$ is the price vector that sets excess demand to zero, clearing all markets.\n",
    "\n",
    "- In a **game theory** context, $X$ is the space of probability distributions over actions, $x$ could be a mixed strategy profile, and $T$ would be the best-response correspondence. A Nash Equilibrium is a fixed point where each player's strategy is a best response to the other players' strategies.\n",
    "\n",
    "Most of computational economics is concerned with the practical task of finding this fixed point $x$ when the operator $T$ is too complex for an analytical solution. This involves choosing a numerical method, implementing it efficiently, and analyzing the properties of the resulting solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Theoretical Guarantee: The Contraction Mapping Theorem\n",
    "\n",
    "While we can often find fixed points computationally through iteration, a crucial question is: when are we *guaranteed* that this process will work? When can we be certain that a fixed point exists, that it is unique, and that our iterative algorithm will converge to it? The foundational result that provides these guarantees is the **Banach Fixed-Point Theorem**, more commonly known in economics as the **Contraction Mapping Theorem**.\n",
    "\n",
    "This theorem is the theoretical bedrock for solving a vast class of dynamic models. To understand it, we first need a few simple concepts:\n",
    "\n",
    "1.  **Metric Space:** A metric space $(X, d)$ is simply a set of objects $X$ (like vectors, or functions) equipped with a \"distance function\" or **metric** $d(x, y)$ that measures the distance between any two objects $x$ and $y$ in the set. A familiar example is the set of real numbers $\\mathbb{R}$ with the metric $d(x, y) = |x - y|$.\n",
    "2.  **Completeness:** A metric space is **complete** if every Cauchy sequence of points in the space has a limit that is also in the space. Intuitively, this means the space has no \"holes\" or \"missing points.\" The set of real numbers $\\mathbb{R}$ is complete, but the set of rational numbers $\\mathbb{Q}$ is not (a sequence of rational numbers can converge to an irrational number like $\\sqrt{2}$).\n",
    "3.  **Contraction Mapping:** An operator $T: X \\to X$ is a **contraction mapping** (or just a **contraction**) if there exists a number $\\beta \\in [0, 1)$ such that for any two points $x, y \\in X$:\n",
    "    $$ d(T(x), T(y)) \\le \\beta \\cdot d(x, y) $$\n",
    "    The number $\\beta$ is called the **modulus** of the contraction. This condition says that applying the operator $T$ is guaranteed to bring any two points in the space strictly closer together, scaled by a factor $\\beta$.\n",
    "\n",
    "With these definitions, we can state the theorem:\n",
    "\n",
    "> **The Contraction Mapping Theorem:** Let $(X, d)$ be a non-empty, complete metric space. If $T: X \\to X$ is a contraction mapping with modulus $\\beta$, then:\n",
    "> 1.  **Existence & Uniqueness:** $T$ has a unique fixed point, $x^*$.\n",
    "> 2.  **Convergence:** For any arbitrary starting point $x_0 \\in X$, the sequence generated by iterating the operator, $x_{n+1} = T(x_n)$, converges to the unique fixed point $x^*$.\n",
    "\n",
    "**Why this is so powerful for economists:**\n",
    "\n",
    "This theorem is the workhorse of modern dynamic programming. In that context, the space $X$ is a set of candidate value functions, and the operator $T$ is the **Bellman operator**. If we can show that the Bellman operator is a contraction mapping (which is often possible under standard assumptions, most critically that the discount factor is less than one), then the theorem provides three powerful results in one fell swoop:\n",
    "- It guarantees that a **unique** solution (the true value function) to the Bellman equation exists.\n",
    "- It tells us that we can **find** that solution using a simple, constructive algorithm: start with *any* guess for the value function and just keep iterating the Bellman operator. This algorithm is known as **value function iteration**.\n",
    "\n",
    "The Contraction Mapping Theorem thus provides the theoretical justification for a huge range of computational methods used to solve dynamic economic models. It connects the abstract mathematical structure of the model to a practical, implementable algorithm with a guarantee of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Economic Example: The Cobweb Model\n",
    "\n",
    "To make this tangible, consider the **cobweb model**, a classic dynamic model of supply and demand. It's particularly useful for illustrating fixed points in an economic context. The model's key assumption is that there is a **time lag** in production. For example, farmers must decide how much to plant *before* they know the market price at harvest time. Their planting decisions are based on the price they *expect* to receive. A simple assumption is that they have **adaptive expectations**: they expect this year's price to be the same as last year's price.\n",
    "\n",
    "The model is defined by three equations:\n",
    "1.  **Demand:** The quantity demanded today depends on today's price: $Q_t^D = a - b P_t$\n",
    "2.  **Supply:** The quantity supplied today depends on *last year's* price: $Q_t^S = c + d P_{t-1}$\n",
    "3.  **Market Clearing:** In equilibrium, quantity supplied equals quantity demanded: $Q_t^D = Q_t^S$\n",
    "\n",
    "By substituting the supply and demand equations into the market clearing condition, we can find the law of motion for prices:\n",
    "$$ a - b P_t = c + d P_{t-1} \\implies P_t = \\frac{a-c}{b} - \\frac{d}{b} P_{t-1} $$\n",
    "\n",
    "This equation describes how the price in period $t$ is a function of the price in period $t-1$. The **equilibrium price**, $P^*$, is a price that, if reached, would remain constant forever. It is a **fixed point** of the pricing equation. We can define our operator $T$ as the function that maps last year's price to this year's price:\n",
    "$$ T(P_{t-1}) = \\frac{a-c}{b} - \\frac{d}{b} P_{t-1} $$\n",
    "The equilibrium price $P^*$ is the fixed point that solves $P^* = T(P^*)$. We can find this value analytically, but we can also find it computationally by starting with a guess and iterating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key question for any dynamic model is whether this equilibrium is **stable**. That is, if the system starts away from the fixed point, does it converge towards it? The answer depends on the parameters of the model. The price dynamics are given by the linear difference equation:\n",
    "$$ P_t = \\left(-\\frac{d}{b}\\right) P_{t-1} + \\frac{a-c}{b} $$\n",
    "This is a standard form, and its stability is determined by the coefficient on the lagged variable. The system will converge to the fixed point if and only if the absolute value of this coefficient is less than 1:\n",
    "$$ \\left| -\\frac{d}{b} \\right| < 1 \\implies \\frac{d}{b} < 1 $$\n",
    "In economic terms, this means the price will converge to a stable equilibrium if the slope of the demand curve (`b`) is steeper than the slope of the supply curve (`d`). If the supply curve is steeper ($d/b > 1$), the price fluctuations will become larger and larger, and the system will be unstable. If the slopes are exactly equal ($d/b=1$), the price will oscillate around the equilibrium in a stable cycle without ever converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Model Parameters ---\n",
    "# This section defines the economic environment.\n",
    "# Demand curve is defined by Q_d = a - b*P, where 'a' is the intercept and 'b' is the slope.\n",
    "# A higher 'b' means demand is more responsive to price changes.\n",
    "a = 100  # Intercept for the demand curve, representing demand at zero price.\n",
    "b = 2.0  # Slope of the demand curve. The negative sign is implicit in the model's structure.\n",
    "\n",
    "# Supply curve is defined by Q_s = c + d*P, where 'c' is the intercept and 'd' is the slope.\n",
    "# A higher 'd' means supply is more responsive to price changes.\n",
    "c = 10   # Intercept for the supply curve.\n",
    "d = 1.5  # Slope of the supply curve. For the stable case, d must be less than b.\n",
    "\n",
    "# The function T represents the core dynamic of the model.\n",
    "# It takes the previous period's price (p_last) and calculates the current period's price.\n",
    "# This is the operator for which we seek a fixed point.\n",
    "def T(p_last, a, b, c, d):\n",
    "    \"\"\"Calculates the current price based on the last period's price.\"\"\"\n",
    "    return (a - c) / b - (d / b) * p_last\n",
    "\n",
    "# --- Computational Solution ---\n",
    "# We will find the equilibrium price by iterating the operator T from an arbitrary starting point.\n",
    "p0 = 5.0            # An initial guess for the price.\n",
    "tolerance = 1e-6    # A small number to define convergence. We stop when the price changes by less than this amount.\n",
    "max_iter = 50       # A safeguard to prevent an infinite loop if the model doesn't converge.\n",
    "price_history_stable = [p0] # A list to store the price in each period for later visualization.\n",
    "p = p0              # Initialize the current price for the loop.\n",
    "\n",
    "# This loop repeatedly applies the operator T to find the fixed point.\n",
    "for i in range(max_iter):\n",
    "    p_new = T(p, a, b, c, d)  # Calculate the next period's price.\n",
    "    price_history_stable.append(p_new) # Record the new price.\n",
    "    # Check for convergence: if the price has stopped changing significantly, we've found the fixed point.\n",
    "    if np.abs(p_new - p) < tolerance:\n",
    "        print(f\"Converged to fixed point in {i+1} iterations.\")\n",
    "        break\n",
    "    p = p_new # Update the current price for the next iteration.\n",
    "\n",
    "# --- Analytical Solution for Verification ---\n",
    "# For this simple model, we can also solve for the fixed point P* where P* = T(P*).\n",
    "# P* = (a-c)/b - (d/b)P*  =>  P*(1 + d/b) = (a-c)/b  =>  P*((b+d)/b) = (a-c)/b  =>  P* = (a-c)/(b+d)\n",
    "p_star_analytical = (a - c) / (b + d)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(price_history_stable, 'o-', label='Price Path (Stable Case, d/b < 1)')\n",
    "ax.axhline(p_star_analytical, color='red', linestyle='--', label=f'Analytical Equilibrium Price P*')\n",
    "ax.set_title('Cobweb Model: Convergence to a Fixed Point', fontsize=16)\n",
    "ax.set_xlabel('Time Period (t)', fontsize=12)\n",
    "ax.set_ylabel('Price (P_t)', fontsize=12)\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize=11)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The calculated fixed point is: {p_new:.4f}\")\n",
    "print(f\"The analytical equilibrium is:  {p_star_analytical:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Unstable Case\n",
    "Now, let's see what happens when the stability condition is violated. We will set the supply slope `d` to be greater than the demand slope `b`. The economic intuition is that suppliers are now *over-reacting* to price changes. A small price increase last period leads to a huge increase in supply this period, which in turn causes the price to crash. This crash then leads to a massive cut in supply, and the price skyrockets, leading to an explosive, divergent cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Parameters for Unstable Case ---\n",
    "# The only change from the stable case is the slope of the supply curve.\n",
    "# We now set d > b, which violates the stability condition d/b < 1.\n",
    "d_unstable = 2.5 # This value is chosen to be greater than b=2.0.\n",
    "\n",
    "# --- Computational Simulation ---\n",
    "# We simulate the price path again, using the new unstable parameter.\n",
    "# We start from the same initial price p0 as before.\n",
    "price_history_unstable = [p0]\n",
    "p = p0\n",
    "\n",
    "# The loop structure is the same, but we expect the price to diverge.\n",
    "# We remove the convergence check because the condition will likely not be met.\n",
    "for i in range(max_iter):\n",
    "    p_new = T(p, a, b, c, d_unstable) # We use the same operator T, but with the new 'd_unstable' parameter.\n",
    "    price_history_unstable.append(p_new)\n",
    "    p = p_new\n",
    "\n",
    "# --- Analytical Equilibrium (Unchanged) ---\n",
    "# It is crucial to note that the analytical fixed point itself does not change.\n",
    "# The equilibrium price is still P* = (a-c)/(b+d), but now d is d_unstable.\n",
    "p_star_unstable_analytical = (a - c) / (b + d_unstable)\n",
    "\n",
    "# --- Visualization ---\n",
    "# The plot will now show a price path that oscillates with increasing amplitude, moving away from the fixed point.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(price_history_unstable, 'o-', label='Price Path (Unstable Case, d/b > 1)', color='orange')\n",
    "# We plot the (unstable) equilibrium price to show how the system diverges from it.\n",
    "ax.axhline(p_star_unstable_analytical, color='red', linestyle='--', label=f'Analytical Unstable Equilibrium P*')\n",
    "ax.set_title('Cobweb Model: Divergence from a Fixed Point', fontsize=16)\n",
    "ax.set_xlabel('Time Period (t)', fontsize=12)\n",
    "ax.set_ylabel('Price (P_t)', fontsize=12)\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modeler's Trilemma: Realism, Generality, and Precision\n",
    "\n",
    "Before delving into the history of computation, it's crucial to understand the fundamental trade-offs inherent in all economic modeling. The ecologist Richard Levins (1966) proposed a \"trilemma,\" suggesting that a model cannot simultaneously be:\n",
    "1.  **Realistic:** Accurately capturing the complex details and heterogeneity of the real world.\n",
    "2.  **General:** Applicable to a wide range of situations, systems, or economies.\n",
    "3.  **Precise:** Yielding exact, analytical, and unambiguous results.\n",
    "\n",
    "A modeler must choose which virtues to prioritize, and therefore, which to sacrifice. As the diagram below illustrates, pursuing any two of these virtues typically requires sacrificing the third.\n",
    "\n",
    "![A diagram of Levins' Modeler's Trilemma, showing that a model cannot simultaneously be realistic, general, and precise.](../images/png/modelers_trilemma.png)\n",
    "\n",
    "This trilemma provides a powerful lens through which to view the history of economic thought:\n",
    "- The **Formalist Revolution** and the Arrow-Debreu model prioritized **generality** and **precision**, but at the cost of **realism**. It proved the existence of an equilibrium for a general class of economies but was too abstract to describe any specific one.\n",
    "- Large-scale **macroeconometric models** of the first revolution aimed for **realism** (by including hundreds of variables) and **precision** (by giving quantitative forecasts), but sacrificed **generality**. The Klein-Goldberger model was a model of the U.S. economy at a specific time, not a general theory of all economies.\n",
    "- **Agent-Based Models** often prioritize **realism** by modeling heterogeneous agents with complex interactions, but may sacrifice **precision**, as the results are often generated through simulation and can be difficult to summarize with a simple analytical formula.\n",
    "\n",
    "The history of computational economics can be seen as an ongoing effort to push the boundaries of this trilemma. Advances in computing power do not eliminate these trade-offs, but they do relax them, allowing for models that can be simultaneously more realistic, more general, or more precise than was previously possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pre-Computational Era: The Dawn of Systemic Economic Thought\n",
    "\n",
    "Long before electronic computers, the intellectual groundwork for computational economics was being laid. The core idea—that an economy could be understood as a quantifiable, interconnected system—was a radical departure from earlier descriptive or moralistic approaches. This shift in perspective, from viewing the economy as a subject of philosophical discourse to seeing it as an object of scientific measurement, was the essential precondition for the computational revolutions that would follow.\n",
    "\n",
    "#### Conceptual Origins: The Invisible Hand as Emergent Order\n",
    "\n",
    "While not quantitative, Adam Smith's concept of the **\"invisible hand\"** in *The Wealth of Nations* (1776) provided a foundational idea for complex systems thinking. Smith's insight was that a coherent, orderly macroeconomic outcome—the efficient allocation of resources—could **emerge** from the uncoordinated, self-interested actions of myriad individual agents. No central planner was needed. This concept of **emergent order**, where macroscopic patterns arise from simple microscopic interactions, is a cornerstone of modern agent-based modeling. It posed a profound question that economics has been trying to answer ever since: what are the precise mechanisms that allow for this self-organization, and when do they fail?\n",
    "\n",
    "#### The Origins of Quantitative Economics: Political Arithmetic\n",
    "\n",
    "The first systematic effort to apply empirical data to governance emerged in 17th-century England with the school of **Political Arithmetic**. Its leading proponent, **Sir William Petty (1623-1687)**, a polymath inspired by Francis Bacon's empiricism, argued for a science of statecraft grounded not in \"discourse and arguments,\" but in \"number, weight, and measure.\" In works like *A Treatise of Taxes and Contributions* (1662), Petty undertook pioneering efforts to estimate the population, income, and wealth of England. His goal was to create a rational basis for taxation and understand the sources of national power. This represented a profound conceptual leap: the belief that the complex organism of a national economy was, at its core, something that could be systematically measured and analyzed.\n",
    "\n",
    "![A portrait of Sir William Petty, an early proponent of using statistical data to analyze national economies. His work laid the philosophical foundation for econometrics and national accounting.](../images/jpg/1.1-william-petty.jpg)\n",
    "\n",
    "#### The First Systematization: From Quesnay's Tableau to Walrasian Equilibrium\n",
    "\n",
    "The effort to formalize the economy as an interconnected system took major leaps forward with François Quesnay and Léon Walras.\n",
    "\n",
    "The French physician and economist **François Quesnay (1694-1774)** created the first formal, abstract model of an entire economy. As the leader of the Physiocratic school, Quesnay believed that all true economic value—the *produit net* (net product)—originated from the land. His ***Tableau Économique*** (1758) was a diagrammatic representation of this theory, illustrating how the agricultural surplus circulated between three social classes: the \"productive\" class (farmers), the \"proprietary\" class (landowners), and the \"sterile\" class (merchants and artisans).\n",
    "\n",
    "![Quesnay's Tableau Économique from 1759. This diagram was a revolutionary attempt to visualize an entire economy as a system of interdependent flows, a direct ancestor of modern national income accounting.](../images/png/1.1-quesnay-tableau.png)\n",
    "\n",
    "While its underlying economic theory was flawed, the *Tableau*'s methodology was a conceptual breakthrough. Its \"zig-zag\" depiction of payments was the first attempt to represent the economy as a circular flow, where the outputs of some sectors become the inputs for others. It explicitly modeled the interdependence of production and consumption. This systemic view is the direct intellectual ancestor of modern input-output analysis and national income accounting.\n",
    "\n",
    "A century later, the French economist **Léon Walras (1834-1910)** took the next monumental step. In his *Elements of Pure Economics* (1874), Walras sought to answer the question posed by Adam Smith with mathematical formalism. He developed the theory of **general equilibrium**, attempting to prove that the interactions of self-interested buyers and sellers in all markets could lead to a state of simultaneous equilibrium. Walras represented the economy as a system of simultaneous equations, where the number of equations (demand and supply for every good) equaled the number of unknowns (the prices of those goods and the quantities produced). His concept of *tâtonnement* (groping) was a hypothetical process in which an auctioneer would call out prices, and agents would report their intended transactions until a set of market-clearing prices was found. While Walras lacked the mathematical tools to prove the existence of such an equilibrium, his work laid the entire conceptual foundation for the Formalist Revolution to come. It framed the central problem of economics as one of solving a massive system of simultaneous equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Twin Catalysts of the Modern Era: Formalism and War\n",
    "\n",
    "The stage for the first computational revolution was set by two powerful forces in the mid-20th century: an internal push for mathematical rigor, institutionalized by the **Cowles Commission**, and an external shock—World War II—that dramatically accelerated the development of computing technology.\n",
    "\n",
    "**1. The Formalist Revolution: The Cowles Commission's Quest for Rigor**\n",
    "\n",
    "Post-World War II economics undertook a concerted effort to rebuild itself on rigorous, axiomatic foundations. This **\"Formalist Revolution\"** sought to replace the literary style of classical economics with a universal, mathematical science. The intellectual epicenter of this movement was the **Cowles Commission for Research in Economics**. Its motto, \"Science is Measurement,\" and its explicit research program, championed by figures like **Tjalling Koopmans**, was to reconstruct economic theory using formal logic and statistics. The goal was to derive complex economic phenomena from fundamental postulates of agent behavior—primarily constrained optimization by rational actors.\n",
    "\n",
    "The crowning achievement of this program was the **Arrow-Debreu model of general equilibrium (1954)**. Kenneth Arrow and Gérard Debreu, both deeply influenced by the Cowles ethos, fulfilled Walras's century-old ambition. They used advanced mathematics (specifically, Kakutani's fixed-point theorem) to prove the existence of a set of prices that would bring all markets in a competitive economy into simultaneous equilibrium. Their intellectual leap was to map the entire economy into a mathematical space and show that an equilibrium was equivalent to a fixed point in that space. Proving the fixed point existed was equivalent to proving equilibrium existed.\n",
    "\n",
    "<table style='width:100%; border:0px'><tr>\n",
    "<td style='width:50%; border:0px; text-align:center;'>\n",
    "<figure>\n",
    "  <img src='../images/jpg/1.1-kenneth-arrow.jpg' alt='Kenneth Arrow'>\n",
    "  <figcaption>Kenneth Arrow (1921-2017)</figcaption>\n",
    "</figure>\n",
    "</td>\n",
    "<td style='width:50%; border:0px; text-align:center;'>\n",
    "<figure>\n",
    "  <img src='../images/jpg/1.1-gerard-debreu.jpeg' alt='Gérard Debreu'>\n",
    "  <figcaption>Gérard Debreu (1921-2004)</figcaption>\n",
    "</figure>\n",
    "</td>\n",
    "</tr></table>\n",
    "<center><i>The architects of the General Equilibrium model, which established the mathematical conditions for the existence of a market-clearing equilibrium.</i></center>\n",
    "\n",
    "However, the model was profoundly abstract and non-constructive. It proved that such an equilibrium *existed* but offered no practical method for *finding* or *computing* it. This created a tension: the most celebrated model in theoretical economics was empirically empty and computationally intractable. This gap between abstract existence proofs and concrete, quantitative application generated a latent demand for tools that could bridge the chasm—a demand that the first generation of computers would begin to meet.\n",
    "\n",
    "**2. The Wartime Forges of Computation**\n",
    "\n",
    "The second catalyst was external and geopolitical. World War II and the ensuing Cold War presented computational challenges on an unprecedented scale. Problems like calculating artillery firing tables, simulating nuclear chain reactions for the Manhattan Project, and breaking cryptographic codes demanded computational power far beyond human capacity. These were not commercial ventures but state-funded, mission-driven projects.\n",
    "\n",
    "This massive government investment directly led to the first general-purpose electronic computers. The **ENIAC (Electronic Numerical Integrator and Computer)**, funded by the U.S. Army, was a direct product of this effort. After the war, the Cold War continued this state patronage, with agencies like the Department of Defense funding hardware and software for logistics, planning, and operations research—fields that would later become intertwined with economics. When economists began searching for tools to operationalize their theories, they found them ready-made, forged in the crucible of war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "margin-note"
    ]
   },
   "source": [
    "#### Biographical Vignette: John von Neumann (1903-1957)\n",
    "\n",
    "No single figure better embodies the twin catalysts of formalism and computation than John von Neumann. A Hungarian-American polymath, his contributions spanned mathematics, physics, and computer science. In economics, he co-authored the 1944 book *Theory of Games and Economic Behavior* with Oskar Morgenstern, a foundational text of game theory that provided a new mathematical language for analyzing strategic interaction. \n",
    "\n",
    "Simultaneously, through his work on the Manhattan Project, von Neumann architected the design for the modern stored-program computer—the **von Neumann architecture**—in which both data and program instructions are held in the same memory. This innovation, outlined in his *First Draft of a Report on the EDVAC*, made computers truly programmable and flexible. Von Neumann's career thus perfectly illustrates the synergy: his theoretical work in economics created new demands for computation, while his applied work in defense helped build the very machines that could meet them.\n",
    "\n",
    "![John von Neumann, a central figure in the development of both game theory and the modern computer.](../images/gif/1.1-john-von-neumann.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Four Revolutions of Computational Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Revolution: Econometrics and the Mainframe (c. 1940s-1970s)\n",
    "\n",
    "The first computational revolution was born from a technocratic ambition: to actively manage the national economy. Fueled by the post-war Keynesian consensus, there was a widespread belief that the economy could be fine-tuned. This revolution fused the newly formalized Keynesian macroeconomic framework with statistical methods developed at institutions like the **Cowles Commission** and the immense computational power of the mainframe computer. The goal was to build large-scale macroeconometric models to forecast economic activity and simulate the effects of policy interventions.\n",
    "\n",
    "The computational challenges were immense. In the pre-digital era, Bill Phillips's **MONIAC (1949)**, a hydraulic computer that used water flowing through pipes to simulate the circular flow of the British economy, was a testament to the desire to visualize and compute economic models. The arrival of machines like the ENIAC and later the IBM System/360, programmed with punch cards and FORTRAN, represented a quantum leap. Economists like Wassily Leontief and Lawrence Klein seized on this new power.\n",
    "\n",
    "![An IBM System/360 mainframe computer, circa 1966. These machines were the workhorses of the first computational revolution, enabling the construction of large-scale econometric models.](../images/jpg/1.1-ibm-system360.jpg)\n",
    "\n",
    "The paradigmatic achievement of this era was the **Klein-Goldberger model (1955)**. Developed by Lawrence Klein, it consisted of a system of 20 simultaneous equations representing aggregate consumption, investment, wages, profits, and other macroeconomic variables. The model's parameters were estimated from historical U.S. time-series data using newly developed econometric techniques. Its purpose was explicitly for policy simulation: an analyst could change a policy variable (e.g., government spending), solve the system, and generate a forecast of its impact on GNP, inflation, and unemployment. This embodied the post-war dream of economic fine-tuning, made manifest in FORTRAN code and punch cards.\n",
    "\n",
    "![Two of the ENIAC programmers, Betty Jennings (left) and Frances Bilas, preparing the computer for a demonstration. Early programming was a highly manual and intricate task.](../images/gif/1.1-eniac-programmers.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "margin-note"
    ]
   },
   "source": [
    "#### Biographical Vignette: Wassily Leontief (1906-1999)\n",
    "\n",
    "Wassily Leontief, a Russian-American economist, was awarded the Nobel Prize in 1973 for his development of **input-output analysis**. Inspired by Quesnay's *Tableau*, Leontief created a quantitative framework for analyzing the interdependence of industries. The core of his model is an **input-output table**, a matrix $A$ where each entry $A_{ij}$ represents the amount of industry $i$'s output needed to produce one unit of industry $j$'s output. \n",
    "\n",
    "Let $x$ be the vector of gross outputs for each industry, and $d$ be the vector of final demand. The fundamental relationship is that total output must equal intermediate demand plus final demand: \n",
    "$$ x = Ax + d $$\n",
    "To find the output $x$ required to satisfy a given final demand $d$, we solve the system:\n",
    "$$ (I - A)x = d \\implies x = (I - A)^{-1} d $$\n",
    "The matrix $(I - A)^{-1}$ is the **Leontief inverse**. For the solution $x$ to be economically meaningful (i.e., non-negative for any non-negative final demand $d$), the Leontief inverse must have all non-negative entries. A key result in linear algebra, the **Hawkins-Simon condition**, states that this is true if and only if all the principal minors of the matrix $(I-A)$ are positive. This condition ensures that the production process is 'productive'—it does not consume more of a good as an input than it produces as an output. In the 1940s, inverting this matrix for a 42-sector representation of the U.S. economy on the Harvard Mark II computer was a monumental task, taking over 50 hours.\n",
    "\n",
    "This can be demonstrated with a simple 3-sector economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's model a simple 3-sector economy: Agriculture, Manufacturing, and Services.\n",
    "# The technology matrix A shows the input requirements for each sector.\n",
    "# Rows represent inputs, columns represent outputs.\n",
    "# For example, A[0, 1] is the amount of Agriculture goods needed to produce $1 of Manufacturing goods.\n",
    "A = np.array([\n",
    "    [0.2, 0.3, 0.1],  # Inputs from Agriculture to produce $1 of (Agri, Manuf, Serv)\n",
    "    [0.1, 0.3, 0.2],  # Inputs from Manufacturing\n",
    "    [0.1, 0.1, 0.3]   # Inputs from Services\n",
    "])\n",
    "\n",
    "# The final demand vector d represents consumption, government spending, and exports.\n",
    "d = np.array([100, 200, 300]) # Final demand for (Agri, Manuf, Serv) in Billions\n",
    "\n",
    "# --- Solve the Model ---\n",
    "# 1. Create the (I - A) matrix\n",
    "I = np.identity(3)\n",
    "I_minus_A = I - A\n",
    "\n",
    "# 2. Calculate the Leontief Inverse\n",
    "leontief_inverse = np.linalg.inv(I_minus_A)\n",
    "\n",
    "# 3. Calculate the required gross output x\n",
    "x = leontief_inverse @ d\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"Technology Matrix (A):\\n\", A)\n",
    "print(\"\\nFinal Demand (d):\\n\", d)\n",
    "print(\"\\nLeontief Inverse (I - A)^-1:\\n\", leontief_inverse.round(3))\n",
    "print(\"\\nRequired Gross Output (x):\\n\", x.round(2))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"To meet the final demand, the economy must produce a gross output of:\")\n",
    "print(f\"- ${x[0]:.2f}B in Agriculture\")\n",
    "print(f\"- ${x[1]:.2f}B in Manufacturing\")\n",
    "print(f\"- ${x[2]:.2f}B in Services\")\n",
    "\n",
    "# Verification: Check if x = Ax + d holds\n",
    "total_demand = A @ x + d\n",
    "print(\"\\nVerification (Gross Output == Total Demand):\", np.allclose(x, total_demand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wassily Leontief, who developed input-output analysis, a foundational tool of quantitative economics.](../images/jpg/1.1-wassily-leontief.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Lucas Critique: A Crisis of Confidence\n",
    "\n",
    "The dominance of large-scale macroeconometric models crumbled in the 1970s. They failed to predict the stagflation of that decade—the simultaneous rise in both inflation and unemployment, which contradicted the stable Phillips Curve relationship at the heart of many models. More fundamentally, they were attacked on theoretical grounds by Robert Lucas. The **Lucas Critique (1976)** argued that the estimated parameters of these models were not deep, structural features of the economy. Instead, they were reduced-form correlations that reflected the behavior of rational agents under a *specific* policy regime.\n",
    "\n",
    "Lucas argued that any change in policy would systematically alter the structure of the econometric models themselves. For example, the observed statistical relationship between interest rates and investment was conditional on agents' expectations of the central bank's historical behavior. If the central bank were to adopt a new, more aggressive inflation-fighting strategy, rational firms would anticipate this, altering their investment decisions in ways the old parameter could not predict. The model's parameters would shift, rendering it useless for evaluating the very policy change it was meant to analyze. It was a devastating critique: the models were unreliable for the primary purpose for which they were built. This triggered a crisis in macroeconomics and set the stage for a new, micro-founded approach.\n",
    "\n",
    "![Robert E. Lucas Jr., Nobel laureate and author of the influential Lucas Critique.](../images/jpg/1.1-robert-lucas.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Phillips Curve: A Case Study for the Critique\n",
    "\n",
    "The most famous target of the Lucas Critique was the **Phillips Curve**. In 1958, A.W. Phillips observed a stable, inverse empirical relationship between the rate of unemployment and the rate of wage inflation in the UK. This was later adapted by economists like Paul Samuelson and Robert Solow into a relationship between unemployment and price inflation. For a time, it presented policymakers with an apparent menu of options: they could \"buy\" lower unemployment at the cost of higher inflation, or vice versa. This relationship was a cornerstone of the large-scale macroeconometric models of the 1960s.\n",
    "\n",
    "However, in the 1970s, this relationship completely broke down. Countries began experiencing **stagflation**—high inflation and high unemployment simultaneously—which was impossible according to the simple Phillips Curve. The neat inverse relationship visible in the 1950s and 60s vanished, as shown in the stylized figure below.\n",
    "\n",
    "![A data-driven visualization showing the breakdown of the Phillips Curve relationship in the 1970s.](../images/png/phillips_curve_breakdown.png)\n",
    "\n",
    "Lucas, building on earlier work by Milton Friedman and Edmund Phelps, argued that the original trade-off was not a structural feature of the economy. It was a statistical artifact that existed only because policymakers had not yet tried to systematically exploit it. \n",
    "\n",
    "The argument, grounded in rational expectations, is as follows:\n",
    "1.  **The Old Regime:** In the past, inflation was low and stable, so people's *expectations* of future inflation were also low and stable. When the government engaged in expansionary policy, it created a surprise burst of inflation. Firms saw their prices rising and, mistaking this for an increase in real demand, hired more workers. Workers saw their nominal wages rise and, mistaking this for an increase in real wages, accepted jobs more readily. The result was a temporary drop in unemployment.\n",
    "2.  **The Policy Change:** Policymakers, believing in the stable trade-off, began to systematically pursue expansionary policy to keep unemployment low.\n",
    "3.  **The New Regime:** But rational agents cannot be fooled forever. People caught on. They began to *expect* higher inflation and incorporated these expectations into their wage and price-setting behavior. The \"surprise\" element vanished. \n",
    "\n",
    "Once expectations adjusted, the only result of systematically expansionary policy was higher inflation, with no long-term reduction in unemployment. The policy change itself destroyed the statistical relationship on which it was based. The Phillips Curve, it turned out, was a reduced-form correlation, not a deep, structural feature of the economy.\n",
    "\n",
    "##### A Simple Coded Illustration of the Lucas Critique\n",
    "\n",
    "To make this concrete, let's consider a simple model of consumption. An econometrician wants to estimate the relationship between consumption ($C_t$) and income ($Y_t$). According to economic theory (e.g., Milton Friedman's Permanent Income Hypothesis), consumption decisions are based on *permanent* income—the household's expected long-run average income—rather than temporary windfalls.\n",
    "\n",
    "We will simulate two different policy regimes:\n",
    "1.  **Regime 1 (Stable):** Income shocks are persistent. A rise in income today is a good indicator that income will be higher in the future. In this world, rational households have a high **Marginal Propensity to Consume (MPC)** out of current income, because it reflects a change in their permanent income. We'll set this true MPC to 0.8.\n",
    "2.  **Regime 2 (Volatile):** The government adopts a new policy of sending out random, one-time stimulus checks. Income shocks are now purely transitory. A rise in income today says nothing about future income. In this world, rational households will save most of the windfall, leading to a low MPC. We'll set this true MPC to 0.1.\n",
    "\n",
    "The Lucas Critique in this context says that an econometrician who estimates the MPC using data *only* from Regime 1 will make systematically wrong predictions if they try to use that model to evaluate the effects of the stimulus policy in Regime 2. The parameter itself changes when the policy regime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility of random data\n",
    "np.random.seed(42)\n",
    "n_obs = 100 # Number of observations (e.g., time periods)\n",
    "\n",
    "# --- Regime 1: Stable Income Process ---\n",
    "# This represents a policy regime where income changes are persistent (e.g., career progression).\n",
    "# Rational agents expect income changes to be mostly permanent.\n",
    "# We model this as a random walk, where each period's income builds on the last.\n",
    "permanent_income_shocks = np.random.randn(n_obs).cumsum() # Cumulative sum creates persistence\n",
    "income_regime1 = 100 + permanent_income_shocks\n",
    "# In this world, the true Marginal Propensity to Consume (MPC) out of current income is high (0.8)\n",
    "# because a change in current income signals a change in permanent income.\n",
    "consumption_regime1 = 20 + 0.8 * income_regime1 + np.random.randn(n_obs) # Add some random noise\n",
    "\n",
    "# --- Regime 2: Transitory Income Process ---\n",
    "# A new policy regime is introduced (e.g., one-off stimulus checks).\n",
    "# Income shocks are now purely temporary and have no bearing on future income.\n",
    "transitory_income_shocks = np.random.randn(n_obs) * 5 # Shocks are larger but not persistent\n",
    "income_regime2 = 100 + transitory_income_shocks\n",
    "# In this world, the true MPC is very low (0.1) because rational agents will save\n",
    "# most of the temporary windfall rather than consume it.\n",
    "consumption_regime2 = 20 + 0.1 * income_regime2 + np.random.randn(n_obs) # Add some random noise\n",
    "\n",
    "# --- The Econometrician's Mistake ---\n",
    "# 1. Estimate the consumption function using data ONLY from the first, stable regime.\n",
    "# The econometrician observes a strong relationship between consumption and income.\n",
    "X = sm.add_constant(income_regime1) # Add a constant for the intercept term in the regression\n",
    "model_regime1 = sm.OLS(consumption_regime1, X)\n",
    "results_regime1 = model_regime1.fit()\n",
    "beta_0_estimated, beta_1_estimated = results_regime1.params\n",
    "print(f\"Estimated MPC from Regime 1 data: {beta_1_estimated:.3f} (The true parameter was 0.8)\")\n",
    "\n",
    "# 2. Use this flawed model to predict the effect of the policy in Regime 2.\n",
    "# The econometrician wrongly assumes the MPC parameter (beta_1) is a deep, structural constant.\n",
    "# They use the MPC estimated from Regime 1 to predict consumption in Regime 2.\n",
    "predicted_consumption_regime2 = beta_0_estimated + beta_1_estimated * income_regime2\n",
    "\n",
    "# --- Plotting the Results to Visualize the Failure ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot the actual, true data from Regime 2 (the low-MPC world)\n",
    "ax.scatter(income_regime2, consumption_regime2, label='Actual Behavior in Regime 2 (True MPC=0.1)', alpha=0.8, s=50)\n",
    "\n",
    "# Plot the econometrician's incorrect prediction for Regime 2\n",
    "ax.plot(income_regime2, predicted_consumption_regime2, 'r-', lw=2.5, \n",
    "        label=f'Model Prediction based on Regime 1 (Estimated MPC={beta_1_estimated:.3f})')\n",
    "\n",
    "ax.set_title('The Lucas Critique: Policy-Dependent Parameters', fontsize=16)\n",
    "ax.set_xlabel('Income', fontsize=12)\n",
    "ax.set_ylabel('Consumption', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "plt.show()\n",
    "\n",
    "print(\"The plot starkly illustrates the failure. The red prediction line, based on the old policy regime's high MPC, \\n\",\n",
    "      \"systematically over-predicts the consumption response to income changes in the new regime. \\n\",\n",
    "      \"The underlying behavior (the true MPC) changed with the policy, invalidating the model for policy evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Second Revolution: The PC and the Credibility Revolution (c. 1980-2000)\n",
    "\n",
    "The second revolution was a paradigm shift away from large-scale, theory-heavy modeling towards a more focused, empirical goal: **causal inference**. This shift was propelled by two key developments: the advent of the **personal computer (PC)**, which democratized computational power, and the increasing availability of large-scale **micro-datasets** (e.g., the Panel Study of Income Dynamics, National Longitudinal Surveys).\n",
    "\n",
    "In the wake of the Lucas Critique, the ambition of modeling the entire economy gave way to the more modest but arguably more achievable goal of credibly identifying a single causal effect. This movement, championed by economists like Joshua Angrist, Guido Imbens, and David Card, became known as the **\"credibility revolution.\"** It championed the use of clever research designs to isolate causal relationships from observational data. The core idea was that while the real world is a messy, uncontrolled place, one could find specific situations—**\"natural experiments\"**—where a policy change or other event created a clean comparison between a treatment and a control group, mimicking a randomized controlled trial (RCT).\n",
    "\n",
    "The philosophy was one of transparency and methodological rigor. The focus shifted from the theoretical elegance of a model to the plausibility and robustness of the research design. User-friendly statistical software like Stata became the workhorse for implementing the techniques that defined this era: **Difference-in-Differences (DiD)**, **Instrumental Variables (IV)**, and **Regression Discontinuity Design (RDD)**. This came with a trade-off: the 'credibility revolution' often prioritized internal validity (the confidence that the identified effect is causal within the study's context) over external validity (the confidence that the effect generalizes to other populations and settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Third Revolution: Computation as a Laboratory (c. 2000-Present)\n",
    "\n",
    "The third revolution represents another shift in the role of computation. Driven by exponential growth in computing power, the computer has evolved from a tool for *analyzing data* to a tool for *generating economic theory itself*. It has become a virtual laboratory where economists build and simulate artificial economies to understand phenomena that are analytically intractable. This approach allows for models with a level of complexity and realism that was unthinkable a generation ago. The three most prominent classes of models in this revolution are DSGE, HANK, and ABMs.\n",
    "\n",
    "- **Dynamic Stochastic General Equilibrium (DSGE) Models:** These are the workhorse models of modern macroeconomics, representing a direct response to the Lucas Critique. The core philosophy is to derive all macroeconomic relationships from the \"deep\" parameters of the economy: the preferences of households, the technology of firms, and the constraints they face. They are *dynamic* (evolving over time), *stochastic* (hit by random shocks), and in *general equilibrium* (all markets clear). A typical DSGE model solves the optimization problem of a single, \"representative\" household, such as:\n",
    "  $$ \\max_{\\{c_t, k_{t+1}\\}} E_0 \\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\quad \\text{s.t.} \\quad c_t + k_{t+1} = A_t k_t^\\alpha + (1-\\delta)k_t $$\n",
    "  While powerful for their theoretical consistency, their reliance on a single representative agent makes them poorly suited for studying questions related to inequality.\n",
    "\n",
    "- **Heterogeneous Agent New Keynesian (HANK) Models:** HANK models are a computationally intensive extension of the DSGE framework. They replace the single representative agent with a large population of *heterogeneous* agents who differ in their income, wealth, and face realistic constraints (e.g., borrowing limits). This heterogeneity is crucial for studying the distributional consequences of macroeconomic policy. The cost of this realism is a significant increase in computational complexity. The state variable of the model is now the full distribution of agents over wealth and income, which is an infinite-dimensional object. This 'curse of dimensionality' means that solutions require sophisticated numerical methods (like projection methods or perturbation) and often high-performance computing.\n",
    "\n",
    "- **Agent-Based Models (ABMs):** ABMs represent a different philosophical approach. Instead of assuming hyper-rational optimizers, ABMs populate a simulated world with agents who follow simpler, often heuristic, behavioral rules. Macroeconomic phenomena are not imposed from the top-down but rather *emerge* from the bottom-up interactions of these agents. For example, a financial crisis in an ABM might emerge organically from the cascading failure of individual, rule-of-thumb traders, rather than from an external, random \"shock\" as in a DSGE model. ABMs are particularly well-suited for studying complex systems and emergent phenomena where the assumption of perfect rationality is a poor approximation of reality.\n",
    "\n",
    "This evolution has triggered a change in the definition of an economic model. In the past, the model was the set of elegant equations in a journal article. Today, for a growing number of economists, **the model *is the code***. The formal publication serves as a high-level description, but the code itself—the specific implementation of the agent's problem, the solution algorithm, the calibration—is the ground truth of the scientific claim. This paradigm demands a new level of transparency and has led to the adoption of best practices from software engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The (Emerging) Fourth Revolution: AI as a Modeling Partner\n",
    "\n",
    "While the third revolution continues, the outlines of a fourth are already taking shape, driven by progress in **Artificial Intelligence (AI) and Machine Learning (ML)**. This emerging revolution does not replace the simulation-based approach of the third, but rather augments it, creating a new synthesis where AI acts as a partner in the modeling process. This partnership is unfolding in several key areas:\n",
    "\n",
    "- **Solving Intractable Models:** One of the biggest bottlenecks in modern macroeconomics is solving the policy functions of heterogeneous agents. Economists have begun using **Deep Learning** to find accurate *approximate* solutions to these complex models, turning problems that were once computationally intractable into manageable tasks (e.g., Lilia Maliar, Serguei Maliar, and Inna Tsener (2021), \"Deep learning for solving and estimating dynamic models\"). AI here acts as a powerful numerical solver, dramatically expanding the scope of what can be analyzed.\n",
    "\n",
    "- **Causal Machine Learning:** The second revolution's focus on causality is merging with the predictive power of ML. New techniques in **Causal ML** (e.g., Athey and Imbens (2016), \"Recursive partitioning for heterogeneous causal effects\") use algorithms like random forests to estimate treatment effects that vary across a population, allowing for more nuanced causal estimates than traditional linear models.\n",
    "\n",
    "- **Generative Models as Artificial Worlds:** AI, particularly **Generative AI**, can be used to create realistic simulations of economic environments. Instead of programming agent rules by hand (as in ABMs), economists can train generative models on real-world data to learn the complex behaviors and interactions of economic agents. This allows for the creation of high-fidelity \"digital twin\" economies for policy experiments.\n",
    "\n",
    "- **Automated Economic Discovery:** Researchers are exploring the use of AI to not just solve models, but to discover new economic mechanisms. For instance, **Reinforcement Learning** has been used to design optimal tax policies or auction mechanisms in simulated environments, sometimes discovering strategies that human theorists had not previously considered (e.g., Zheng et al. (2022), \"The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies\").\n",
    "\n",
    "This fourth revolution is still in its infancy, but it promises to further blur the lines between theorist, econometrician, and computer scientist. It raises new philosophical questions: If a model's logic is contained within the millions of learned parameters of a neural network, what does it mean to 'understand' the economic mechanism? Is the AI a 'black box' solver, or can we develop new techniques to interpret its logic? The economist of the near future will need to be a 'modeler of modelers', designing, training, and interpreting AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Python for Computational Economics?\n",
    "\n",
    "The choice of a programming language for a scientific task always involves a trade-off between **developer time** and **computation time**. The goal is to minimize the *total time* to solution, which includes not just the time it takes for the computer to run the code, but also the time it takes for the researcher to write, debug, and verify it.\n",
    "\n",
    "- **Low-level languages** (like C++ or Fortran) are \"close to the metal.\" They give the programmer fine-grained control over computer memory and processes, which allows for highly optimized, fast-executing code. However, this control comes at the cost of being more verbose, complex, and time-consuming to write.\n",
    "- **High-level languages** (like Python, R, or MATLAB) are more abstract. They handle many of the complex details of memory management automatically, allowing the programmer to express complex ideas more concisely. The trade-off is that this abstraction can sometimes lead to slower execution speeds.\n",
    "\n",
    "To make this concrete, consider the simple task of summing a list of numbers. Here is how you might do it in C++:\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <numeric> // Required for std::accumulate\n",
    "\n",
    "int main() {\n",
    "    std::vector<double> numbers = {1.5, 2.5, 3.5, 4.5, 5.5};\n",
    "    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n",
    "    std::cout << \"The sum is: \" << sum << std::endl;\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "Now, here is the equivalent task in Python:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "numbers = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "total = np.sum(numbers)\n",
    "print(f\"The sum is: {total}\")\n",
    "```\n",
    "The Python version is not only shorter but is arguably more readable and closer to the mathematical idea it represents. For a research project, where the logic is often far more complex than a simple sum, this difference in developer time becomes a decisive factor.\n",
    "\n",
    "Python has emerged as a dominant language in economics and data science for several key reasons that build on this advantage:\n",
    "\n",
    "- **A World-Class Scientific Ecosystem:** Python's greatest strength is its rich collection of high-quality, open-source libraries. These libraries form a powerful, integrated ecosystem for scientific computing:\n",
    "    - **NumPy:** The foundational package for numerical computing, providing efficient array objects and mathematical functions.\n",
    "    - **SciPy:** Builds on NumPy to provide a vast collection of algorithms for optimization, integration, linear algebra, statistics, and more.\n",
    "    - **Pandas:** Provides powerful and intuitive data structures (like the DataFrame) for manipulating and analyzing tabular data.\n",
    "    - **Matplotlib** and **Seaborn:** For comprehensive and publication-quality data visualization.\n",
    "    - **Statsmodels:** For classical statistical modeling and econometrics.\n",
    "- **Performance at the Right Level:** While pure Python code can be slow, the core scientific libraries (like NumPy and SciPy) are written in C or Fortran under the hood. When you perform a vectorized operation like `np.sum(numbers)`, you are actually executing a highly optimized, pre-compiled C loop. This gives you the best of both worlds: the expressive power of a high-level language and the speed of a low-level one for critical numerical tasks. For cases where custom logic is the bottleneck, tools like **Numba** and **Cython** allow you to selectively compile your Python code to achieve massive speedups.\n",
    "- **The Language of Machine Learning:** Python is the undisputed leader in machine learning and artificial intelligence, providing access to state-of-the-art libraries like **Scikit-Learn**, **TensorFlow**, and **PyTorch**. As economics increasingly incorporates ML techniques, fluency in Python becomes essential.\n",
    "- **A Massive, Active Community:** Its immense popularity means there is a massive community providing support (e.g., Stack Overflow), tutorials, and a vast array of third-party packages for almost any task imaginable. If you have a problem, it is highly likely someone else has already solved it and shared the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Roadmap for This Course\n",
    "\n",
    "This course is designed to equip you with the fundamental skills for modern economic analysis, taking you from the basics of programming to the frontiers of computational research. We will journey through the economist's computational toolkit, mirroring the historical and thematic revolutions discussed above. You will learn to:\n",
    "\n",
    "1.  **Translate Economic Models into Code:** Implement a given economic model and solve it numerically.\n",
    "2.  **Apply Numerical Methods:** Implement and use common computational methods, including root-finding, optimization, function approximation, and numerical integration.\n",
    "3.  **Analyze Model Behavior:** Use simulations to perform comparative statics and analyze how a model's predictions change in response to shifts in policy instruments, preferences, or technologies.\n",
    "4.  **Handle Data:** Obtain, manipulate, and visualize economic data, whether it's generated from a model or observed in the real world.\n",
    "\n",
    "Our curriculum is structured as follows:\n",
    "\n",
    "- **Part 1: The Foundations.** We begin with the essentials of the Python programming language, data structures, and the core libraries for scientific computing: NumPy and Pandas. We will also cover the principles of data visualization and debugging.\n",
    "- **Part 2: Numerical Methods.** This section covers the workhorse algorithms of computational economics, including numerical differentiation, root-finding, optimization, interpolation, and integration.\n",
    "- **Part 3: Dynamic Programming.** We dive into dynamic programming, the mathematical foundation for solving sequential decision problems over time. We will cover discrete and continuous state problems, optimal stopping, and estimation.\n",
    "- **Part 4: Macroeconomic Models.** We will build and analyze canonical macroeconomic models, including Neoclassical Growth, Real Business Cycle (RBC), Overlapping Generations (OLG), and New Keynesian models.\n",
    "- **Part 5: Microeconomic Models.** This part focuses on microeconomic theory, including general equilibrium, game theory, auction theory, and principal-agent models.\n",
    "- **Part 6: Econometrics.** We will cover the theory and practice of modern econometrics, from OLS and Maximum Likelihood to the core techniques of the credibility revolution: IV, RDD, and DiD. We will also explore advanced time series analysis and Bayesian methods.\n",
    "- **Part 7: The Frontiers.** The final part of the course explores the cutting edge of computational economics, including Machine Learning, Heterogeneous Agent (HANK) models, Reinforcement Learning, NLP, and High-Performance Computing.\n",
    "- **Part 8 & 9: Advanced Topics & Applications.** We will delve into specialized topics in finance, such as asset pricing and option pricing, as well as advanced deep learning architectures and practical skills like publishing with Quarto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Tools and Professional Norms\n",
    "\n",
    "To facilitate a reproducible and interactive learning experience, we will rely on two key tools and a core professional norm.\n",
    "\n",
    "1.  **Jupyter Notebooks:** These documents allow for the seamless integration of live code, mathematical equations (using LaTeX), visualizations, and narrative text. They are an excellent medium for exploring data, developing models, and presenting results in a clear, step-by-step manner.\n",
    "\n",
    "2.  **Git and GitHub:** Version control is an indispensable skill for any serious coding work. We will use **Git** to track changes in our code and **GitHub** to host and share our work. This ensures reproducibility and facilitates collaboration.\n",
    "\n",
    "3.  **Reproducibility:** A growing norm in economics is the principle of **reproducibility**, where authors make their code and data publicly available. This allows others to verify the findings, build upon the work, and check for errors. This practice is a direct consequence of the \"model as code\" paradigm. Throughout this course, we will emphasize writing clean, well-documented code that is easy for others to read and reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Summary\n",
    "\n",
    "This introductory chapter has laid the groundwork for our study of computational economics. We've established that the history of economics is deeply intertwined with the history of computation, with each technological leap expanding the frontier of possible research. The central theme is that computation is not merely a tool, but a paradigm that shapes how economists ask questions, build models, and define what constitutes a valid answer.\n",
    "\n",
    "**Key Concepts Introduced:**\n",
    "- **Computational Economics as Fixed-Point Problems:** The core of many economic models can be distilled to finding a fixed point $x$ of an operator $T$, such that $x = T(x)$.\n",
    "- **The Contraction Mapping Theorem:** This is the key theoretical result that guarantees the existence and uniqueness of a fixed point and ensures that we can find it through simple iteration, provided the operator is a contraction. This theorem underpins the solution methods for a vast array of dynamic models.\n",
    "- **The Four Revolutions:** We've framed the evolution of the field through four major shifts:\n",
    "  1. **Econometrics and the Mainframe:** The era of large-scale macro models for policy fine-tuning.\n",
    "  2. **The Credibility Revolution:** A shift towards identifying causal inference using natural experiments, powered by the PC and micro-data.\n",
    "  3. **Computation as a Laboratory:** The use of simulation (DSGE, HANK, ABMs) to generate theory from the bottom-up.\n",
    "  4. **AI as a Modeling Partner:** The emerging synthesis of AI/ML with economic modeling to solve intractable problems and discover new mechanisms.\n",
    "- **The Model as Code:** In modern computational economics, the code itself is the ultimate definition of the model, demanding new standards of reproducibility and transparency.\n",
    "\n",
    "As we move forward, we will build on these foundational ideas. We will start by mastering the essential tools of the trade—Python and its scientific libraries—before using them to implement the numerical methods, economic models, and analytical techniques discussed in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curated References and Further Reading\n",
    "\n",
    "This list provides a set of references, categorized for focused exploration.\n",
    "\n",
    "**Primary Historical Sources & Foundational Texts**\n",
    "- Leontief, W. (1986). *Input-Output Economics*. 2nd ed. Oxford University Press. (The definitive collection of his work on the topic.)\n",
    "- Lucas, R. E. (1976). Econometric Policy Evaluation: A Critique. *Carnegie-Rochester Conference Series on Public Policy*, 1, 19-46. (The paper that launched the critique.)\n",
    "- Schelling, T. C. (1978). *Micromotives and Macrobehavior*. W. W. Norton & Company. (A masterclass in agent-based thinking, highly accessible.)\n",
    "- von Neumann, J., & Morgenstern, O. (1944). *Theory of Games and Economic Behavior*. Princeton University Press. (The book that founded game theory.)\n",
    "\n",
    "**On the History of Economic Thought & Revolutions**\n",
    "- Blaug, M. (2003). The Formalist Revolution of the 1950s. *Journal of the History of Economic Thought*, 25(2), 145-156. (A critical perspective on the shift towards mathematical formalism.)\n",
    "- Cherrier, B. (2017). The Econo-technical Aesthetic in Cold War America: A Tale of Three Market Simulators. *Engineering Studies*, 9(1), 27-52. (A fascinating look at the military-economic nexus, including the MONIAC.)\n",
    "- Hoover, K. D. (2008). Causality in Economics and Econometrics. *The New Palgrave Dictionary of Economics*, 2nd ed.\n",
    "- Mirowski, P. (2002). *Machine Dreams: Economics Becomes a Cyborg Science*. Cambridge University Press. (A dense, critical, but highly influential history of the co-evolution of economics and computation.)\n",
    "\n",
    "**Modern Econometrics and Causal Inference**\n",
    "- Angrist, J. D., & Pischke, J. S. (2009). *Mostly Harmless Econometrics: An Empiricist's Companion*. Princeton University Press. (The essential reference for the credibility revolution.)\n",
    "- Athey, S., & Imbens, G. W. (2017). The State of Applied Econometrics: Causality and Policy Evaluation. *Journal of Economic Perspectives*, 31(2), 3-32. (An excellent, accessible summary of the field by two of its leaders.)\n",
    "- Cunningham, S. (2021). *Causal Inference: The Mixtape*. Yale University Press. (A modern, accessible, and code-oriented introduction to causal methods.)\n",
    "\n",
    "**Agent-Based and Complex Systems Modeling**\n",
    "- Epstein, J. M. (2006). *Generative Social Science: Studies in Agent-Based Computational Modeling*. Princeton University Press.\n",
    "- Tesfatsion, L., & Judd, K. L. (Eds.). (2006). *Handbook of Computational Economics, Vol. 2: Agent-Based Computational Economics*. North-Holland.\n",
    "\n",
    "**Frontiers in Computational Economics & AI/ML**\n",
    "- Athey, S., & Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. *Proceedings of the National Academy of Sciences*, 113(27), 7353-7360.\n",
    "- Maliar, L., Maliar, S., & Tsener, I. (2021). Deep learning for solving and estimating dynamic models. *Available at SSRN 3806207*.\n",
    "- Zheng, S., et al. (2022). The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies. *Science Advances*, 8(25), eabk2607."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    \"\"\"Prints a formatted message with a notebook icon.\"\"\"\n",
    "    formatted_msg = textwrap.fill(msg, width=100, subsequent_indent='   ')\n",
    "    print(f\"\\n\ud83d\udcdd {formatted_msg}\", **kwargs)\n",
    "def sec(title):\n",
    "    \"\"\"Prints a formatted section title for code blocks.\"\"\"\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(f\"Environment initialized. Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations\n",
    "## Chapter 1.13: Pandas: The Framework for Practical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Labeled, Relational Data for Economics\n",
    "\n",
    "While NumPy provides the engine for high-performance numerical computing, it lacks features for handling labeled, heterogeneous data—the kind of messy, real-world data ubiquitous in economics. **Pandas** is the library that solves this problem. It provides two primary data structures, the `Series` (1D) and the `DataFrame` (2D), which are built on NumPy but add meaningful labels for rows and columns. \n",
    "\n",
    "This integration of data and labels, combined with a rich API for data manipulation (cleaning, merging, reshaping, aggregating), makes Pandas the indispensable tool for empirical analysis in Python. Whether you are cleaning survey data, analyzing financial time series, or preparing a dataset for an econometric model, Pandas provides the tools to do so efficiently and expressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Pandas Data Model: `Series`, `DataFrame`, and `Index`\n",
    "\n",
    "Understanding the internal structure of Pandas objects is key to using them effectively.\n",
    "- **`Series`**: A 1D labeled array capable of holding any data type. It is essentially a NumPy array with an associated **`Index`**.\n",
    "- **`DataFrame`**: A 2D labeled data structure with columns of potentially different types. It can be thought of as a dictionary of `Series` objects, all sharing the same `Index`.\n",
    "- **`Index`**: The crucial, immutable array of labels for the rows. The index provides fast lookups and enables **alignment**—the automatic matching of data based on labels during operations. This is a cornerstone feature that prevents a huge class of common data manipulation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"DataFrame Structure and Automatic Alignment\")\n",
    "# Note the misaligned and missing labels\n",
    "population = pd.Series({'USA': 331, 'DEU': 83, 'JPN': 126, 'CAN': 38}, name='Population (M)')\n",
    "gdp = pd.Series({'USA': 25.46, 'DEU': 4.07, 'JPN': 4.23, 'FRA': 2.96}, name='GDP ($T)')\n",
    "\n",
    "# The DataFrame aligns the data based on the shared index labels.\n",
    "# 'CAN' and 'FRA' will have missing values (NaN) where they don't have a matching label.\n",
    "df = pd.DataFrame({'Population': population, 'GDP': gdp})\n",
    "display(df)\n",
    "\n",
    "note(\"The DataFrame's Index, Columns, and underlying NumPy data:\")\n",
    "print(f\"Index: {df.index}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Underlying NumPy array:\\n{df.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading and Inspecting Data\n",
    "The first step in any data analysis project is to load and inspect the data. We will use a dataset on the economic outcomes of different college majors, compiled by FiveThirtyEight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Loading Data from a URL and Initial Inspection\")\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv'\n",
    "grads = pd.read_csv(url)\n",
    "\n",
    "note(\"Displaying the first 3 rows with .head()\")\n",
    "display(grads.head(3))\n",
    "\n",
    "note(\"Getting a concise summary of the DataFrame with .info()\")\n",
    "grads.info()\n",
    "\n",
    "note(\"\\nGenerating descriptive statistics with .describe()\")\n",
    "display(grads.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Selection and Indexing: `.loc`, `.iloc`, and `[]`\n",
    "Pandas provides a powerful and flexible indexing system. Using the explicit indexers is a best practice to write clear, unambiguous, and future-proof code.\n\n",
    "| Method | Selects By... | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **`df.loc[rows, cols]`** | **Labels** | The primary method for selecting data. Slices are inclusive. |\n",
    "| **`df.iloc[rows, cols]`** | **Integer Positions** | For when you need to select by position, like in NumPy. Slices are exclusive. |\n",
    "| **`df[col]`** | **Column Label** | A convenient shorthand for selecting a single column (`Series`). |\n",
    "| **`df[bool_series]`**| **Boolean Mask** | A convenient shorthand for filtering rows. |\n",
    "\n",
    "**`loc` is the most versatile and is generally preferred.** It can take single labels, lists of labels, slices of labels, and boolean arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `SettingWithCopyWarning`\n",
    "A common pitfall is **chained indexing** (e.g., `df[cols][rows]`). This can sometimes return a view and sometimes a copy of the data. Trying to assign a value to a copy will have no effect on the original DataFrame and will raise a `SettingWithCopyWarning`. **Always use `.loc` for assignment:** `df.loc[rows, cols] = value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Robust Data Selection with .loc\")\n",
    "grads_indexed = grads.set_index('Major')\n",
    "\n",
    "note(\"Using .loc to select the 'ECONOMICS' row and specific columns:\")\n",
    "econ_data = grads_indexed.loc['ECONOMICS', ['Median', 'Unemployment_rate']]\n",
    "display(econ_data)\n",
    "\n",
    "note(\"\\nUsing a boolean mask within .loc to find STEM majors with low unemployment:\")\n",
    "stem_categories = ['Engineering', 'Biology & Life Science', 'Computers & Mathematics', 'Physical Sciences']\n",
    "is_stem = grads_indexed['Major_category'].isin(stem_categories)\n",
    "low_unemployment = grads_indexed['Unemployment_rate'] < 0.04\n",
    "\n",
    "result = grads_indexed.loc[is_stem & low_unemployment, ['Major_category', 'Median', 'Unemployment_rate']]\n",
    "display(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Grouping and Aggregating Data\n",
    "The **split-apply-combine** strategy is a core pattern for data analysis, implemented in Pandas with the `.groupby()` method. This process involves:\n",
    "1.  **Split:** The data is split into groups based on a categorical variable (e.g., `Major_category`).\n",
    "2.  **Apply:** A function is applied independently to each group (e.g., calculating the `mean` or `sum`).\n",
    "3.  **Combine:** The results are combined back into a new DataFrame.\n",
    "\n",
    "The `.agg()` method is the most powerful tool in this process, allowing you to apply multiple aggregation functions at once and to name the resulting columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Advanced Aggregation with .groupby() and .agg()\")\n",
    "\n",
    "note(\"Using .agg() to calculate multiple, named statistics for each major category:\")\n",
    "stats_by_cat = grads.groupby('Major_category').agg(\n",
    "    Num_Majors=('Major', 'count'),\n",
    "    Median_Salary=('Median', 'median'),\n",
    "    Avg_Unemployment_Rate=('Unemployment_rate', 'mean'),\n",
    "    Std_Dev_Salary=('Median', 'std')\n",
    ").sort_values('Median_Salary', ascending=False)\n",
    "\n",
    "display(stats_by_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Grouping: `.transform` and `.filter`\n",
    "Beyond aggregation, `groupby` offers two other powerful methods:\n",
    "- **`.transform(func)`**: Applies a function to each group and returns a `Series` or `DataFrame` with the same shape as the original. This is ideal for group-wise operations like standardization (e.g., de-meaning a variable within each country group).\n",
    "- **`.filter(func)`**: Subsets the original DataFrame, keeping only the groups for which the function returns `True`. This is useful for dropping groups that don't meet a certain criterion (e.g., groups with too few observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Group-wise Standardization with .transform\")\n",
    "\n",
    "note(\"De-meaning salary within each major category to see which majors over/underperform their category average.\")\n",
    "# The lambda function is applied to the 'Median' salary Series for each group.\n",
    "demeaned_salary = grads.groupby('Major_category')['Median'].transform(lambda x: x - x.mean())\n",
    "\n",
    "grads['Demeaned_Salary'] = demeaned_salary\n",
    "display(grads.loc[grads['Major_category'] == 'Business', ['Major', 'Median', 'Demeaned_Salary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Merging and Joining Datasets\n",
    "Combining data from different sources is a fundamental task. Pandas provides `pd.merge`, a high-performance, database-style join operation.\n",
    "\n",
    "- `how='inner'` (default): Returns only the rows with matching keys in **both** DataFrames.\n",
    "- `how='left'`: Returns all rows from the **left** DataFrame, and matched rows from the right. Unmatched rows in the right get `NaN`.\n",
    "- `how='right'`: Returns all rows from the **right** DataFrame.\n",
    "- `how='outer'`: Returns all rows from **both** DataFrames, filling `NaN` where there is no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Merging DataFrames\")\n",
    "# Create a hypothetical second DataFrame with category-level information\n",
    "category_info = pd.DataFrame({\n",
    "    'Major_category': ['Engineering', 'Business', 'Humanities & Liberal Arts', 'Arts'],\n",
    "    'Difficulty': ['High', 'Medium', 'Medium', 'Low'],\n",
    "    'Is_STEM': [True, False, False, False]\n",
    "})\n",
    "\n",
    "note(\"Original graduates data (first 3 rows):\")\n",
    "display(grads[['Major', 'Major_category', 'Median']].head(3))\n",
    "\n",
    "note(\"\\nHypothetical category info data:\")\n",
    "display(category_info)\n",
    "\n",
    "# Merge the two DataFrames on the 'Major_category' column\n",
    "grads_with_info = pd.merge(grads, category_info, on='Major_category', how='left')\n",
    "\n",
    "note(\"\\nResult of the left merge (first 5 rows):\")\n",
    "display(grads_with_info[['Major', 'Major_category', 'Median', 'Difficulty', 'Is_STEM']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reshaping Data: `stack`, `unstack`, and `pivot_table`\n",
    "\n",
    "Data often needs to be reshaped between **long format** (where each observation is a row) and **wide format** (where each observation is spread across multiple columns).\n",
    "\n",
    "- `.stack()`: Pivots a level of the column labels into the index, making the DataFrame taller (longer).\n",
    "- `.unstack()`: Pivots a level of the row index into the column labels, making the DataFrame wider.\n",
    "- `pd.pivot_table()`: A powerful and user-friendly tool for creating a spreadsheet-style pivot table. It's essentially a `groupby` operation followed by a reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Using pivot_table to summarize data\")\n",
    "\n",
    "# Let's analyze the median salary by major category and gender.\n",
    "# First, we need a 'Gender' column. For this example, let's create a dummy one.\n",
    "rng = np.random.default_rng(42)\n",
    "grads['Gender'] = rng.choice(['Men', 'Women'], size=len(grads), p=[0.52, 0.48])\n",
    "\n",
    "note(\"Creating a pivot table to show median salary by category and gender:\")\n",
    "salary_pivot = pd.pivot_table(\n",
    "    grads,\n",
    "    values='Median', # The values to aggregate\n",
    "    index='Major_category', # The rows of the pivot table\n",
    "    columns='Gender', # The columns of the pivot table\n",
    "    aggfunc='mean' # The aggregation function\n",
    ")\n",
    "\n",
    "display(salary_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Time Series Analysis\n",
    "Pandas has first-class support for time series data, built around the `DatetimeIndex`. It provides a powerful toolkit for resampling, rolling window calculations, and other time-based operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Time Series Operations\")\n",
    "rng = np.random.default_rng(42)\n",
    "dates = pd.date_range(start='2022-01-01', periods=365, freq='D')\n",
    "data = rng.standard_normal(365).cumsum() + 100\n",
    "ts = pd.Series(data, index=dates, name='StockPrice')\n",
    "\n",
    "note(\"Slicing a time series with date strings:\")\n",
    "display(ts['2023-01-01':'2023-01-05'])\n",
    "\n",
    "note(\"\\nResampling daily data to monthly averages:\")\n",
    "monthly_avg = ts.resample('M').mean()\n",
    "display(monthly_avg.head())\n",
    "\n",
    "note(\"\\nCalculating a 30-day rolling moving average and standard deviation:\")\n",
    "rolling_mean = ts.rolling(window=30).mean()\n",
    "rolling_std = ts.rolling(window=30).std()\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "ts.plot(label='Daily Price', style='-', alpha=0.6)\n",
    "rolling_mean.plot(label='30-Day Rolling Mean', style='--', color='firebrick')\n",
    "plt.fill_between(rolling_mean.index, \n",
    "                 rolling_mean - 2 * rolling_std, \n",
    "                 rolling_mean + 2 * rolling_std, \n",
    "                 color='gray', alpha=0.2, label='Bollinger Bands')\n",
    "plt.legend(); plt.title('Stock Price and Rolling Statistics'); plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Writing Clean and Performant Pandas Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Method Chaining and `.pipe()`\n",
    "A hallmark of professional Pandas code is the use of **method chaining** to create readable, linear data processing pipelines. Instead of creating many intermediate variables, you chain operations together. The `.pipe()` method is a key tool in this pattern, allowing you to cleanly insert any function that takes a DataFrame and returns a DataFrame into the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Data Processing with Method Chaining\")\n",
    "\n",
    "def add_salary_range(df):\n",
    "    \"\"\"A function designed to be used in a .pipe() call.\"\"\"\n",
    "    df_out = df.copy()\n",
    "    df_out['Salary_Range'] = df_out['P75th'] - df_out['P25th']\n",
    "    return df_out\n",
    "\n",
    "note(\"A data pipeline to find the top 5 majors with the largest salary range:\")\n",
    "\n",
    "top_5_range = (\n",
    "    grads\n",
    "    .dropna(subset=['P25th', 'P75th'])\n",
    "    .pipe(add_salary_range)\n",
    "    .sort_values('Salary_Range', ascending=False)\n",
    "    .loc[:, ['Major', 'Major_category', 'P25th', 'P75th', 'Salary_Range']]\n",
    "    .head(5)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(top_5_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Using Categorical Data for Memory and Performance\n",
    "Columns that have a low number of unique values (e.g., 'Major_category') are often stored as the `object` dtype, which means each entry is a full Python string object. This is very inefficient. Converting these columns to the `Categorical` dtype can lead to massive memory savings and performance improvements in operations like `groupby`.\n",
    "\n",
    "A `Categorical` column stores the unique values once and then uses a simple integer array to represent the data, which is much more memory-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Memory Savings with Categorical Data\")\n",
    "\n",
    "note(\"Memory usage of the 'Major_category' column as an object dtype:\")\n",
    "mem_object = grads['Major_category'].memory_usage(deep=True)\n",
    "print(f\"  {mem_object / 1024:.2f} KB\")\n",
    "\n",
    "note(\"\\nConverting to a Categorical dtype:\")\n",
    "grads['Major_category'] = grads['Major_category'].astype('category')\n",
    "mem_categorical = grads['Major_category'].memory_usage(deep=True)\n",
    "print(f\"  Memory usage as categorical: {mem_categorical / 1024:.2f} KB\")\n",
    "\n",
    "note(f\"\\nMemory savings: {(1 - mem_categorical / mem_object):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Exercises\n",
    "\n",
    "1.  **Data Cleaning and Selection:** Using the `grads` DataFrame, first drop all rows that have missing values in the `Median` or `Unemployment_rate` columns (`.dropna()`). Then, using `.loc` with boolean indexing, select all majors where the median salary is greater than $60,000 and the unemployment rate is less than the overall average unemployment rate for the entire dataset.\n",
    "\n",
    "2.  **Advanced Grouping:** Group the `grads` DataFrame by `Major_category`. Use `.agg()` to find the following for each category:\n",
    "    a. The 10th percentile of `Median` salary.\n",
    "    b. The 90th percentile of `Median` salary.\n",
    "    c. The total number of `Men` and `Women` graduates (hint: `('Men', 'sum')`).\n",
    "    Create a new column in the resulting DataFrame called `Salary_Range` which is the difference between the 90th and 10th percentile salaries.\n",
    "\n",
    "3.  **Merging and Analysis:** Create a new DataFrame called `stem_status` with two columns: `Major_category` and `is_stem`. The `is_stem` column should be `True` for the categories 'Engineering', 'Biology & Life Science', 'Computers & Mathematics', 'Physical Sciences' and `False` otherwise. Merge this DataFrame with the original `grads` DataFrame. Finally, use `pivot_table` to calculate the average `Median` salary for STEM vs. non-STEM majors.\n",
    "\n",
    "4.  **Time Series Analysis:** Create a time series of 500 daily stock prices with a starting value of 200 and daily returns drawn from a normal distribution with `mean=0.0001` and `std_dev=0.01`. \n",
    "    - Resample the data to get the last price of each month (`'M'`). This is the monthly closing price.\n",
    "    - From the monthly data, calculate the monthly returns: `(price_t / price_{t-1}) - 1`.\n",
    "    - Plot the monthly returns.\n",
    "\n",
    "5.  **Reshaping with `melt`:** Start with the `salary_pivot` table you created in section 6. This is a \"wide\" DataFrame. Use `pd.melt()` to transform it into a \"long\" DataFrame with three columns: `Major_category`, `Gender`, and `Median_Salary`. This format is often required for plotting libraries like Seaborn or for econometric models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
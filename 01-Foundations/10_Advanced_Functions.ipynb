{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, datetime\n",
    "from functools import wraps, partial, lru_cache, singledispatch, reduce\n",
    "from typing import Callable, Any, List, Dict\n",
    "from contextlib import contextmanager\n",
    "import operator\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    \"\"\"Prints a formatted message with a notebook icon.\"\"\"\n",
    "    formatted_msg = textwrap.fill(msg, width=100, subsequent_indent='   ')\n",
    "    print(f\"\\n\ud83d\udcdd {formatted_msg}\", **kwargs)\n",
    "def sec(title):\n",
    "    \"\"\"Prints a formatted section title for code blocks.\"\"\"\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Foundations\n",
    "## Chapter 1.10: Advanced Function Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Functions as First-Class Citizens\n",
    "\n",
    "In Python, functions are **first-class objects**. This is a core design principle with profound implications for how we structure code. It means that a function can be treated like any other piece of data: it can be assigned to a variable, passed as an argument to another function, returned as a result from a function, and stored in data structures like lists or dictionaries. This paradigm elevates functions from mere executable blocks to versatile tools for abstraction and composition.\n",
    "\n",
    "This capability is the foundation for a style of programming known as **functional programming**, which emphasizes pure functions, immutability, and the avoidance of side effects. For the computational economist, these patterns are not mere academic curiosities; they are practical tools for building modular, testable, and maintainable models. For instance, a complex economic model can be composed of smaller, independent functions representing utility, production, and policy rules. These components can be swapped, tested in isolation, and reused across different models, leading to more robust and flexible research code.\n",
    "\n",
    "This chapter explores these advanced patterns, moving beyond basic function definition to cover the tools that unlock the full potential of functions in Python:\n",
    "\n",
    "- **First-Class Functions**: The core concept of treating functions as data.\n",
    "- **Higher-Order Functions**: Functions that accept other functions as arguments or return them as results (the Strategy Pattern).\n",
    "- **Anonymous `lambda` Functions**: Creating small, one-off functions.\n",
    "- **Closures and the `nonlocal` keyword**: Functions that carry their enclosing environment with them, enabling function factories.\n",
    "- **Callable Classes (`__call__`)**: An alternative, object-oriented way to create stateful function-like objects.\n",
    "- **Decorators**: A clean syntax for applying higher-order functions, including decorators that take arguments and the powerful built-in `@lru_cache`.\n",
    "- **Generic Functions**: Using single dispatch to create functions that adapt to different data types.\n",
    "- **`functools.partial`**: A tool for creating specialized functions by pre-filling some arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Higher-Order Functions: The Strategy Pattern\n",
    "\n",
    "A **higher-order function** is a function that operates on other functions, either by taking them as arguments or by returning them. The ability to pass a function as an argument allows for the implementation of the **Strategy Pattern**, a design pattern where the specific algorithm used by a function can be selected at runtime by passing in a different \"strategy\" function.\n",
    "\n",
    "**Economic Application:** Imagine you have a model that can be solved using different numerical methods (e.g., bisection, Newton's method) or evaluated using different utility functions (e.g., CRRA, CARA). Instead of writing a large `if/elif/else` block to select the method, you can write a single `solve` function that accepts the chosen solution algorithm as an argument. This makes the code cleaner, more modular, and easier to extend with new solution methods without modifying the `solve` function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Using Functions as Strategies\")\n",
    "\n",
    "def crra_utility(c: float, *, gamma: float) -> float:\n",
    "    \"\"\"Calculates Constant Relative Risk Aversion utility.\"\"\"\n",
    "    if not c > 0: raise ValueError(\"Consumption must be positive.\")\n",
    "    if gamma == 1:\n",
    "        return math.log(c)\n",
    "    return (c**(1 - gamma)) / (1 - gamma)\n",
    "\n",
    "def cara_utility(c: float, *, eta: float) -> float:\n",
    "    \"\"\"Calculates Constant Absolute Risk Aversion utility.\"\"\"\n",
    "    if not eta > 0: raise ValueError(\"Risk aversion coefficient eta must be positive.\")\n",
    "    return (1 - math.exp(-eta * c)) / eta\n",
    "\n",
    "# A registry mapping names to function objects. This is the core of the pattern.\n",
    "UTILITY_FUNCTIONS = {\n",
    "    'crra': crra_utility,\n",
    "    'cara': cara_utility\n",
    "}\n",
    "\n",
    "def evaluate_policy(consumption_path: List[float], utility_func: Callable, params: Dict) -> float:\n",
    "    \"\"\"A higher-order function that evaluates a policy by applying a selected utility function.\"\"\"\n",
    "    # The chosen utility function is called inside the list comprehension.\n",
    "    total_utility = sum(utility_func(c, **params) for c in consumption_path)\n",
    "    return total_utility\n",
    "\n",
    "path = [10, 11, 12]\n",
    "crra_params = {'gamma': 2.0}\n",
    "cara_params = {'eta': 0.5}\n",
    "\n",
    "note(\"Evaluating the same policy with different utility function strategies:\")\n",
    "u1 = evaluate_policy(path, UTILITY_FUNCTIONS['crra'], crra_params)\n",
    "u2 = evaluate_policy(path, UTILITY_FUNCTIONS['cara'], cara_params)\n",
    "\n",
    "print(f\"Total CRRA utility (gamma=2.0) over path {path}: {u1:.3f}\")\n",
    "print(f\"Total CARA utility (eta=0.5) over path {path}:   {u2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anonymous `lambda` Functions and the `operator` Module\n",
    "\n",
    "While `def` is the standard way to create a function, Python also provides the `lambda` keyword for creating small, anonymous functions. These are syntactically restricted to a single expression and are often used when you need a simple, one-off function for a short period.\n",
    "\n",
    "**Syntax:** `lambda arguments: expression`\n",
    "\n",
    "The expression's result is implicitly returned. Lambdas are most commonly found as arguments to higher-order functions like `sorted()`, `map()`, or `filter()`. While convenient, they should be used sparingly. If the logic is complex or the function is used in multiple places, a named function using `def` is almost always better for readability and debugging.\n",
    "\n",
    "For the common case of creating a key function that simply accesses an attribute or item, the `operator` module provides `attrgetter` and `itemgetter`. These are generally more efficient and more readable than an equivalent `lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Using `lambda` and `operator` for Sorting\")\n",
    "\n",
    "models = [\n",
    "    {'name': 'Solow', 'complexity': 3, 'year': 1956},\n",
    "    {'name': 'RBC', 'complexity': 7, 'year': 1982},\n",
    "    {'name': 'HANK', 'complexity': 9, 'year': 2018}\n",
    "]\n",
    "\n",
    "note(\"Sorting models by complexity using a lambda function as the key:\")\n",
    "# The key argument expects a function that takes one item and returns a sorting key.\n",
    "# A lambda is a concise way to provide this function inline.\n",
    "models_sorted_by_complexity = sorted(models, key=lambda model: model['complexity'])\n",
    "for model in models_sorted_by_complexity:\n",
    "    print(f\"  - {model['name']} (Complexity: {model['complexity']})\")\n",
    "\n",
    "note(\"\\nSorting models by year using operator.itemgetter (more efficient and readable):\")\n",
    "models_sorted_by_year = sorted(models, key=operator.itemgetter('year'))\n",
    "for model in models_sorted_by_year:\n",
    "    print(f\"  - {model['name']} (Year: {model['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Closures and the `nonlocal` Keyword\n",
    "\n",
    "A **closure** is an inner function that remembers and has access to the variables from its enclosing **lexical scope**, even after the outer function has finished executing. The inner function \"closes over\" these **free variables** (variables that are not local to the inner function but are from its enclosing scope). This mechanism is fundamental to functional programming patterns in Python.\n",
    "\n",
    "![The Closure Mechanism](images\png\1.10-closure-diagram.png)\n",
    "\n",
    "As the diagram illustrates, when the outer function (`make_cobb_douglas`) is called, the inner function (`production_function`) is created. The inner function maintains a reference to the `alpha` variable from its parent scope in its `__closure__` attribute. When the outer function finishes and its local scope is destroyed, the `alpha` variable lives on, kept alive by the closure. This is a powerful concept for creating **function factories**: higher-order functions that generate and return new, specialized functions based on their inputs. The factory takes the configuration parameters, and the returned function is a clean, simple callable that has those parameters \"baked in\".\n",
    "\n",
    "The `nonlocal` keyword allows the inner function to *modify* a free variable from the enclosing scope. Without it, assigning to a variable inside the inner function would create a new local variable, shadowing the outer one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Function Factory for Cobb-Douglas Production Functions\")\n",
    "\n",
    "def make_cobb_douglas(alpha: float) -> Callable[[float, float], float]:\n",
    "    \"\"\"A factory that creates a Cobb-Douglas production function with a specific alpha.\"\"\"\n",
    "    # `alpha` is a free variable for the `production_function` closure.\n",
    "    if not 0 < alpha < 1:\n",
    "        raise ValueError(\"alpha must be between 0 and 1.\")\n",
    "    \n",
    "    def production_function(K: float, L: float) -> float:\n",
    "        \"\"\"Calculates output Y = K**alpha * L**(1-alpha).\"\"\"\n",
    "        return (K**alpha) * (L**(1-alpha))\n",
    "    \n",
    "    return production_function\n",
    "\n",
    "note(\"Creating two different production functions from the factory:\")\n",
    "# us_prod_func has alpha=0.33 'baked in'\n",
    "us_prod_func = make_cobb_douglas(0.33)\n",
    "# eu_prod_func has alpha=0.30 'baked in'\n",
    "eu_prod_func = make_cobb_douglas(0.30)\n",
    "\n",
    "K, L = 100, 200 # Capital and Labor\n",
    "print(f\"US-calibrated output (alpha=0.33) with K={K}, L={L}: {us_prod_func(K, L):.2f}\")\n",
    "print(f\"EU-calibrated output (alpha=0.30) with K={K}, L={L}: {eu_prod_func(K, L):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Callable Classes: An Object-Oriented Alternative to Closures\n",
    "\n",
    "Another way to create a stateful, function-like object is to create a class that implements the `__call__` special method. This allows an *instance* of the class to be called as if it were a function. The object's state can be stored as instance attributes (e.g., `self.count`, `self.total`).\n",
    "\n",
    "**Closures vs. Callable Classes:**\n",
    "- **Closures** are often lighter-weight and more concise for simple cases where you just need to store a few pieces of state.\n",
    "- **Callable Classes** are better when the state is more complex or when you need multiple methods to interact with that state. They provide a more formal, structured approach that is familiar to object-oriented programmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"A Callable Class for a Running Average\")\n",
    "\n",
    "class RunningAverager:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.total = 0.0\n",
    "        \n",
    "    def __call__(self, new_value: float) -> float:\n",
    "        \"\"\"This method makes instances of the class callable.\"\"\"\n",
    "        self.count += 1\n",
    "        self.total += new_value\n",
    "        return self.total / self.count\n",
    "\n",
    "note(\"The callable instance maintains its internal state across calls:\")\n",
    "avg_instance = RunningAverager()\n",
    "print(f\"Average after adding 10: {avg_instance(10):.2f}\")\n",
    "print(f\"Average after adding 20: {avg_instance(20):.2f}\")\n",
    "print(f\"Average after adding 3:  {avg_instance(3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decorators: Syntactic Sugar for Function Wrapping\n",
    "\n",
    "A **decorator** is a higher-order function that takes another function as an argument, adds some functionality to it (without modifying the original function's code), and returns the new, enhanced function. The `@` syntax is syntactic sugar that makes this pattern clean and readable.\n",
    "\n",
    "![The Decorator Pattern](images\png\1.10-decorator-pattern.png)\n",
    "\n",
    "As the diagram shows, the `@my_decorator` syntax is exactly equivalent to defining `my_func` and then immediately reassigning the name `my_func` to the result of calling `my_decorator(my_func)`. The decorator consumes the original function and replaces it with a new, wrapped version.\n",
    "\n",
    "Decorators are ideal for handling **cross-cutting concerns**—aspects of a program that affect many other parts of the system, such as logging, timing, caching, or authentication. They allow you to separate this boilerplate logic from the core business logic of your functions.\n",
    "\n",
    "A well-behaved decorator uses `@functools.wraps` on its inner wrapper function. This is crucial as it copies the metadata (like the name `__name__` and docstring `__doc__`) from the original function to the wrapped function, which is essential for introspection and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"A Decorator for Retrying Failing Operations\")\n",
    "\n",
    "def retry(max_attempts: int, delay: float) -> Callable:\n",
    "    \"\"\"Factory for a decorator that retries a function if it raises an exception.\"\"\"\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs) # Attempt to run the function\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts >= max_attempts:\n",
    "                        print(f\"Function {func.__name__!r} failed after {max_attempts} attempts.\")\n",
    "                        raise # Re-raise the final exception\n",
    "                    else:\n",
    "                        print(f\"Attempt {attempts} failed: {e}. Retrying in {delay}s...\")\n",
    "                        time.sleep(delay)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "FAIL_COUNT = 0\n",
    "@retry(max_attempts=3, delay=0.1)\n",
    "def fetch_data_from_flaky_api():\n",
    "    \"\"\"Simulates fetching data from an API that might fail temporarily.\"\"\"\n",
    "    global FAIL_COUNT\n",
    "    print(\"  Attempting to fetch data...\")\n",
    "    if FAIL_COUNT < 2:\n",
    "        FAIL_COUNT += 1\n",
    "        raise IOError(\"Network timeout\")\n",
    "    return {\"data\": \"some important economic data\"}\n",
    "\n",
    "note(\"Calling the decorated function that fails twice before succeeding:\")\n",
    "data = fetch_data_from_flaky_api()\n",
    "print(f\"Successfully retrieved data: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 `@lru_cache`: A Powerful Built-in Decorator for Dynamic Programming\n",
    "A classic application of decorators is **memoization**: caching the results of expensive function calls and returning the cached result when the same inputs occur again. This is particularly powerful for optimizing recursive functions with **overlapping subproblems**, a hallmark of dynamic programming.\n",
    "\n",
    "The standard library's `@functools.lru_cache` provides a robust, production-ready implementation of this pattern. It implements a **L**east **R**ecently **U**sed cache: when the cache is full, it discards the item that hasn't been accessed for the longest time. The `maxsize` argument controls the cache size; setting `maxsize=None` allows the cache to grow indefinitely.\n",
    "\n",
    "**Warning:** Use `@lru_cache` only on pure functions whose output depends solely on their inputs. It should not be used on functions with side effects (e.g., modifying a global variable, writing to a file) or functions that depend on external state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Memoization for Dynamic Programming with @lru_cache\")\n",
    "\n",
    "def fib_naive(n):\n",
    "    if n < 2: return n\n",
    "    return fib_naive(n - 2) + fib_naive(n - 1)\n",
    "\n",
    "@lru_cache(maxsize=None) # maxsize=None means the cache can grow indefinitely\n",
    "def fib_cached(n):\n",
    "    \"\"\"A cached version of the Fibonacci function.\"\"\"\n",
    "    if n < 2: return n\n",
    "    return fib_cached(n - 2) + fib_cached(n - 1)\n",
    "\n",
    "N = 35\n",
    "note(f\"Calculating fib({N}) with the cached version...\")\n",
    "start_time = time.perf_counter()\n",
    "result_cached = fib_cached(N)\n",
    "time_cached = time.perf_counter() - start_time\n",
    "print(f\"  Result: {result_cached:,}, Time: {time_cached:.6f}s\")\n",
    "print(f\"  Cache info: {fib_cached.cache_info()}\")\n",
    "\n",
    "note(f\"\\nCalculating fib({N}) with the naive, uncached version (will be slow)...\")\n",
    "start_time = time.perf_counter()\n",
    "result_naive = fib_naive(N)\n",
    "time_naive = time.perf_counter() - start_time\n",
    "print(f\"  Result: {result_naive:,}, Time: {time_naive:.6f}s\")\n",
    "\n",
    "if time_cached > 0 and time_cached < time_naive:\n",
    "    note(f\"The cached version was approximately {time_naive / time_cached:,.0f}x faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generic Functions with Single Dispatch\n",
    "\n",
    "A common problem is wanting a function to behave differently based on the type of its first argument. The traditional approach is a messy `if/isinstance` chain. A much cleaner, more extensible solution is to use **generic functions**, a concept from **multiple dispatch**. While Python doesn't support full multiple dispatch, the `@functools.singledispatch` decorator allows dispatching based on the type of the *first* argument.\n",
    "\n",
    "You define the base function for the default case, and then use the `.register()` attribute of the new generic function (which acts as a decorator itself) to provide specialized implementations for specific types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Generic Functions with @singledispatch\")\n",
    "\n",
    "@singledispatch\n",
    "def summarize(data: Any) -> str:\n",
    "    \"\"\"The base/default implementation for our generic function.\"\"\"\n",
    "    return f\"Unstructured data of type {type(data).__name__}\"\n",
    "\n",
    "@summarize.register(dict)\n",
    "def _(data: dict) -> str:\n",
    "    \"\"\"A specialized implementation for dictionaries.\"\"\"\n",
    "    keys = \", \".join(data.keys())\n",
    "    return f\"A dictionary with {len(data)} keys: {keys}\"\n",
    "\n",
    "@summarize.register(pd.DataFrame)\n",
    "def _(data: pd.DataFrame) -> str:\n",
    "    \"\"\"A specialized implementation for pandas DataFrames.\"\"\"\n",
    "    return f\"A DataFrame with shape {data.shape} and columns: {list(data.columns)}\"\n",
    "\n",
    "@summarize.register(np.ndarray)\n",
    "def _(data: np.ndarray) -> str:\n",
    "    \"\"\"A specialized implementation for NumPy arrays.\"\"\"\n",
    "    return f\"A NumPy array with shape {data.shape}, dtype {data.dtype}, and mean {data.mean():.2f}\"\n",
    "\n",
    "note(\"Calling the generic function with different types:\")\n",
    "print(f\"\\n--- Summarizing an integer ---\\n{summarize(123)}\")\n",
    "print(f\"\\n--- Summarizing a dictionary ---\\n{summarize({'a': 1, 'b': [2, 3]})}\")\n",
    "df = pd.DataFrame({'price': [1, 2], 'volume': [100, 200]})\n",
    "print(f\"\\n--- Summarizing a DataFrame ---\\n{summarize(df)}\")\n",
    "arr = np.random.rand(10, 3)\n",
    "print(f\"\\n--- Summarizing a NumPy array ---\\n{summarize(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. `functools.partial`: Freezing Function Arguments\n",
    "\n",
    "`functools.partial` is a higher-order function that takes a function and a set of arguments, and returns a new, callable `partial` object that behaves like the original function but with those arguments pre-filled, or \"frozen.\"\n",
    "\n",
    "**Economic Application:** This is extremely useful when working with numerical libraries. For instance, a numerical optimizer like `scipy.optimize.minimize_scalar` requires a function of a single variable, `f(x)`. However, your objective function might be a more complex `objective(x, param1, param2)`. You can use `partial` to create a new, one-argument function that is compatible with the optimizer: `f = partial(objective, param1=value1, param2=value2)`.\n",
    "\n",
    "**`partial` vs. `lambda`:** While you could achieve a similar result with `lambda x: objective(x, param1=value1, param2=value2)`, `partial` is generally superior. It is more explicit, often more readable, and the resulting object has useful attributes (`.func`, `.args`, `.keywords`) for introspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Using `partial` for Numerical Optimization\")\n",
    "\n",
    "def log_likelihood(mu: float, data: np.ndarray, sigma: float) -> float:\n",
    "    \"\"\"Calculates the *negative* log-likelihood for a normal distribution.\n",
    "    We use the negative because optimizers typically minimize.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    log_lik = -n/2 * np.log(2 * np.pi * sigma**2) - (1 / (2 * sigma**2)) * np.sum((data - mu)**2)\n",
    "    return -log_lik # Return negative for minimization\n",
    "\n",
    "# Generate some sample data\n",
    "true_mu, true_sigma = 5.0, 2.0\n",
    "sample_data = np.random.normal(loc=true_mu, scale=true_sigma, size=1000)\n",
    "\n",
    "note(f\"We want to find the Maximum Likelihood Estimate for 'mu', assuming sigma={true_sigma} is known.\")\n",
    "note(\"`minimize_scalar` requires a function of a single variable (mu). `partial` is perfect for this.\")\n",
    "\n",
    "# Create a specialized version of the function where `data` and `sigma` are frozen.\n",
    "objective_func = partial(log_likelihood, data=sample_data, sigma=true_sigma)\n",
    "\n",
    "note(\"The partial object stores the original function and the 'frozen' arguments:\")\n",
    "print(f\"  - Original function: {objective_func.func.__name__}\")\n",
    "print(f\"  - Frozen keyword arguments: {{'data': 'np.ndarray(...)', 'sigma': objective_func.keywords['sigma']}})\")\n",
    "\n",
    "# Now `objective_func` is a callable that only takes `mu`.\n",
    "# We can pass it directly to the optimizer.\n",
    "result = minimize_scalar(objective_func, bounds=(0, 10), method='bounded')\n",
    "\n",
    "print(\"\\n--- Optimization Results ---\")\n",
    "print(f\"Optimizer success: {result.success}\")\n",
    "print(f\"Estimated mu (MLE): {result.x:.4f}\")\n",
    "print(f\"True mu:            {true_mu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. `functools.reduce`: Aggregating Sequences\n",
    "\n",
    "`functools.reduce` is a classic functional programming tool for producing a single result from a sequence. It applies a two-argument function cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, `reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])` calculates `((((1+2)+3)+4)+5)`.\n",
    "\n",
    "While Python's creator prefers a simple `for` loop for readability in most cases (especially for sums, where the built-in `sum()` is better), `reduce` can be an elegant tool for certain types of accumulation, such as calculating a cumulative product or applying a series of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Using `reduce` to Calculate Cumulative Portfolio Return\")\n",
    "\n",
    "# Sequence of daily gross returns (1 + daily_return)\n",
    "daily_gross_returns = [1.01, 0.99, 1.02, 1.005, 0.98]\n",
    "\n",
    "# We want to calculate the total return over the period.\n",
    "# The function to apply is lambda a, b: a * b (multiplication)\n",
    "total_gross_return = reduce(operator.mul, daily_gross_returns)\n",
    "\n",
    "print(f\"Daily gross returns: {daily_gross_returns}\")\n",
    "print(f\"Total gross return (product): {total_gross_return:.4f}\")\n",
    "print(f\"Total net return: {(total_gross_return - 1):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Exercises\n",
    "\n",
    "#### Exercise 1: Callable Class for Path-Dependent Pricing\n",
    "Create a callable class `PathDependentOption` that is initialized with a `strike_price`. When an instance is called with a new `price`, it should update an internal list of historical prices and return `True` if the average price so far is above the strike price, and `False` otherwise. Demonstrate its use by creating an instance and calling it with a sequence of prices.\n",
    "\n",
    "#### Exercise 2: `partial` for Data Cleaning\n",
    "You have a general-purpose data cleaning function `clean_data(series, remove_outliers=False, iqr_multiplier=1.5, fillna_method=None)`. Use `functools.partial` to create two new, specialized functions:\n",
    "a. `clean_for_ml`: This version should have `remove_outliers=True` and `fillna_method='median'` pre-filled.\n",
    "b. `clean_for_viz`: This version should only have `fillna_method='mean'` pre-filled and `remove_outliers=False`.\n",
    "Demonstrate by creating a pandas Series with `NaN` values and outliers, then calling both partial functions on it. (You will need to implement a simple `clean_data` function for the demonstration).\n",
    "\n",
    "#### Exercise 3: Closure for Policy Simulation\n",
    "Write a function factory `make_tax_calculator(brackets: dict, exemption: float)`. The `brackets` dictionary should map income thresholds to marginal tax rates (e.g., `{10000: 0.1, 40000: 0.2, 100000: 0.3}`). The factory should return a closure `calculate_tax(income)` that correctly computes the total tax for a given income based on the progressive bracket system and the initial exemption. Create two calculators for two different tax regimes and test them.\n",
    "\n",
    "#### Exercise 4: `lru_cache` for a Grid Search\n",
    "You have an expensive function `evaluate_policy(alpha, beta)` that returns a score and takes 0.1 seconds to run (simulate this with `time.sleep(0.1)`). You want to run a grid search over a list of `(alpha, beta)` pairs, but your list contains duplicates.\n",
    "- **Task:** Apply the `@lru_cache` decorator to `evaluate_policy`. Write a loop that calls the function for a list of `(alpha, beta)` pairs that contains duplicates. Show that the total execution time is much less than `(number_of_calls * 0.1)` seconds, and print the `cache_info()` to verify that the cache was used.\n",
    "\n",
    "--- \n",
    "\n",
    "### Challenge Exercise: A Pluggable Event System for Simulations\n",
    "\n",
    "This exercise synthesizes multiple concepts to build a simple but powerful event-driven system, a common architecture for agent-based models or simulations.\n",
    "\n",
    "**Objective:** Create an `EventSystem` class that allows different parts of a program to register functions (handlers) that will be called when a specific event occurs.\n",
    "\n",
    "1.  **The `EventSystem` Class:**\n",
    "    - `__init__(self)`: Initialize an empty dictionary, `self._handlers`, to store the event handlers. The keys will be event names (strings), and the values will be lists of functions (the handlers).\n",
    "    - `register(self, event_name: str) -> Callable`: This method should be a **decorator factory**. It takes an `event_name` and returns a decorator. The decorator it returns should take a function (`handler_func`) and append it to the list for the correct event in `self._handlers`.\n",
    "    - `trigger(self, event_name: str, *args, **kwargs)`: This method takes an event name and any additional arguments. It should look up all registered handlers for that event and call each of them with the provided `*args` and `**kwargs`.\n",
    "\n",
    "2.  **Implementation and Demonstration:**\n",
    "    a. Create an instance: `events = EventSystem()`.\n",
    "    b. Use the `@events.register('simulation_start')` decorator to register a function `log_start_time(timestamp)`.\n",
    "    c. Use the `@events.register('simulation_end')` decorator to register a function `log_end_time(timestamp, status)`.\n",
    "    d. Write a `run_simulation` function that:\n",
    "        i. Calls `events.trigger('simulation_start', timestamp=datetime.datetime.now())`.\n",
    "        ii. Prints a message like \"... Running core logic ...\".\n",
    "        iii. Calls `events.trigger('simulation_end', timestamp=datetime.datetime.now(), status='SUCCESS')`.\n",
    "    e. Run the simulation and show that your registered handlers were called automatically at the correct time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
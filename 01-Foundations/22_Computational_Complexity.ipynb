{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (11, 7), 'figure.dpi': 130})\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2.9: Computational Complexity in Economics\n",
    "\n",
    "---\n",
    "### Introduction: Why Algorithm Speed Matters\n",
    "\n",
    "As economic models become more complex, the question of whether they can be solved in a reasonable amount of time becomes critical. **Computational complexity theory** is the branch of computer science that studies the resources (primarily time and memory) required to run an algorithm. For economists, a basic understanding of complexity is essential for recognizing which problems are feasible to solve, for choosing the right algorithm for the job, and for understanding the trade-offs inherent in model design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Big-O Notation: Characterizing Scalability\n",
    "\n",
    "**Big-O notation** is the standard way to describe an algorithm's efficiency in the worst-case scenario. It characterizes how the runtime or memory usage of an algorithm grows as the size of the input, $n$, grows. It ignores constant factors and lower-order terms, focusing on the dominant term that determines the algorithm's scalability.\n",
    "\n",
    "**Common Complexity Classes:**\n",
    "- **O(1) - Constant Time:** The runtime is independent of the input size. (e.g., accessing an element in an array by its index).\n",
    "- **O(log n) - Logarithmic Time:** The runtime grows logarithmically with the input size. Extremely efficient. (e.g., binary search).\n",
    "- **O(n) - Linear Time:** The runtime grows linearly with the input size. (e.g., finding the maximum value in an unsorted list).\n",
    "- **O(n log n) - Log-Linear Time:** The 'gold standard' for sorting algorithms. (e.g., Merge Sort, Quicksort).\n",
    "- **O(n^2) - Quadratic Time:** The runtime grows with the square of the input size. Becomes slow quickly. (e.g., nested loops, naive matrix multiplication).\n",
    "- **O(2^n) - Exponential Time:** The runtime doubles with each additional element in the input. Becomes intractable for even moderately sized inputs. (e.g., traveling salesman problem via brute force).\n",
    "- **O(n!) - Factorial Time:** Even worse than exponential. Unfeasible for all but the smallest inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing Complexity Classes\")\n",
    "n = np.arange(1, 100)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n, np.ones_like(n), label='O(1) - Constant')\n",
    "plt.plot(n, np.log(n), label='O(log n) - Logarithmic')\n",
    "plt.plot(n, n, label='O(n) - Linear')\n",
    "plt.plot(n, n * np.log(n), label='O(n log n) - Log-Linear')\n",
    "plt.plot(n, n**2, label='O(n^2) - Quadratic')\n",
    "plt.title('Growth Rates of Common Complexity Classes')\n",
    "plt.xlabel('Input Size (n)')\n",
    "plt.ylabel('Number of Operations (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.ylim(1, 10**5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Curse of Dimensionality\n",
    "\n",
    "The **curse of dimensionality**, a term coined by Richard Bellman (of the Bellman equation), refers to the exponential explosion in the volume of a space as its number of dimensions increases. This has profound consequences for computational economics, especially in dynamic programming.\n",
    "\n",
    "Imagine solving a household's consumption-saving problem where the state is their current wealth. If we discretize wealth into a grid of 100 points, we must solve the Bellman equation at each point. Now, suppose the household has two assets (e.g., liquid and illiquid). To maintain the same resolution for each asset, our state space grid now has $100 \\times 100 = 10,000$ points. With three assets, it's $100^3 = 1,000,000$ points. The size of the problem grows exponentially with the number of state variables.\n",
    "\n",
    "This is why so many models are simplified to have only one or two state variables. Overcoming the curse of dimensionality is a major motivation for developing more advanced numerical methods, such as sparse grids, adaptive grids, and machine learning-based function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Complexity in Economic Applications\n",
    "\n",
    "- **Solving Linear Systems (Matrix Inversion):** Solving a system of $n$ linear equations, $Ax=b$, is fundamental to econometrics (OLS) and general equilibrium models. The standard algorithm, Gaussian elimination, has a complexity of **O(n^3)**. This means that doubling the number of equations increases the solution time by a factor of eight.\n",
    "\n",
    "- **Maximum Likelihood Estimation:** The complexity of MLE depends on the optimization algorithm used. For a simple logit model with $N$ observations and $k$ features, each iteration of a Newton-like optimizer often requires computing the Hessian, which can be **O(Nk^2)**. This informs the choice of optimizer for large datasets.\n",
    "\n",
    "- **Dynamic Programming:** As discussed, grid-based solutions to DP problems suffer from the curse of dimensionality. If there are $d$ state variables and we use $m$ grid points for each, the complexity is **O(m^d)**, which is exponential in the number of dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
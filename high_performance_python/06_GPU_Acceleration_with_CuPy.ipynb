{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. GPU Acceleration with CuPy\n",
    "\n",
    "## CPU vs. GPU: An Analogy\n",
    "\n",
    "Imagine you need to solve thousands of simple arithmetic problems. You have two choices:\n",
    "\n",
    "1.  **A master mathematician (the CPU):** This person is brilliant and can solve any complex problem you give them, one at a time, very quickly.\n",
    "2.  **An army of schoolchildren (the GPU):** None of them are individually as skilled as the master mathematician, but you have thousands of them. You can give each child one of the simple problems, and they can all solve them at the same time.\n",
    "\n",
    "For complex, sequential tasks, the master mathematician is the best choice. But for a large number of simple, independent tasks, the army of schoolchildren will finish the job much faster. This is the core difference between a CPU and a GPU. CPUs have a few powerful cores for sequential tasks, while GPUs have thousands of simpler cores for parallel tasks.\n",
    "\n",
    "Many operations in economic modeling, like matrix multiplication, are like the army of schoolchildren problem. By offloading these tasks to a GPU, we can often achieve a notable speedup.\n",
    "\n",
    "**CuPy** is a Python library that makes GPU computing accessible. It provides a **drop-in replacement for NumPy**, meaning its API is almost identical. This allows you to accelerate your existing NumPy code on an NVIDIA GPU with minimal code changes.\n",
    "\n",
    "--- \n",
    "\\n",
    "**<font color='red'>IMPORTANT NOTE:</font>** To run this notebook, you **must** have a compatible **NVIDIA GPU** and the **NVIDIA CUDA Toolkit** installed. The CUDA Toolkit is a software layer that gives direct access to the GPU's virtual instruction set. Without it, your system cannot communicate with the GPU for general-purpose computing. If you lack the required hardware or software, the code in this notebook will fail.\\n",
    "\\n",
    "---\\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- The fundamental differences between CPU and GPU architectures.\n",
    "- How to use CuPy to create arrays on the GPU.\n",
    "- How to move data between the CPU (NumPy) and the GPU (CuPy).\n",
    "- How to benchmark the performance difference for common numerical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CuPy API: NumPy on the GPU\n",
    "\n",
    "The convention is to import NumPy as `np` and CuPy as `cp`. You can then create arrays on the GPU using familiar syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import timeit\n",
    "\n",
    "# A flag to check if CuPy is available\n",
    "try:\n",
    "    cp.cuda.runtime.getDeviceCount()\n",
    "    CUPY_AVAILABLE = True\n",
    "    print(\"CuPy is available.\")\n",
    "except cp.cuda.runtime.CUDARuntimeError:\n",
    "    CUPY_AVAILABLE = False\n",
    "    print(\"CuPy is not available. Code in this notebook will not run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Create a NumPy array on the CPU\n",
    "    x_cpu = np.arange(10)\n",
    "    print(f\"NumPy array on CPU: {x_cpu}\")\n",
    "\n",
    "    # Create a CuPy array on the GPU\n",
    "    x_gpu = cp.arange(10)\n",
    "    print(f\"CuPy array on GPU: {x_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Data Between CPU and GPU\n",
    "\n",
    "A critical concept in GPU computing is data transfer. For the GPU to operate on data, that data must first be moved from the host (CPU) memory to the device (GPU) memory. This transfer has a cost, so efficient GPU computing often involves minimizing CPU-GPU data transfers.\n",
    "\n",
    "- To move a NumPy array to the GPU, use `cupy.asarray()`.\n",
    "- To move a CuPy array back to the CPU, use `cupy.asnumpy()` or the `.get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Create a NumPy array\n",
    "    numpy_arr = np.random.rand(5)\n",
    "    print(f\"Original NumPy array: {numpy_arr}\")\n",
    "\n",
    "    # Move it to the GPU\n",
    "    cupy_arr = cp.asarray(numpy_arr)\n",
    "    print(f\"CuPy array on GPU: {cupy_arr}\")\n",
    "\n",
    "    # Move it back to the CPU\n",
    "    numpy_arr_back = cp.asnumpy(cupy_arr)\n",
    "    print(f\"Array back on CPU: {numpy_arr_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking: CPU vs. GPU Performance\n",
    "\n",
    "Let's demonstrate the performance difference with a classic example: multiplying two large matrices. This is a highly parallelizable task where GPUs excel. We use `astype(np.float32)` because single-precision floating-point arithmetic is often much faster on consumer GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Define the size of the matrices\n",
    "    size = 5000\n",
    "\n",
    "    # Create two random matrices in NumPy (CPU)\n",
    "    a_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "    b_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "    # Create two random matrices in CuPy (GPU)\n",
    "    a_gpu = cp.random.rand(size, size).astype(cp.float32)\n",
    "    b_gpu = cp.random.rand(size, size).astype(cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Time the CPU operation\n",
    "    cpu_time = timeit.timeit(lambda: np.dot(a_cpu, b_cpu), number=10)\n",
    "    print(f\"CPU time: {cpu_time / 10:.4f} seconds\")\n",
    "\n",
    "    # Time the GPU operation\n",
    "    # We use cp.cuda.Stream.null.synchronize() to ensure the computation is finished\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    gpu_time = timeit.timeit(lambda: cp.dot(a_gpu, b_gpu), number=10)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    print(f\"GPU time: {gpu_time / 10:.4f} seconds\")\n",
    "\n",
    "    print(f\"Speedup: {cpu_time / gpu_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost of Memory Transfer\n",
    "\n",
    "The speedup from GPU computation is not free. Moving data between the CPU and GPU takes time. For the speedup to be worthwhile, the time saved by the GPU computation must be greater than the time spent on data transfer.\n",
    "\n",
    "Let's see this in action with a small array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    size = 100\n",
    "    a_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "    b_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "    # Time just the CPU computation\n",
    "    cpu_time = timeit.timeit(lambda: np.dot(a_cpu, b_cpu), number=100)\n",
    "    print(f\"Small matrix, CPU only: {cpu_time / 100:.6f} seconds\")\n",
    "\n",
    "    # Time the full process: move to GPU, compute, move back\n",
    "    def gpu_full_trip(a, b):\n",
    "        a_gpu = cp.asarray(a)\n",
    "        b_gpu = cp.asarray(b)\n",
    "        c_gpu = cp.dot(a_gpu, b_gpu)\n",
    "        c_cpu = cp.asnumpy(c_gpu)\n",
    "    \n",
    "    gpu_time = timeit.timeit(lambda: gpu_full_trip(a_cpu, b_cpu), number=100)\n",
    "    print(f\"Small matrix, GPU full trip: {gpu_time / 100:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the small matrix, the GPU version is likely slower because the overhead of transferring data to and from the GPU outweighs the computational speedup. This illustrates a key principle: **use the GPU for large, intensive computations where the data transfer cost is a small fraction of the total execution time.**\n",
    "\n",
    "## Summary: When to Use CuPy\n",
    "\n",
    "CuPy is a powerful tool, but it's not always the right choice. Consider using CuPy when:\n",
    "\n",
    "1.  **You have an NVIDIA GPU:** CuPy is built on CUDA and only works with NVIDIA hardware.\n",
    "2.  **Your code is NumPy-heavy:** It is designed as a drop-in replacement for NumPy.\n",
    "3.  **Your operations are highly parallelizable:** It excels at large-scale matrix algebra, element-wise operations on large arrays, and other vectorized computations.\n",
    "4.  **The performance bottleneck is significant:** The overhead of transferring data to the GPU is not trivial. For small arrays or simple operations, NumPy on the CPU may be faster. CuPy shines when the computation is heavy enough to make the data transfer cost worthwhile.\n",
    "\n",
    "By providing a familiar API, CuPy lowers the barrier to entry for GPU computing, enabling economists to harness the parallel processing power of modern graphics cards for their research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
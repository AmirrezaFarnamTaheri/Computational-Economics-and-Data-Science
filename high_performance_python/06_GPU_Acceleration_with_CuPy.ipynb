{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Acceleration with CuPy\n",
    "\n",
    "## CPU vs. GPU: A Tale of Two Processors\n",
    "\n",
    "Central Processing Units (CPUs) and Graphics Processing Units (GPUs) are designed with fundamentally different architectures for different purposes.\n",
    "\n",
    "- **CPUs** are composed of a few, very powerful cores optimized for **sequential, latency-sensitive tasks**. They are masters of executing complex instructions one after another very quickly.\n",
    "- **GPUs**, on the other hand, are composed of thousands of smaller, simpler cores designed for **parallel, throughput-sensitive tasks**. They excel at performing the same simple operation on thousands of data points simultaneously.\n",
    "\n",
    "Many operations in scientific computing and economic modeling, such as matrix multiplication, vector operations, and simulations, are inherently parallel. By offloading these tasks to a GPU, we can often achieve massive performance gains over a CPU.\n",
    "\n",
    "**CuPy** is a Python library that makes GPU computing incredibly accessible. It provides a **drop-in replacement for NumPy**, meaning its API is designed to be a near-perfect mirror of NumPy's. This allows you to accelerate your existing NumPy code on an NVIDIA GPU with minimal code changes.\n",
    "\n",
    "--- \n",
    "\\n",
    "**<font color='red'>IMPORTANT NOTE:</font>** To run this notebook, you **must** have a compatible **NVIDIA GPU** and the correct version of the **NVIDIA CUDA Toolkit** installed on your system. If you do not have the required hardware and software, the code cells in this notebook will fail. You can still read the content to understand the concepts, but you will not be able to execute the code.\\n",
    "\\n",
    "---\\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- The fundamental differences between CPU and GPU architectures.\n",
    "- How to use CuPy to create arrays on the GPU.\n",
    "- How to move data between the CPU (NumPy) and the GPU (CuPy).\n",
    "- How to benchmark the performance difference for common numerical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CuPy API: NumPy on the GPU\n",
    "\n",
    "The convention is to import NumPy as `np` and CuPy as `cp`. You can then create arrays on the GPU using familiar syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "    # Get GPU device name\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode('utf-8')\n",
    "    print(f\"Found compatible GPU: {gpu_name}\")\n",
    "except (ImportError, cp.cuda.runtime.CUDARuntimeError):\n",
    "    CUPY_AVAILABLE = False\n",
    "    print(\"CuPy is not installed or no compatible NVIDIA GPU is found.\")\n",
    "\n",
    "if CUPY_AVAILABLE:\n",
    "    # Create a NumPy array on the CPU\n",
    "    x_cpu = np.arange(10)\n",
    "    print(f\"NumPy array on CPU: {x_cpu}\")\n",
    "\n",
    "    # Create a CuPy array on the GPU\n",
    "    x_gpu = cp.arange(10)\n",
    "    print(f\"CuPy array on GPU: {x_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Data Between CPU and GPU\n",
    "\n",
    "A critical concept in GPU computing is data transfer. For the GPU to operate on data, that data must first be moved from the host (CPU) memory to the device (GPU) memory. This transfer has a cost, so efficient GPU computing often involves minimizing CPU-GPU data transfers.\n",
    "\n",
    "- To move a NumPy array to the GPU, use `cupy.asarray()`.\n",
    "- To move a CuPy array back to the CPU, use `cupy.asnumpy()` or the `.get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Create a NumPy array\n",
    "    numpy_arr = np.random.rand(5)\n",
    "    print(f\"Original NumPy array: {numpy_arr}\")\n",
    "\n",
    "    # Move it to the GPU\n",
    "    cupy_arr = cp.asarray(numpy_arr)\n",
    "    print(f\"CuPy array on GPU: {cupy_arr}\")\n",
    "\n",
    "    # Move it back to the CPU\n",
    "    numpy_arr_back = cp.asnumpy(cupy_arr)\n",
    "    print(f\"Array back on CPU: {numpy_arr_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking: CPU vs. GPU Performance\n",
    "\n",
    "Let's demonstrate the performance difference with a classic example: multiplying two large matrices. This is a highly parallelizable task where GPUs excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    # Define the size of the matrices\n",
    "    size = 5000\n",
    "\n",
    "    # Create two random matrices in NumPy (CPU)\n",
    "    a_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "    b_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "    # Create two random matrices in CuPy (GPU)\n",
    "    a_gpu = cp.random.rand(size, size).astype(cp.float32)\n",
    "    b_gpu = cp.random.rand(size, size).astype(cp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's time the matrix multiplication on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    cpu_time = timeit.timeit(lambda: np.dot(a_cpu, b_cpu), number=10)\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's time the same operation on the GPU. Note that for a fair comparison, we should ensure the data is already on the GPU. We also need to synchronize the device to ensure the computation is finished before stopping the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    gpu_time = timeit.timeit(lambda: cp.dot(a_gpu, b_gpu), number=10)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup:  {cpu_time / gpu_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe a dramatic speedup, potentially 50-100x or more, depending on your specific CPU and GPU. This is the power of offloading highly parallelizable work to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complex Example: Singular Value Decomposition (SVD)\n",
    "\n",
    "The benefits extend to more complex linear algebra, which is at the heart of many econometric and statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    cpu_svd_time = timeit.timeit(lambda: np.linalg.svd(a_cpu), number=1)\n",
    "    print(f\"CPU SVD time: {cpu_svd_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUPY_AVAILABLE:\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    gpu_svd_time = timeit.timeit(lambda: cp.linalg.svd(a_gpu), number=1)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    print(f\"GPU SVD time: {gpu_svd_time:.4f} seconds\")\n",
    "    print(f\"SVD Speedup:  {cpu_svd_time / gpu_svd_.time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: When to Use CuPy\n",
    "\n",
    "CuPy is a powerful tool, but it's not always the right choice. Consider using CuPy when:\n",
    "\n",
    "1.  **You have an NVIDIA GPU:** CuPy is built on CUDA and only works with NVIDIA hardware.\n",
    "2.  **Your code is NumPy-heavy:** It is designed as a drop-in replacement for NumPy.\n",
    "3.  **Your operations are highly parallelizable:** It excels at large-scale matrix algebra, element-wise operations on large arrays, and other vectorized computations.\n",
    "4.  **The performance bottleneck is significant:** The overhead of transferring data to the GPU is not trivial. For small arrays or simple operations, NumPy on the CPU may be faster. CuPy shines when the computation is heavy enough to make the data transfer cost worthwhile.\n",
"By providing a familiar API, CuPy dramatically lowers the barrier to entry for GPU computing, enabling economists to harness the massive parallel processing power of modern graphics cards for their research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
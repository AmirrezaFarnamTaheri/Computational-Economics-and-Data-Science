{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Parallel and Distributed Computing with Dask\n",
    "\n",
    "## The Scaling Problem: When Data Exceeds Memory\n",
    "\n",
    "Economists are increasingly working with datasets that are too large to fit into a single computer's RAM. A typical pandas DataFrame, for example, resides entirely in memory. If you try to load a 50 GB file on a machine with 16 GB of RAM, your process will fail. Similarly, many computations are CPU-bound and could be sped up by using all the available cores on a modern processor.\n",
    "\n",
    "**Dask** is a Python library for parallel and distributed computing. It helps you scale the Python tools you already use—like NumPy, pandas, and scikit-learn—to multi-core machines and large, distributed clusters.\n",
    "\n",
    "Dask's core idea is **lazy evaluation**. It builds a **task graph** of the computations you want to perform, and only executes them when you explicitly ask for a result. This allows Dask to optimize the execution plan for parallel performance.\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- The core concepts behind Dask: lazy evaluation and task graphs.\n",
    "- How to use **Dask DataFrames** to work with larger-than-memory tabular data.\n",
    "- How to use **Dask Arrays** for scalable numerical computing.\n",
    "- How to use **Dask Delayed** for custom parallel algorithms.\n",
    "- How to set up a local Dask cluster and use its diagnostic dashboard to monitor computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lazy Evaluation and Task Graphs\n",
    "\n",
    "When you work with Dask, you are not manipulating data directly. Instead, you are building a graph that represents the sequence of tasks to be performed. Each task is a function call on a piece of data, and the graph captures the dependencies between these tasks.\n",
    "\n",
    "For example, a simple computation like `(a + b) * (c + d)` can be represented as a task graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A Dask Task Graph](../../images/high_performance_python/dask_task_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask can analyze this graph and see that `sum(a, b)` and `sum(c, d)` are independent and can be computed in parallel. This lazy approach allows Dask to handle complex computations on large datasets efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Local Dask Cluster\n",
    "\n",
    "To use Dask, you first need a **cluster**, which consists of a **scheduler** that manages tasks and **workers** that execute them. For a single machine, the `dask.distributed` library makes it simple to create a local cluster. When you create a `Client`, it automatically sets up a scheduler and a set of workers, typically one per CPU core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a local cluster with a scheduler and workers.\n",
    "# The client provides a dashboard link to monitor your work.\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Click the link for the Dashboard. It is a powerful tool that provides a real-time visualization of your computations, showing you which tasks are running on which workers and how memory is being used. Keep it open in a separate browser tab as you go through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask DataFrames: Parallelizing Pandas\n",
    "\n",
    "A Dask DataFrame is a large, parallel DataFrame composed of many smaller pandas DataFrames, split along the index. These smaller DataFrames are called partitions, and each partition can be computed on by a different worker.\n",
    "\n",
    "The Dask DataFrame API mimics the pandas API, so you can perform many familiar operations. However, these operations are **lazy**—Dask builds a task graph of the planned computation but doesn't execute it until you explicitly request a result with a method like `.compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Analyzing a Large CSV\n",
    "\n",
    "Dask comes with some sample data. Let's use `dask.datasets` to create a large, synthetic dataset of time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Dask DataFrame\n",
    "# This represents a much larger dataset than can fit in memory.\n",
    "ddf = dd.demo.make_timeseries(\n",
    "    '2000-01-01', '2000-12-31',\n",
    "    freq='1s', partition_freq='1M', dtypes={'x': float, 'y': float, 'id': int}\n",
    ")\n",
    "\n",
    "# Notice that printing the DataFrame doesn't show data.\n",
    "# It shows the structure: columns, types, and number of partitions.\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a standard pandas-like operation: group by an ID and find the mean of one of the columns. Notice that this operation executes instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lazy operation. No computation is done yet.\n",
    "mean_x_by_id = ddf.groupby('id').x.mean()\n",
    "mean_x_by_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the computation, we call `.compute()`. Now, watch your Dask dashboard. You will see the task graph being executed in parallel across all your CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers the actual computation.\n",
    "result = mean_x_by_id.compute()\n",
    "\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Arrays: Parallelizing NumPy\n",
    "\n",
    "Similarly, a Dask Array is composed of many smaller NumPy arrays, called \"chunks.\" It supports a large subset of the NumPy API.\n",
    "\n",
    "Let's create a large random Dask array and perform some standard operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 20000x20000 array of random numbers.\n",
    "# This would be ~3.2 GB, potentially too large for some machines.\n",
    "# Dask handles it by chunking it into smaller NumPy arrays.\n",
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform a computation, like taking the mean along an axis. Again, this is lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lazy operation\n",
    "y = x.mean(axis=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, trigger the computation with `.compute()` and watch the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = y.compute()\n",
    "print(result_array.shape)\n",
    "print(result_array[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Delayed: Custom Parallel Algorithms\n",
    "\n",
    "For problems that don't fit the DataFrame or Array models, Dask provides `dask.delayed`. This is a decorator that makes a function lazy, adding it to the task graph. This is useful for parallelizing custom code, such as a loop of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "@delayed\n",
    "def simulate(params):\n",
    "    # In a real scenario, this would be a complex simulation\n",
    "    time.sleep(0.1)\n",
    "    return sum(params)\n",
    "\n",
    "simulations = [simulate([i, i+1]) for i in range(10)]\n",
    "\n",
    "# `dask.compute` runs all the delayed objects in parallel\n",
    "results = dask.compute(*simulations)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: When to Use Dask\n",
    "\n",
    "Dask is the right tool when your data is large and your computations are parallelizable. It excels when:\n",
    "\n",
    "- **Working with Large Datasets:** Your data does not fit in RAM, and you need to perform pandas- or NumPy-like operations on it.\n",
    "- **Leveraging Multi-Core CPUs:** You have computationally intensive workflows that can be sped up by running them in parallel across multiple cores.\n",
    "- **Scaling to a Cluster:** While we have used a local cluster, Dask is designed to scale seamlessly to large HPC or cloud-based clusters with minimal code changes.\n",
    "\n",
    "By providing scalable versions of familiar tools, Dask empowers economists to tackle larger and more complex empirical problems without straying from the Python ecosystem. It is a foundational tool for modern, data-intensive research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to close the client to release resources\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
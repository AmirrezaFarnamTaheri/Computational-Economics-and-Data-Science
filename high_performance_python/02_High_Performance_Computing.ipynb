{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, timeit\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from numba import njit, prange, vectorize\n",
    "    NUMBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Define dummy decorators if Numba is not available\n",
    "    def njit(func=None, **kwargs): return func if func else lambda f: f\n",
    "    def prange(*args, **kwargs): return range(*args, **kwargs)\n",
    "    def vectorize(func=None, **kwargs): return np.vectorize(func) if func else lambda f: np.vectorize(f)\n",
    "    NUMBA_AVAILABLE = False\n",
    "try:\n",
    "    import dask\n",
    "    from dask import delayed\n",
    "    DASK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DASK_AVAILABLE = False\n",
    "import multiprocessing as mp\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg):\n",
    "    \"\"\"Displays a formatted note in a Jupyter Notebook.\"\"\"\n",
    "    display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title):\n",
    "    \"\"\"Prints a section header to the console.\"\"\"\n",
    "    print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for High-Performance Computing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Computing for Economics\n",
    "\n",
    "### Introduction: Why and When to Optimize\n",
    "\n",
    "As economic models become more complex and datasets grow, the time it takes to run code can become a major research bottleneck. Standard Python, while excellent for rapid development, is an interpreted language and can be orders of magnitude slower than compiled languages like C++ or Fortran for number-crunching tasks, especially those involving explicit `for` loops.\n",
    "\n",
    "**High-Performance Computing (HPC)** provides a set of techniques to break through these computational barriers. However, it is crucial to follow a disciplined workflow and heed the famous adage from computer scientist Donald Knuth: **\"Premature optimization is the root of all evil.\"**\n",
    "\n",
    "The workflow for a computational project should always be:\n",
    "1.  **Write it correctly:** First, write clear, simple, and well-tested code that verifiably produces the correct result.\n",
    "2.  **Profile it:** If the correct code is too slow for your needs, use a profiler to identify exactly *which functions or lines of code* are the bottlenecks.\n",
    "3.  **Optimize the bottleneck:** Only after identifying a specific, measured bottleneck should you apply the advanced tools discussed in this chapter to that specific part of the code.\n",
    "\n",
    "This chapter provides a hands-on introduction to the most common HPC techniques used in economic research, focusing on Just-In-Time (JIT) compilation, parallel computing, and GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Just-In-Time (JIT) Compilation with Numba\n",
    "\n",
    "When a vectorized NumPy solution is not obvious or is too memory-intensive, **Numba** is the first line of defense against slow loops. Numba is a **Just-In-Time (JIT) compiler** that translates a subset of Python and NumPy code into fast, optimized machine code at runtime.\n",
    "\n",
    "This is achieved through a simple function decorator, `@njit`. This decorator stands for **\"no-python JIT,\"** a mode that guarantees the entire function is compiled to machine code without any calls back to the slow Python interpreter. If Numba cannot compile some part of the function (e.g., because you used an unsupported feature like a Python dictionary), it will raise an error. This forces you to use data structures (primarily NumPy arrays and simple scalar types) that can be compiled to highly efficient machine code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Numba for Accelerating Loops: A Monte Carlo Example\")\n",
    "\n",
    "# We will estimate pi using a simple Monte Carlo simulation.\n",
    "# This is a classic example of a task that is slow in pure Python but easy to speed up.\n",
    "def monte_carlo_pi_python(n_samples):\n",
    "    \"\"\"Estimates pi using a pure Python loop.\"\"\"\n",
    "    acc = 0\n",
    "    # This loop is the bottleneck. Python's dynamic typing makes each iteration slow.\n",
    "    for i in range(n_samples):\n",
    "        x, y = random.random(), random.random()\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "# The @njit decorator tells Numba to compile this function.\n",
    "# - `parallel=True` allows Numba to automatically parallelize the loop.\n",
    "# - `cache=True` saves the compiled function to disk, making subsequent calls faster.\n",
    "@njit(parallel=True, cache=True)\n",
    "def monte_carlo_pi_numba(n_samples):\n",
    "    \"\"\"Estimates pi using a Numba-compiled, parallel loop.\"\"\"\n",
    "    acc = 0\n",
    "    # `prange` is Numba's parallel range, which works like range() but splits the\n",
    "    # work across multiple CPU cores.\n",
    "    for i in prange(n_samples):\n",
    "        # Inside a Numba function, we must use NumPy's random functions.\n",
    "        x, y = np.random.rand(), np.random.rand()\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "n = 10_000_000\n",
    "if NUMBA_AVAILABLE:\n",
    "    # Time the pure Python version\n",
    "    py_time = timeit.timeit(lambda: monte_carlo_pi_python(n), number=1)\n",
    "    \n",
    "    # The first time a Numba function is called, it has to compile. This is a one-off cost.\n",
    "    # We run it once on a small input to \"warm it up\" before timing.\n",
    "    print(\"Warming up Numba (compiling the function)...\")\n",
    "    monte_carlo_pi_numba(1)\n",
    "    \n",
    "    # Time the compiled Numba version\n",
    "    numba_time = timeit.timeit(lambda: monte_carlo_pi_numba(n), number=1)\n",
    "    \n",
    "    print(f\"Pure Python time: {py_time:.4f}s\")\n",
    "    print(f\"Numba time:       {numba_time:.4f}s\")\n",
    "    note(f\"Numba provides a **{py_time / numba_time:.1f}x** speedup over pure Python for this task.\")\n",
    "else:\n",
    "    note(\"Numba is not installed. Skipping performance comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Computing: Theory and Practice\n",
    "\n",
    "### 2.1 The Limits of Parallelization: Amdahl's Law\n",
    "Before diving into parallel coding, it's important to understand its theoretical limits. **Amdahl's Law**, formulated by computer architect Gene Amdahl in 1967, specifies the maximum possible speedup from parallelizing a task. Let $P$ be the proportion of a program that can be parallelized (and $1-P$ be the proportion that is inherently serial). The maximum speedup from using $N$ processors is:\n",
    "\n",
    "$$ \\text{Speedup}(N, P) = \\frac{1}{(1-P) + \\frac{P}{N}} $$ \n",
    "\n",
    "The key insight is that the serial portion $(1-P)$ acts as a permanent bottleneck. As the number of processors $N$ approaches infinity, the term $P/N$ goes to zero, and the maximum speedup is limited to $1 / (1-P)$.\n",
    "\n",
    "For example, if 10% of your code is inherently serial ($P=0.9$), you can never achieve more than a 10x speedup, no matter how many cores you throw at the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Global Interpreter Lock (GIL) and Multiprocessing\n",
    "\n",
    "A common source of confusion when trying to parallelize Python code is the **Global Interpreter Lock (GIL)**. In the standard CPython interpreter, the GIL is a mutex (a type of lock) that prevents multiple native threads from executing Python bytecodes at the same time within a single process. This means that for **CPU-bound** code (tasks limited by CPU speed, like our pi calculation), Python's `threading` module provides no performance gain and can even make things slower due to overhead.\n",
    "\n",
    "To achieve true parallelism for CPU-bound tasks, we must bypass the GIL. The standard library's **`multiprocessing`** module does this by running each task in a separate process, managed by the operating system. Each process has its own Python interpreter and memory space, so the GIL of one process does not block the others. This approach is ideal for **\"embarrassingly parallel\"** problems where tasks are independent and require little communication (e.g., parameter sweeps, bootstrapping, or Monte Carlo simulations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Parallelism with Multiprocessing\")\n",
    "\n",
    "# This function must be defined at the top level of a module so that it can be \"pickled\"\n",
    "# (serialized) and sent to the worker processes.\n",
    "def run_simulation(params):\n",
    "    \"\"\"A dummy function that performs some CPU-intensive work.\"\"\"\n",
    "    sim_id, alpha, beta = params\n",
    "    result = 0\n",
    "    # This loop is a placeholder for a more complex calculation.\n",
    "    for i in range(1_000_000):\n",
    "        result += np.sin(i * alpha) * np.cos(i * beta)\n",
    "    return sim_id, result\n",
    "\n",
    "# The `if __name__ == '__main__':` guard is essential for multiprocessing on some platforms\n",
    "# (like Windows and macOS). It prevents worker processes from re-importing and re-executing\n",
    "# the script's main code, which would lead to an infinite loop of process creation.\n",
    "if __name__ == '__main__': \n",
    "    # Create a grid of parameters for our simulation.\n",
    "    # Each tuple in the list is a separate, independent task.\n",
    "    n_sims = mp.cpu_count() # Run one simulation per available CPU core\n",
    "    param_grid = [(i, alpha, beta) for i, (alpha, beta) in enumerate(np.random.rand(n_sims, 2))]\n",
    "    note(f\"Running a parameter sweep with {len(param_grid)} simulations on {n_sims} cores...\")\n",
    "    \n",
    "    # Time the serial execution first for comparison\n",
    "    start_serial = time.time()\n",
    "    serial_results = [run_simulation(p) for p in param_grid]\n",
    "    end_serial = time.time()\n",
    "    serial_time = end_serial - start_serial\n",
    "    print(f\"Serial execution time: {serial_time:.2f}s\")\n",
    "\n",
    "    # Now, execute in parallel\n",
    "    start_parallel = time.time()\n",
    "    # `mp.Pool` creates a pool of worker processes. Using a `with` statement ensures\n",
    "    # the pool is properly closed afterwards.\n",
    "    with mp.Pool(processes=n_sims) as pool:\n",
    "        # `pool.map` is like the built-in `map` function but it distributes the\n",
    "        # tasks in `param_grid` across the worker processes.\n",
    "        parallel_results = pool.map(run_simulation, param_grid)\n",
    "    end_parallel = time.time()\n",
    "    parallel_time = end_parallel - start_parallel\n",
    "    print(f\"Multiprocessing execution time: {parallel_time:.2f}s\")\n",
    "    \n",
    "    note(f\"Parallel execution provides a **{serial_time / parallel_time:.2f}x** speedup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 High-Level Parallelism with `Dask`\n",
    "\n",
    "`Dask` is a modern, powerful library for parallel computing in Python. It can replicate the functionality of `multiprocessing` with a more flexible API and can also scale from a single machine to a distributed cluster of many machines.\n",
    "\n",
    "A core concept in Dask is the **delayed** object. When you wrap a function call with `dask.delayed`, Dask does not execute it immediately. Instead, it builds a **symbolic computation graph** (a directed acyclic graph or DAG) representing all tasks and their dependencies. This \"lazy evaluation\" allows Dask's intelligent scheduler to analyze the entire workflow and optimize its execution, which can lead to better performance by minimizing data movement and maximizing parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Computing for Massively Parallel Problems\n",
    "\n",
    "For certain types of problems that fit the **SIMD (Single Instruction, Multiple Data)** paradigm—like large matrix multiplications, image processing, or deep learning—we can leverage a **Graphics Processing Unit (GPU)**. Consumer GPUs contain thousands of simpler cores compared to a CPU's handful of complex cores, making them exceptionally fast for these specific tasks.\n",
    "\n",
    "Two popular libraries for GPU computing in Python are:\n",
    "*   **`CuPy`**: A library that provides a near-complete clone of the NumPy API but executes operations on an NVIDIA GPU. For code already written using NumPy, you can often achieve massive speedups by simply replacing `import numpy as np` with `import cupy as cp`.\n",
    "*   **`JAX`**: A newer library from Google that combines a NumPy-like API with its own JIT compiler (similar to Numba), automatic differentiation (for machine learning), and the ability to run transparently on CPUs, GPUs, and TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"GPU Acceleration Example: NumPy vs. CuPy\")\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "    # Get GPU device name\n",
    "    gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode('utf-8')\n",
    "except (ImportError, cp.cuda.runtime.CUDARuntimeError):\n",
    "    CUPY_AVAILABLE = False\n",
    "\n",
    "if CUPY_AVAILABLE:\n",
    "    note(f\"Found compatible GPU: {gpu_name}\")\n",
    "    # Create large random matrices on both CPU (NumPy) and GPU (CuPy)\n",
    "    size = 4000\n",
    "    np_A, np_B = np.random.rand(size, size).astype(np.float32), np.random.rand(size, size).astype(np.float32)\n",
    "    \n",
    "    # Transfer data to the GPU\n",
    "    cp_A, cp_B = cp.asarray(np_A), cp.asarray(np_B)\n",
    "    \n",
    "    note(f\"Timing matrix multiplication for a {size}x{size} matrix...\")\n",
    "    # Time NumPy on the CPU\n",
    "    numpy_time = timeit.timeit(lambda: np_A @ np_B, number=10)\n",
    "    \n",
    "    # Time CuPy on the GPU. We must synchronize the device to ensure the computation\n",
    "    # is finished before stopping the timer.\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    cupy_time = timeit.timeit(lambda: cp_A @ cp_B, number=10)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "    print(f\"NumPy (CPU) time: {numpy_time:.4f} seconds\")\n",
    "    print(f\"CuPy (GPU) time:  {cupy_time:.4f} seconds\")\n",
    "    note(f\"GPU provides a **{numpy_time / cupy_time:.1f}x** speedup for this operation.\")\n",
    "else:\n",
    "    note(\"CuPy is not installed or no compatible NVIDIA GPU is found. Skipping GPU example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling: Finding the Bottleneck\n",
    "\n",
    "As emphasized in the introduction, optimization efforts should always be guided by **profiling**. A profiler is a tool that measures where a program spends its time and how often different functions are called. Instead of guessing, you can get precise, empirical data on which functions are the performance bottlenecks.\n",
    "\n",
    "Python's built-in **`cProfile`** module provides function-level profiling. It is a good first step for getting a high-level overview of your program's performance. For more granular detail, third-party libraries like `line_profiler` can provide line-by-line profiling statistics, which is invaluable for identifying the single slowest line within a larger function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Profiling with cProfile\")\n",
    "import cProfile, pstats\n",
    "\n",
    "# Define two functions to simulate a simple workflow\n",
    "def slow_function():\n",
    "    \"\"\"This function is deliberately slow to simulate a bottleneck.\"\"\"\n",
    "    time.sleep(0.1) # Represents I/O-bound work\n",
    "    _ = [math.sqrt(i) for i in range(10**4)] # Represents CPU-bound work\n",
    "\n",
    "def fast_function():\n",
    "    \"\"\"This function is fast and does not contribute much to the total runtime.\"\"\"\n",
    "    pass\n",
    "\n",
    "def main_workflow():\n",
    "    \"\"\"The main entry point of our program, calling the other functions.\"\"\"\n",
    "    # slow_function is called only 5 times, but is the bottleneck.\n",
    "    for _ in range(5):\n",
    "        slow_function()\n",
    "    # fast_function is called 100 times, but is not the bottleneck.\n",
    "    for _ in range(100):\n",
    "        fast_function()\n",
    "\n",
    "# 1. Create a profiler object\n",
    "profiler = cProfile.Profile()\n",
    "\n",
    "# 2. Wrap the code you want to profile between enable() and disable()\n",
    "profiler.enable()\n",
    "main_workflow()\n",
    "profiler.disable()\n",
    "\n",
    "# 3. Print the statistics\n",
    "# We create a pstats.Stats object to analyze the profiler's output.\n",
    "# We sort the results by 'cumulative' time to see which functions took the longest in total.\n",
    "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "\n",
    "# `print_stats(10)` shows the top 10 functions contributing to the runtime.\n",
    "stats.print_stats(10)\n",
    "\n",
    "note(\"The profiler output (see `cumtime` column) clearly shows that nearly all the execution time is spent inside `slow_function`, making it the obvious and only target for optimization efforts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
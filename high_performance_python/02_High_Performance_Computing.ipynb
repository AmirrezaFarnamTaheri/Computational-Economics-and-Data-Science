{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, timeit\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from numba import njit, prange, vectorize\n",
    "    NUMBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    def njit(func=None, **kwargs): return func if func else lambda f: f\n",
    "    def prange(*args, **kwargs): return range(*args, **kwargs)\n",
    "    def vectorize(func=None, **kwargs): return np.vectorize(func) if func else lambda f: np.vectorize(f)\n",
    "    NUMBA_AVAILABLE = False\n",
    "try:\n",
    "    import dask\n",
    "    from dask import delayed\n",
    "    DASK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DASK_AVAILABLE = False\n",
    "import multiprocessing as mp\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for High-Performance Computing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Advanced Computational Methods\n",
    "## Chapter 7.6: High-Performance Computing for Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Why and When to Optimize\n",
    "\n",
    "As economic models become more complex and datasets larger, computational cost can become a significant bottleneck. Standard Python, an interpreted language, can be orders of magnitude slower than compiled languages like C++ or Fortran for number-crunching tasks, especially those involving explicit `for` loops.\n",
    "\n",
    "**High-Performance Computing (HPC)** provides techniques to break through these barriers. Mastering these tools is essential for the modern computational economist. However, it is crucial to heed the famous adage: **\"Premature optimization is the root of all evil.\"**\n",
    "\n",
    "The workflow for a computational project should always be:\n",
    "1.  **Write it correctly:** First, write clear, simple, and well-tested code.\n",
    "2.  **Profile it:** If the correct code is too slow, use a profiler to identify exactly *which functions* are the bottlenecks.\n",
    "3.  **Optimize the bottleneck:** Only after identifying a specific bottleneck should you apply the advanced tools discussed in this chapter.\n",
    "\n",
    "This chapter provides a hands-on introduction to the most common HPC techniques for economic research, focusing on JIT compilation, parallel computing, and GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Just-In-Time (JIT) Compilation with Numba\n",
    "\n",
    "When a vectorized NumPy solution is not feasible, **Numba** is the first line of defense against slow loops. Numba is a **Just-In-Time (JIT) compiler** that translates a subset of Python and NumPy code into fast, optimized machine code via a simple function decorator.\n",
    "\n",
    "The `@njit` decorator is the recommended mode. It stands for **\"no-python JIT,\"** which guarantees that the entire function is compiled to machine code without any calls back to the slow Python interpreter. If Numba cannot compile some part of the function (e.g., because you used an unsupported feature like a dictionary), it will raise an error. This forces you to use data structures (primarily NumPy arrays and simple scalar types) that can be efficiently compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Numba for Accelerating Loops: A Monte Carlo Example\")\n",
    "\n",
    "def monte_carlo_pi_python(n_samples):\n",
    "    acc = 0\n",
    "    for i in range(n_samples):\n",
    "        x, y = random.random(), random.random()\n",
    "        if x**2 + y**2 < 1.0: acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def monte_carlo_pi_numba(n_samples):\n",
    "    acc = 0\n",
    "    for i in prange(n_samples):\n",
    "        x, y = np.random.rand(), np.random.rand()\n",
    "        if x**2 + y**2 < 1.0: acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "n = 10_000_000\n",
    "if NUMBA_AVAILABLE:\n",
    "    py_time = timeit.timeit(lambda: monte_carlo_pi_python(n), number=1)\n",
    "    # Warm up Numba\n",
    "    monte_carlo_pi_numba(1)\n",
    "    numba_time = timeit.timeit(lambda: monte_carlo_pi_numba(n), number=1)\n",
    "    note(f\"Numba provides a {py_time / numba_time:.1f}x speedup over pure Python for this task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Computing: Theory and Practice\n",
    "\n",
    "### 2.1 The Limits of Parallelization: Amdahl's Law\n",
    "**Amdahl's Law** provides the formula for the maximum possible speedup from parallelizing a task. Let $P$ be the proportion of a program that can be parallelized. The maximum speedup from using $N$ processors is:\n",
    "$$ \\text{Speedup}(N, P) = \\frac{1}{(1-P) + \\frac{P}{N}} $$ \n",
    "The serial portion $(1-P)$ acts as a permanent bottleneck. As $N \\to \\infty$, the maximum speedup is limited to $1 / (1-P)$. If 10% of your code is inherently serial, you can never achieve more than a 10x speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Global Interpreter Lock (GIL) and Multiprocessing\n",
    "A naive approach to use multiple CPU cores might be Python's `threading` module. However, the standard Python interpreter (CPython) has a **Global Interpreter Lock (GIL)**, a mutex that prevents multiple native threads from executing Python bytecodes at the same time within a single process. This means that for **CPU-bound** code, multithreading provides no performance gain.\n",
    "\n",
    "We bypass the GIL by using **multi-processing**, where the OS runs each task in a separate process with its own interpreter and memory space. This is ideal for **\"embarrassingly parallel\"** problems where tasks are independent and require little communication (e.g., parameter sweeps, bootstrapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Parallelism with Multiprocessing\")\n",
    "\n",
    "# This function must be defined at the top level of the module to be pickleable\n",
    "def run_simulation(params):\n",
    "    sim_id, alpha, beta = params\n",
    "    result = 0\n",
    "    for i in range(1_000_000): result += np.sin(i * alpha) * np.cos(i * beta)\n",
    "    return sim_id, result\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    param_grid = [(i, alpha, beta) for i, (alpha, beta) in enumerate(np.random.rand(8, 2))]\n",
    "    note(f\"Running a parameter sweep with {len(param_grid)} simulations...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with mp.Pool(processes=4) as pool:\n",
    "        results = pool.map(run_simulation, param_grid)\n",
    "    end_time = time.time()\n",
    "    note(f\"Multiprocessing execution time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 High-Level Parallelism with `Dask`\n",
    "`Dask` is a modern library for parallel computing. It can replicate `multiprocessing`'s functionality with a more flexible API and can also scale to a distributed cluster. A core concept is the **delayed** object. When you wrap a function call with `delayed`, Dask builds a **symbolic computation graph** representing all tasks and their dependencies. This \"lazy evaluation\" allows Dask's intelligent scheduler to optimize the workflow, often leading to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Computing for Massively Parallel Problems\n",
    "\n",
    "For problems that fit the **SIMD (Single Instruction, Multiple Data)** paradigm, like large matrix multiplications, we can leverage a **Graphics Processing Unit (GPU)**. GPUs contain thousands of simpler cores, making them exceptionally fast for such tasks.\n",
    "\n",
    "*   **`CuPy`**: A near-complete clone of the NumPy API that executes on an NVIDIA GPU. For code already written using NumPy, you can often achieve massive speedups by simply replacing `import numpy as np` with `import cupy as cp`.\n",
    "*   **`JAX`**: A newer library from Google that combines a NumPy-like API with its own JIT compiler, automatic differentiation, and the ability to run transparently on CPUs, GPUs, and TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"GPU Acceleration Example: NumPy vs. CuPy\")\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CUPY_AVAILABLE = False\n",
    "\n",
    "if CUPY_AVAILABLE:\n",
    "    # Create large random matrices on both CPU (NumPy) and GPU (CuPy)\n",
    "    size = 4000\n",
    "    np_A, np_B = np.random.rand(size, size), np.random.rand(size, size)\n",
    "    cp_A, cp_B = cp.asarray(np_A), cp.asarray(np_B)\n",
    "    \n",
    "    note(f\"Timing matrix multiplication for a {size}x{size} matrix...\")\n",
    "    # Time NumPy\n",
    "    numpy_time = timeit.timeit(lambda: np_A @ np_B, number=10)\n",
    "    # Time CuPy (after a warm-up)\n",
    "    cp.cuda.runtime.deviceSynchronize() # Synchronize to get accurate timing\n",
    "    cupy_time = timeit.timeit(lambda: cp_A @ cp_B, number=10)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "    print(f\"NumPy (CPU) time: {numpy_time:.4f} seconds\")\n",
    "    print(f\"CuPy (GPU) time:  {cupy_time:.4f} seconds\")\n",
    "    print(f\"GPU Speedup:      {numpy_time / cupy_time:.1f}x\")\n",
    "else:\n",
    "    note(\"CuPy is not installed or no compatible GPU is found. Skipping GPU example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling: Finding the Bottleneck\n",
    "\n",
    "Optimization efforts should always be guided by **profiling**. A profiler is a tool that measures where a program spends its time. Instead of guessing, you can get precise data on which functions are the performance bottlenecks.\n",
    "\n",
    "Python's built-in `cProfile` module provides function-level profiling. For more granular detail, the third-party `line_profiler` library provides line-by-line profiling, which is invaluable for identifying the single slowest line within a larger function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Profiling with cProfile\")\n",
    "import cProfile, pstats\n",
    "\n",
    "# Define two functions to simulate a workflow\n",
    "def slow_function():\n",
    "    # This function is deliberately slow\n",
    "    time.sleep(0.1)\n",
    "    [math.sqrt(i) for i in range(10**4)]\n",
    "\n",
    "def fast_function():\n",
    "    # This function is fast\n",
    "    pass\n",
    "\n",
    "def main_workflow():\n",
    "    for _ in range(5):\n",
    "        slow_function()\n",
    "    for _ in range(100):\n",
    "        fast_function()\n",
    "\n",
    "# Create a profiler object and run it on our main function\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "main_workflow()\n",
    "profiler.disable()\n",
    "\n",
    "# Print the stats\n",
    "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "stats.print_stats(10) # Print the top 10 offenders\n",
    "note(\"The profiler output clearly shows that nearly all the execution time is spent inside `slow_function`, making it the obvious target for optimization efforts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
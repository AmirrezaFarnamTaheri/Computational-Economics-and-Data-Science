{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings, timeit\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from numba import njit, prange, vectorize\n",
    "    NUMBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    def njit(func=None, **kwargs): return func if func else lambda f: f\n",
    "    def prange(*args, **kwargs): return range(*args, **kwargs)\n",
    "    def vectorize(func=None, **kwargs): return np.vectorize(func) if func else lambda f: np.vectorize(f)\n",
    "    NUMBA_AVAILABLE = False\n",
    "try:\n",
    "    import dask\n",
    "    from dask import delayed\n",
    "    DASK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DASK_AVAILABLE = False\n",
    "import multiprocessing as mp\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for High-Performance Computing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Advanced Computational Methods\n",
    "## Chapter 7.6: High-Performance Computing for Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Why and When to Optimize\n",
    "\n",
    "As economic models become more complex and datasets larger, computational cost can become a significant bottleneck. Standard Python, an interpreted language, can be orders of magnitude slower than compiled languages like C++ or Fortran for number-crunching tasks, especially those involving explicit `for` loops.\n",
    "\n",
    "**High-Performance Computing (HPC)** provides techniques to break through these barriers. Mastering these tools is essential for the modern computational economist. However, it is crucial to heed the famous adage: **\"Premature optimization is the root of all evil.\"**\n",
    "\n",
    "The workflow for a computational project should always be:\n",
    "1.  **Write it correctly:** First, write clear, simple, and well-tested code.\n",
    "2.  **Profile it:** If the correct code is too slow, use a profiler to identify exactly *which functions* are the bottlenecks.\n",
    "3.  **Optimize the bottleneck:** Only after identifying a specific bottleneck should you apply the advanced tools discussed in this chapter.\n",
    "\n",
    "This chapter provides a hands-on introduction to the most common HPC techniques for economic research, focusing on JIT compilation, parallel computing, and GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Just-In-Time (JIT) Compilation with Numba\n",
    "\n",
    "When a vectorized NumPy solution is not feasible, **Numba** is the first line of defense against slow loops. Numba is a **Just-In-Time (JIT) compiler** that translates a subset of Python and NumPy code into fast, optimized machine code via a simple function decorator.\n",
    "\n",
    "The `@njit` decorator is the recommended mode. It stands for **\"no-python JIT,\"** which guarantees that the entire function is compiled to machine code without any calls back to the slow Python interpreter. If Numba cannot compile some part of the function (e.g., because you used an unsupported feature like a dictionary), it will raise an error. This forces you to use data structures (primarily NumPy arrays and simple scalar types) that can be efficiently compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Numba for Accelerating Loops: A Monte Carlo Example\")\n",
    "\n",
    "def monte_carlo_pi_python(n_samples):\n",
    "    acc = 0\n",
    "    for i in range(n_samples):\n",
    "        x, y = random.random(), random.random()\n",
    "        if x**2 + y**2 < 1.0: acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "# The @njit decorator compiles the function to machine code.\n",
    "# - `parallel=True`: Numba will attempt to automatically parallelize loops.\n",
    "# - `cache=True`: The compiled function is saved to disk to avoid re-compilation on subsequent runs.\n",
    "@njit(parallel=True, cache=True)\n",
    "def monte_carlo_pi_numba(n_samples):\n",
    "    acc = 0\n",
    "    # `prange` is Numba's parallel range, which is enabled by `parallel=True`.\n",
    "    for i in prange(n_samples):\n",
    "        x, y = np.random.rand(), np.random.rand()\n",
    "        if x**2 + y**2 < 1.0: acc += 1\n",
    "    return 4.0 * acc / n_samples\n",
    "\n",
    "n = 10_000_000\n",
    "if NUMBA_AVAILABLE:\n",
    "    py_time = timeit.timeit(lambda: monte_carlo_pi_python(n), number=1)\n",
    "    # The first call to a Numba function has a compilation overhead.\n",
    "    monte_carlo_pi_numba(1)\n",
    "    numba_time = timeit.timeit(lambda: monte_carlo_pi_numba(n), number=1)\n",
    "    note(f\"Numba provides a {py_time / numba_time:.1f}x speedup over pure Python for this task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Computing: Theory and Practice\n",
    "\n",
    "### 2.1 The Limits of Parallelization: Amdahl's Law\n",
    "**Amdahl's Law** provides a sobering formula for the maximum possible speedup from parallelizing a task. The core idea is that the portion of a program that is inherently sequential acts as a bottleneck, limiting the gains from adding more processors. \n",
    "\n",
    "Let $P$ be the proportion of a program that can be parallelized, and $(1-P)$ the proportion that must be run sequentially. The maximum speedup from using $N$ processors is:\n",
    "$$ \\text{Speedup}(N, P) = \\frac{1}{(1-P) + \\frac{P}{N}} $$ \n",
    "\n",
    "As the number of processors $N$ approaches infinity, the term $\\frac{P}{N}$ goes to zero. This means the maximum theoretical speedup is limited to $1 / (1-P)$. For example, if 95% of your code can be parallelized (P = 0.95), the maximum speedup you can ever achieve is $1 / (1 - 0.95) = 20x$, regardless of how many cores you use. This highlights the importance of minimizing the sequential parts of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Amdahl's Law Visualization](../../images/high_performance_python/AmdahlsLaw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Global Interpreter Lock (GIL) and Multiprocessing\n",
    "\n",
    "A common desire is to use multiple CPU cores with Python's `threading` module. However, the standard Python interpreter (CPython) has a feature called the **Global Interpreter Lock (GIL)**. The GIL is a lock that prevents multiple native threads from executing Python bytecodes at the same time within a single process. It was implemented to simplify memory management and prevent race conditions in C extensions, but it has the side effect of making multithreading ineffective for **CPU-bound** code (code that is limited by the speed of the CPU).\n",
    "\n",
    "To truly parallelize CPU-bound tasks, we bypass the GIL by using **multi-processing**. With multiprocessing, the operating system runs each task in a separate process, each with its own Python interpreter and memory space. This is ideal for **\"embarrassingly parallel\"** problems where tasks are independent and require little communication, such as parameter sweeps or bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Parallelism with Multiprocessing\")\n",
    "\n",
    "# This function must be defined at the top level of the module to be 'pickleable' (serializable) by multiprocessing.\n",
    "def run_simulation(params):\n",
    "    sim_id, alpha, beta = params\n",
    "    result = 0\n",
    "    for i in range(1_000_000): result += np.sin(i * alpha) * np.cos(i * beta)\n",
    "    return sim_id, result\n",
    "\n",
    "# The `if __name__ == '__main__':` block is essential for multiprocessing on some platforms.\n",
    "# It prevents child processes from re-importing and re-executing the script, which would lead to an infinite loop.\n",
    "if __name__ == '__main__': \n",
    "    param_grid = [(i, alpha, beta) for i, (alpha, beta) in enumerate(np.random.rand(8, 2))]\n",
    "    note(f\"Running a parameter sweep with {len(param_grid)} simulations...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # We create a pool of 4 worker processes.\n",
    "    with mp.Pool(processes=4) as pool:\n",
    "        # `pool.map` distributes the `param_grid` across the worker processes.\n",
    "        results = pool.map(run_simulation, param_grid)\n",
    "    end_time = time.time()\n",
    "    note(f\"Multiprocessing execution time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 High-Level Parallelism with `Dask`\n",
    "`Dask` is a modern library for parallel computing that can scale from a single machine to a distributed cluster. A key concept in Dask is the **delayed** object. When you wrap a function call with `delayed`, Dask builds a **symbolic computation graph** representing all tasks and their dependencies. This \"lazy evaluation\" allows Dask's intelligent scheduler to optimize the workflow before executing it, which can lead to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Dask Delayed Example\")\n",
    "\n",
    "if DASK_AVAILABLE:\n",
    "    @delayed\n",
    "    def inc(x):\n",
    "        time.sleep(0.1) # Simulate work\n",
    "        return x + 1\n",
    "\n",
    "    @delayed\n",
    "    def add(x, y):\n",
    "        time.sleep(0.1) # Simulate work\n",
    "        return x + y\n",
    "\n",
    "    x = inc(1)\n",
    "    y = inc(2)\n",
    "    total = add(x, y)\n",
    "\n",
    "    # At this point, no computation has happened. `total` is a delayed object.\n",
    "    note(\"Visualizing the Dask computation graph:\")\n",
    "    display(total) # In a Jupyter environment, this would show the graph\n",
    "\n",
    "    # To run the computation, we call .compute()\n",
    "    start_time = time.time()\n",
    "    result = total.compute()\n",
    "    end_time = time.time()\n",
    "    note(f\"Dask execution time: {end_time - start_time:.2f}s, Result: {result}\")\n",
    "else:\n",
    "    note(\"Dask is not installed. Skipping Dask example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An Alternative: `Cython`\n",
    "\n",
    "`Cython` offers another path to high performance by allowing you to write C-like static types directly in Python-like code. Cython code is translated into optimized C or C++ and then compiled into a Python extension module. This gives you fine-grained control over performance-critical sections of your code.\n",
    "\n",
    "**When to use Cython vs. Numba:**\n",
    "*   **Numba:** Best for accelerating numerical algorithms that are expressed as Python functions with loops over NumPy arrays. It's often easier to get started with, requiring just a decorator.\n",
    "*   **Cython:** More flexible and powerful. It's a good choice when you need to interact with external C libraries, work with complex data structures not supported by Numba, or when you need to optimize code that is not purely numerical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU Computing for Massively Parallel Problems\n",
    "\n",
    "For problems that fit the **SIMD (Single Instruction, Multiple Data)** paradigm—where the same operation is applied to many data points simultaneously—we can use a **Graphics Processing Unit (GPU)**. GPUs contain thousands of simpler cores, making them exceptionally fast for tasks like large matrix multiplications.\n",
    "\n",
    "*   **`CuPy`**: A near-complete clone of the NumPy API that executes on an NVIDIA GPU. For code already written using NumPy, you can often achieve massive speedups by simply replacing `import numpy as np` with `import cupy as cp`.\n",
    "*   **`JAX`**: A newer library from Google that combines a NumPy-like API with its own JIT compiler, automatic differentiation, and the ability to run transparently on CPUs, GPUs, and TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"GPU Acceleration Example: NumPy vs. CuPy\")\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CUPY_AVAILABLE = False\n",
    "\n",
    "if CUPY_AVAILABLE:\n",
    "    # Create large random matrices on both CPU (NumPy) and GPU (CuPy)\n",
    "    size = 4000\n",
    "    np_A, np_B = np.random.rand(size, size), np.random.rand(size, size)\n",
    "    cp_A, cp_B = cp.asarray(np_A), cp.asarray(np_B)\n",
    "    \n",
    "    note(f\"Timing matrix multiplication for a {size}x{size} matrix...\")\n",
    "    # Time NumPy\n",
    "    numpy_time = timeit.timeit(lambda: np_A @ np_B, number=10)\n",
    "    # Time CuPy (after a warm-up)\n",
    "    cp.cuda.runtime.deviceSynchronize() # Synchronize to get accurate timing\n",
    "    cupy_time = timeit.timeit(lambda: cp_A @ cp_B, number=10)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "    print(f\"NumPy (CPU) time: {numpy_time:.4f} seconds\")\n",
    "    print(f\"CuPy (GPU) time:  {cupy_time:.4f} seconds\")\n",
    "    print(f\"GPU Speedup:      {numpy_time / cupy_time:.1f}x\")\n",
    "else:\n",
    "    note(\"CuPy is not installed or no compatible GPU is found. Skipping GPU example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Profiling: Finding the Bottleneck\n",
    "\n",
    "Optimization efforts should always be guided by **profiling**. A profiler is a tool that measures where a program spends its time. Instead of guessing, you can get precise data on which functions are the performance bottlenecks.\n",
    "\n",
    "Python's built-in `cProfile` module provides function-level profiling. For more granular detail, the third-party `line_profiler` library provides line-by-line profiling, which is invaluable for identifying the single slowest line within a larger function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Profiling with cProfile\")\n",
    "import cProfile, pstats\n",
    "\n",
    "# Define two functions to simulate a workflow\n",
    "def slow_function():\n",
    "    # This function is deliberately slow\n",
    "    time.sleep(0.1)\n",
    "    [math.sqrt(i) for i in range(10**4)]\n",
    "\n",
    "def fast_function():\n",
    "    # This function is fast\n",
    "    pass\n",
    "\n",
    "def main_workflow():\n",
    "    for _ in range(5):\n",
    "        slow_function()\n",
    "    for _ in range(100):\n",
    "        fast_function()\n",
    "\n",
    "# Create a profiler object and run it on our main function\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "main_workflow()\n",
    "profiler.disable()\n",
    "\n",
    "# Print the stats\n",
    "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "stats.print_stats(10) # Print the top 10 offenders\n",
    "note(\"The profiler output clearly shows that nearly all the execution time is spent inside `slow_function`, making it the obvious target for optimization efforts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for the Transformer Architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.10: The Transformer Architecture: Attention Is All You Need\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Introduction: Beyond Recurrence**](#intro)\n",
    "2.  [**The Transformer Architecture: A Formal View**](#architecture)\n",
    "    - [Positional Encoding](#positional)\n",
    "    - [Scaled Dot-Product Attention](#attention)\n",
    "    - [Multi-Head Attention](#multi-head)\n",
    "3.  [**Theoretical Advantages over RNNs**](#advantages)\n",
    "4.  [**Implementation from Scratch**](#scratch)\n",
    "    - [From-Scratch Self-Attention](#scratch-attention)\n",
    "    - [Case Study: Forecasting a Chaotic Time Series](#scratch-lorenz)\n",
    "5.  [**Vision Transformer (ViT)**](#vit)\n",
    "6.  [**Exercises**](#exercises)\n",
    "7.  [**Summary and Key Takeaways**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: Beyond Recurrence\n",
    "\n",
    "Recurrent Neural Networks (RNNs), LSTMs, and GRUs process data sequentially. This sequential nature creates two major challenges:\n",
    "1.  **The Sequential Bottleneck:** The hidden state at time $t$ must encapsulate all relevant information from the entire sequence up to that point, making it difficult to preserve information from the distant past.\n",
    "2.  **Lack of Parallelization:** The need to process tokens one after another makes it impossible to fully leverage modern parallel computing hardware like GPUs.\n",
    "\n",
    "The **Transformer** architecture, introduced in the seminal 2017 paper \"Attention Is All You Need\" by Vaswani et al., revolutionized sequence modeling by dispensing with recurrence entirely. Instead of a sequential loop, it relies on a powerful mechanism called **self-attention**, which allows the model to directly weigh the importance of all other words in the input sequence when processing a given word. This enables the model to draw global dependencies and allows for massive parallelization, forming the foundation for modern Large Language Models (LLMs) like GPT and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architecture'></a>\n",
    "## 2. The Transformer Architecture: A Formal View\n",
    "\n",
    "The Transformer maintains the high-level **encoder-decoder structure** common in machine translation, but each component is built without recurrence. \n",
    "\n",
    "![Transformer Architecture](../images/png/transformer_architecture_1.png)\n",
    "*<center><b>Figure 1: High-level overview of the Transformer's encoder-decoder architecture.</b></center>*\n",
    "\n",
    "The core building blocks are:\n",
    "- **Positional Encoding:** Since the model has no inherent sense of order, we inject vectors that provide a unique signature for each position in the sequence.\n",
    "- **Multi-Head Self-Attention:** The core mechanism that allows the model to weigh the importance of different tokens in the sequence.\n",
    "- **Position-wise Feed-Forward Networks:** A simple fully connected network applied independently to each position.\n",
    "- **Residual Connections and Layer Normalization:** These are crucial for enabling stable training of very deep stacks of these layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='positional'></a>\n",
    "### 2.1 Positional Encoding\n",
    "\n",
    "Since the self-attention mechanism processes all tokens in parallel, it has no inherent sense of sequence order. To fix this, we inject **positional encodings** into the input embeddings. The original paper uses a clever scheme with sine and cosine functions of different frequencies:\n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}}) $$ \n",
    "$$ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}}) $$ \n",
    "\n",
    "where `pos` is the position of the token in the sequence and `i` is the dimension in the embedding vector. This method has the advantage that for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$, which may allow the model to easily learn to attend to relative positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attention'></a>\n",
    "### 2.2 Scaled Dot-Product Attention\n",
    "\n",
    "**Self-attention** allows the model to look at other positions in the input sequence for clues that can help lead to a better encoding for the current position. The intuition comes from information retrieval.\n",
    "\n",
    "For each input token embedding, we create three vectors by multiplying it by learned weight matrices: a **Query (Q)**, a **Key (K)**, and a **Value (V)**.\n",
    "\n",
    "The process is:\n",
    "1.  **Calculate Scores:** The attention score is the dot product of the **Query** vector of the current token with the **Key** vectors of all other tokens. This measures compatibility.\n",
    "2.  **Scale and Softmax:** The scores are scaled down by $\\sqrt{d_k}$ (the dimension of the key vectors). This is crucial for stabilizing gradients during training. A softmax function then turns the scores into attention weights, which sum to 1.\n",
    "3.  **Weighted Sum:** The final output for the token is the weighted sum of the **Value** vectors of all tokens, using the attention weights.\n",
    "\n",
    "The full calculation for a sequence, performed in a single matrix operation, is:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi-head'></a>\n",
    "### 2.3 Multi-Head Attention\n",
    "\n",
    "Instead of a single attention function, Transformers use **Multi-Head Attention**. This involves projecting the Q, K, and V vectors into different subspaces with different learned linear projections ($W_i^Q, W_i^K, W_i^V$). We run the attention mechanism independently in parallel for each \"head,\" and finally, concatenate the results and project them back to the original dimension.\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O $$ \n",
    "$$ \\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$ \n",
    "This allows the model to jointly attend to information from different representation subspaces at different positions. For example, one attention head might learn to focus on syntactic relationships, while another focuses on semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advantages'></a>\n",
    "## 3. Theoretical Advantages over RNNs\n",
    "\n",
    "The Transformer architecture has several key advantages that explain its dominance.\n",
    "\n",
    "1.  **Computational Complexity:** For a sequence of length $n$ and representation dimension $d$, the complexity per layer is $O(n^2 \\cdot d)$ for a Transformer (due to the attention matrix) and $O(n \\cdot d^2)$ for an RNN. If the sequence length $n$ is smaller than the dimension $d$ (which is common), the Transformer is computationally more efficient.\n",
    "\n",
    "2.  **Parallelization:** The Transformer contains no recurrence. The computations for all tokens within a layer can be performed in parallel, making it ideal for modern GPUs.\n",
    "\n",
    "3.  **Path Length:** The most critical advantage is the **path length** between any two positions in the network. In an RNN, the path length is $O(n)$, requiring the signal to propagate through all intermediate steps. In a Transformer, the self-attention mechanism allows for a direct connection between any two tokens, making the path length $O(1)$. This dramatically reduces the difficulty of learning long-range dependencies and is the fundamental solution to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scratch'></a>\\n",
    "## 4. Implementation from Scratch\\n",
    "\\n",
    "<a id='scratch-attention'></a>\\n",
    "### 4.1 From-Scratch Self-Attention\\n",
    "To demystify the core mechanism, we can implement the scaled dot-product attention function from scratch. This makes the matrix operations explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"From-Scratch Scaled Dot-Product Attention\")\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "note(\"This function encapsulates the core logic: Score, Scale, Softmax, and Weighted Sum.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing Self-Attention Weights\")\n",
    "\n",
    "# Example: A sequence of 4 words, each with an embedding of size 8\n",
    "x_example = tf.random.normal((4, 8))\n",
    "qkv_layer = keras.layers.Dense(3 * 8, use_bias=False)\n",
    "qkv = qkv_layer(x_example)\n",
    "q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "\n",
    "_, attention_weights_example = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights_example.numpy(), annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Key (other words)')\n",
    "plt.ylabel('Query (current word)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scratch-lorenz'></a>\\n",
    "### 4.2 Case Study: Forecasting a Chaotic Time Series\\n",
    "\\n",
    "A powerful demonstration of the Transformer's ability to learn complex, non-linear dynamics is to train it on a chaotic time series, such as the **Lorenz attractor**. This system is deterministic but highly sensitive to initial conditions, making long-term prediction extremely difficult. We will build a simple Transformer encoder model to predict future values of the Lorenz system from its past values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Forecasting the Lorenz Attractor with a Transformer\")\n",
    "\n",
    "# 1. Generate Lorenz Attractor Data\n",
    "def lorenz(xyz, *, s=10, r=28, b=2.667):\n",
    "    x, y, z = xyz\n",
    "    x_dot = s*(y - x)\n",
    "    y_dot = r*x - y - x*z\n",
    "    z_dot = x*y - b*z\n",
    "    return np.array([x_dot, y_dot, z_dot])\n",
    "\n",
    "dt = 0.01\n",
    "num_steps = 10000\n",
    "xyzs = np.empty((num_steps + 1, 3))\n",
    "xyzs[0] = (0., 1., 1.05)\n",
    "for i in range(num_steps):\n",
    "    xyzs[i + 1] = xyzs[i] + lorenz(xyzs[i]) * dt\n",
    "\n",
    "lorenz_series = xyzs[:, 0]\n",
    "\n",
    "# 2. Create Windowed Dataset\n",
    "def create_ts_sequences(series, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size):\n",
    "        X.append(series[i:i+window_size])\n",
    "        y.append(series[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 50\n",
    "X, y = create_ts_sequences(lorenz_series, window_size)\n",
    "X = X[..., np.newaxis]; y = y[..., np.newaxis]\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# 3. Build a Simple Transformer Encoder Model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(inputs + x)\n",
    "    ff_out = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    ff_out = keras.layers.Dropout(dropout)(ff_out)\n",
    "    ff_out = keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff_out)\n",
    "    return keras.layers.LayerNormalization(epsilon=1e-6)(x + ff_out)\n",
    "\n",
    "inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = inputs\n",
    "for _ in range(2):\n",
    "    x = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=4, dropout=0.1)\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "lorenz_model = keras.Model(inputs, outputs)\n",
    "lorenz_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "note(\"Skipping training for demonstration. This example shows that the general Transformer architecture can be successfully applied to complex, non-textual sequence data like time series.\")\n",
    "lorenz_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vit'></a>\\n",
    "## 5. Vision Transformer (ViT)\\n",
    "\\n",
    "The success of the Transformer in NLP led researchers to ask if the same architecture could be applied to computer vision. The **Vision Transformer (ViT)**, introduced by Dosovitskiy et al. (2020), showed that it could, and with remarkable success.\n",
    "\n",
    "The core idea is to treat an image as a sequence of patches:\n",
    "1.  **Patching:** The input image is split into a grid of fixed-size patches (e.g., 16x16 pixels).\n",
    "2.  **Linear Projection:** Each patch is flattened into a vector and linearly projected into an embedding dimension, similar to how words are embedded.\n",
    "3.  **Positional Encoding:** Positional encodings are added to the patch embeddings to retain spatial information.\n",
    "4.  **Transformer Encoder:** This sequence of patch embeddings is then fed directly into a standard Transformer encoder.\n",
    "5.  **Classification Head:** The output corresponding to a special `[CLS]` token is passed to a simple MLP head for final classification.\n",
    "\n",
    "ViT demonstrated that the general-purpose Transformer architecture, with its reliance on self-attention, could achieve state-of-the-art results on image classification tasks, challenging the long-held dominance of Convolutional Neural Networks (CNNs) in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\\n",
    "## 6. Exercises\\n",
    "\\n",
    "1.  **Attention Scores:** In the scaled dot-product attention formula, why is it important to scale the $QK^T$ product by $\\sqrt{d_k}$? What happens if $d_k$ is large and we don't scale?\\n",
    "2.  **Positional Encoding:** The original Transformer used sinusoidal positional encodings. An alternative is to use *learned* positional embeddings, where the positional vectors are treated as trainable parameters. What are the potential pros and cons of this approach compared to the sinusoidal method?\\n",
    "3.  **Inductive Bias:** CNNs have a strong **inductive bias** for vision tasks (locality and translation invariance). RNNs have an inductive bias for sequentiality. What is the inductive bias of the Transformer architecture? Is it strong or weak?\\n",
    "4.  **Self-Attention vs. Cross-Attention:** In the decoder part of the Transformer, there are two attention layers. The first is masked self-attention over the decoder's inputs. The second takes its Queries from the decoder but its Keys and Values from the *encoder's* output. This is often called **cross-attention**. What is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\\n",
    "## 7. Summary and Key Takeaways\\n",
    "\\n",
    "This chapter introduced the Transformer, the architecture that has become the foundation for most state-of-the-art models in NLP and beyond.\\n",
    "\\n",
    "**Key Concepts**:\\n",
    "- **Beyond Recurrence**: The Transformer dispenses with the sequential recurrence of RNNs, allowing for massive parallelization and direct modeling of long-range dependencies.\\n",
    "- **Self-Attention**: The core mechanism is scaled dot-product self-attention, where each token creates a Query, Key, and Value vector. The attention weights are computed by comparing the Query of one token to the Keys of all other tokens, and the output is a weighted sum of the Value vectors.\\n",
    "- **Path Length**: By creating direct connections between all tokens, the attention mechanism reduces the path length for gradient flow to $O(1)$, which solves the vanishing gradient problem that plagues RNNs.\\n",
    "- **Multi-Head Attention**: This mechanism allows the model to jointly attend to information from different representation subspaces at different positions, enhancing the richness of the learned representations.\\n",
    "- **Positional Encoding**: Since the architecture itself contains no notion of order, we must inject positional information into the input embeddings.\\n",
    "- **Generality**: The Transformer is a general-purpose architecture for learning on sets and sequences, as demonstrated by its successful application to computer vision in the Vision Transformer (ViT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**1. Attention Scores:**\\n",
    "The scaling by $\\sqrt{d_k}$ is a crucial stabilization technique. If the key dimension $d_k$ is large, the dot products $QK^T$ can grow very large in magnitude. When these large values are fed into the softmax function, the function's gradients can become extremely small, effectively killing the gradient flow and stalling training. Scaling by $\\sqrt{d_k}$ keeps the variance of the dot products at 1, ensuring that the softmax function operates in a region with healthy gradients.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**2. Learned Positional Embeddings:**\\n",
    "- **Pros:** Learned embeddings are more flexible. The model can learn the optimal positional representations for the specific task and dataset, which might be better than the fixed sinusoidal functions.\\n",
    "- **Cons:** They do not generalize to sequence lengths longer than those seen during training. The sinusoidal encoding, being a deterministic function of position, can handle sequences of any length.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**3. Inductive Bias of Transformers:**\\n",
    "The Transformer has a very **weak inductive bias**. Unlike a CNN, it does not assume locality in the input. Unlike an RNN, it does not assume sequentiality. Its primary assumption is that the necessary information for representing a token can be derived from a weighted sum of all other tokens in the sequence. This lack of a strong prior is both a strength and a weakness. It makes the Transformer incredibly flexible and general-purpose, but it also means it requires enormous amounts of data to learn these relationships from scratch.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**4. Self-Attention vs. Cross-Attention:**\\n",
    "Cross-attention is the key mechanism for connecting the encoder and the decoder. In a translation task, for example, the decoder has generated the first few words of the French translation (which are fed into its masked self-attention layer). To generate the *next* French word, it needs to know which English words in the source sentence are most relevant. Cross-attention allows the decoder (via its Query vectors) to attend to the encoder's output (the Keys and Values representing the full English sentence), effectively asking, \"Given the French words I've generated so far, which English words should I focus on now?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
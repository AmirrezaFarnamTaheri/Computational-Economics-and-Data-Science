{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# === Environment Setup ===\n",
        "import os, sys, math, time, random, json, textwrap, warnings, re\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "try:\n",
        "    import graphviz\n",
        "    GRAPHVIZ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GRAPHVIZ_AVAILABLE = False\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    def download_nltk_data():\n",
        "        resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']\n",
        "        for resource in resources:\n",
        "            try: nltk.data.find(f'tokenizers/{resource}' if resource=='punkt' else f'corpora/{resource}')\n",
        "            except LookupError: nltk.download(resource, quiet=True)\n",
        "    download_nltk_data()\n",
        "    NLTK_AVAILABLE = True\n",
        "except (ImportError, LookupError): NLTK_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import gensim\n",
        "    import gensim.downloader as api\n",
        "    GENSIM_AVAILABLE = True\n",
        "except ImportError: GENSIM_AVAILABLE = False\n",
        "\n",
        "# --- Configuration ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
        "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
        "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
        "\n",
        "note(\"Environment initialized for Advanced Natural Language Processing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 7: Advanced Deep Learning & Frontier Topics\n",
        "## Chapter 7.10: Natural Language Processing for Economics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction: Text as Data\n",
        "\n",
        "This chapter provides a PhD-level introduction to **Natural Language Processing (NLP)**. A vast amount of economic information is stored as unstructured text\u2014news articles, central bank statements, corporate 10-K filings, political speeches, and social media posts. NLP provides a powerful suite of tools for converting this text into quantitative data suitable for rigorous economic analysis.\n",
        "\n",
        "This chapter covers the entire pipeline, from classical methods to the state-of-the-art:\n",
        "1.  **Text Preprocessing:** The essential first step of cleaning and standardizing text.\n",
        "2.  **Vector Space Models:** Representing documents as numerical vectors using TF-IDF.\n",
        "3.  **Topic Modeling:** Unsupervised discovery of latent themes using Latent Dirichlet Allocation (LDA).\n",
        "4.  **Word Embeddings:** Learning dense, semantic vector representations of words with Word2Vec.\n",
        "5.  **Contextualized Embeddings:** A brief introduction to the Transformer architecture and its role in modern NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The NLP Pipeline: From Raw Text to Numerical Data\n",
        "\n",
        "### 1.1 Preprocessing\n",
        "Before any analysis, raw text must be cleaned and standardized. The goal is to reduce the vocabulary to only the most meaningful terms.\n",
        "\n",
        "1.  **Normalization:** Convert text to a consistent case (e.g., lowercase) and remove punctuation, numbers, and special characters.\n",
        "2.  **Tokenization:** Split the text into individual words or \"tokens\".\n",
        "3.  **Stop-Word Removal:** Remove extremely common words (e.g., \"the\", \"a\", \"is\") that carry little semantic weight.\n",
        "4.  **Lemmatization:** Reduce words to their dictionary root form, or \"lemma\" (e.g., \"rates\", \"rating\", \"rated\" all become \"rate\"). This is more linguistically informed than **stemming**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sec(\"Text Preprocessing Demonstration\")\n",
        "\n",
        "if NLTK_AVAILABLE:\n",
        "    raw_text = \"The U.S. economy showed robust growth in the 3rd quarter, with GDP increasing by 4.9%.\"\n",
        "    lemmatizer = WordNetLemmatizer(); stop_words = set(stopwords.words('english'))\n",
        "    def preprocess_text(text):\n",
        "        text = re.sub(r'[^a-zA-Z\\\\s]', '', text.lower())\n",
        "        tokens = word_tokenize(text)\n",
        "        return [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n",
        "    note(f\"**Original:** {raw_text}\\\\n**Processed:** {preprocess_text(raw_text)}\")\n",
        "else:\n",
        "    note(\"NLTK not available. Skipping preprocessing demonstration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Vectorization: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "**Intellectual Provenance:** The concept of TF-IDF was developed in the early 1970s by Karen Sp\u00e4rck Jones, a British computer scientist. Her work laid the foundation for modern information retrieval and search engines. The core idea was to find a way to automatically determine which terms in a document are most representative of its content, allowing for more relevant search results.\n",
        "\n",
        "**TF-IDF** represents each document as a vector where each entry reflects the importance of a word in that document relative to the entire corpus.\n",
        "$$ \\text{tfidf}(t, d) = \\underbrace{tf(t, d)}_{\\text{Term Frequency}} \\times \\underbrace{\\text{idf}(t)}_{\\text{Inverse Document Frequency}} $$\n",
        "*   **Term Frequency (TF):** How often a term appears in a document.\n",
        "*   **Inverse Document Frequency (IDF):** A measure of a term's informativeness. A word that appears in many documents (like \"economy\" in financial news) gets a low IDF score, while a rare word gets a high score. The product gives higher weight to words that are frequent in a specific document but rare across the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Unsupervised Methods: Topic Modeling\n",
        "\n",
        "**Intellectual Provenance:** LDA was introduced in a seminal 2003 paper by David Blei, Andrew Ng, and Michael I. Jordan. It revolutionized the field of topic modeling by providing a probabilistic framework for discovering the thematic structure of large text corpora. It is a generalization of earlier models and has become a standard tool in the digital humanities, social sciences, and economics for analyzing large collections of documents.\n",
        "\n",
        "**Latent Dirichlet Allocation (LDA)** is an unsupervised generative statistical model that treats documents as mixtures of topics, and topics as distributions over words. The model uses the observed documents to infer the latent topic structure that most likely generated them. The number of topics, $k$, is a hyperparameter that must be chosen by the researcher, often by finding the $k$ that maximizes a **coherence score**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sec(\"Text Preprocessing Demonstration\")\n",
        "\n",
        "if NLTK_AVAILABLE:\n",
        "    raw_text = \"The U.S. economy showed robust growth in the 3rd quarter, with GDP increasing by 4.9%.\"\n",
        "    lemmatizer = WordNetLemmatizer(); stop_words = set(stopwords.words('english'))\n",
        "    def preprocess_text(text):\n",
        "        text = re.sub(r'[^a-zA-Z\\\\s]', '', text.lower())\n",
        "        tokens = word_tokenize(text)\n",
        "        return [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n",
        "    note(f\"**Original:** {raw_text}\\\\n**Processed:** {preprocess_text(raw_text)}\")\n",
        "else:\n",
        "    note(\"NLTK not available. Skipping preprocessing demonstration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Modern NLP: Word Embeddings\n",
        "\n",
        "While TF-IDF is powerful, it treats words as independent entities, missing semantic relationships. Modern NLP uses **word embeddings** to represent words as dense, low-dimensional vectors in a way that captures these relationships.\n",
        "\n",
        "The core idea of algorithms like **Word2Vec** and **GloVe** is based on the **distributional hypothesis**: words that frequently appear in similar linguistic contexts tend to have similar meanings. These algorithms learn a vector for each word such that the geometric relationships between vectors mirror the semantic relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Word2Vec Architectures: CBOW and Skip-gram\n",
        "\n",
        "Word2Vec learns embeddings by training a shallow neural network on a \"fake\" prediction task. \n",
        "1.  **Continuous Bag-of-Words (CBOW):** Predicts a target word from its context words.\n",
        "2.  **Skip-gram:** Predicts context words from a single target word. Skip-gram is typically slower but performs better for infrequent words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![CBOW Architecture](../images/07-Machine-Learning/cbow_architecture.png)\\n",
        "![Skip-Gram Architecture](../images/07-Machine-Learning/skip_gram_architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sec(\"Training and Visualizing Word Embeddings\")\n",
        "\n",
        "if GENSIM_AVAILABLE and NLTK_AVAILABLE:\n",
        "    note(\"Loading pre-trained GloVe embeddings... (This may take a moment on first run)\")\n",
        "    try:\n",
        "        # 1. Load a pre-trained model.\n",
        "        glove_vectors = api.load('glove-wiki-gigaword-100')\n",
        "        note(\"GloVe model loaded successfully.\")\n",
        "        \n",
        "        # 2. Explore Semantic Similarities\n",
        "        note(\"Words most similar to 'finance':\")\n",
        "        display(pd.DataFrame(glove_vectors.most_similar('finance'), columns=['Word', 'Similarity']))\n",
        "        \n",
        "        # 3. Perform Semantic Arithmetic\n",
        "        note(\"Semantic Arithmetic: king - man + woman = ?\")\n",
        "        display(pd.DataFrame(glove_vectors.most_similar(positive=['king', 'woman'], negative=['man']), columns=['Word', 'Similarity']))\n",
        "\n",
        "    except Exception as e:\n",
        "        note(f\"Could not download pre-trained model. Error: {e}\")\n",
        "else:\n",
        "    note(\"Gensim or NLTK not available. Skipping Word2Vec example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The Frontier: Transformers and Contextualized Embeddings\n",
        "\n",
        "Word embeddings like Word2Vec are **static**: the vector for \"bank\" is the same regardless of whether it appears in \"river bank\" or \"central bank\". The state-of-the-art in NLP is dominated by **contextualized embeddings** generated by models with a **Transformer architecture**, such as BERT, RoBERTa, and the GPT family.\n",
        "\n",
        "These **Large Language Models (LLMs)** are pre-trained on a massive corpus of text using self-supervised objectives. The key innovation is the **self-attention mechanism**, which allows the model to \"look\" at all other words in the sentence and assign a weight to each one, representing its importance for understanding the target word *in its specific context*. This allows the model to generate a different vector for \"bank\" depending on the surrounding words.\n",
        "\n",
        "These pre-trained models can then be **fine-tuned** for specific downstream tasks like sentiment analysis, document classification, or question answering, achieving state-of-the-art performance with relatively little task-specific data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
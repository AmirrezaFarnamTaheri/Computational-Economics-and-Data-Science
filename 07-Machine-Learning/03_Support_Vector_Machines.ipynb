{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*\"=\"}')\n",
    "\n",
    "note(\"Environment initialized for Support Vector Machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.3: Support Vector Machines (SVMs)\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**The Geometric Intuition: Maximal Margin Classifier**](#intro)\n",
    "2.  [**Support Vectors and the Soft Margin**](#soft-margin)\n",
    "3.  [**The Kernel Trick for Non-linear Data**](#kernel-trick)\n",
    "4.  [**Code Lab: SVM for Classification**](#code-lab)\n",
    "5.  [**When to Use SVMs**](#when-to-use)\n",
    "6.  [**Summary**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. The Geometric Intuition: Maximal Margin Classifier\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful and elegant class of supervised learning algorithms. At their core, they are based on a simple geometric idea. For a linearly separable dataset, there can be infinitely many hyperplanes (lines in 2D, planes in 3D, etc.) that separate the classes.\n",
    "\n",
    "> **Historical Context: The SVM**\n",
    "> The Support Vector Machine was first introduced by Vapnik and Chervonenkis in 1963. The algorithm was further developed by Vapnik and his colleagues at AT&T Bell Laboratories in the 1990s. The SVM is a powerful tool for classification and regression, and it is one of the most widely used machine learning algorithms today.\n",
    "\n",
    "The question is: which hyperplane is best? The SVM answers this by choosing the hyperplane that **maximizes the margin** between the classes. The margin is defined as the distance between the separating hyperplane and the closest data points from either class. This maximal margin hyperplane is considered optimal because it is the most robust to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='../images/07-Machine-Learning/svm_hyperplanes.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='soft-margin'></a>\n",
    "## 2. Support Vectors and the Soft Margin\n",
    "\n",
    "The data points that lie exactly on the margin are called **support vectors**. These are the most critical points in the dataset because they alone define the position and orientation of the maximal margin hyperplane. If any of these points were moved, the hyperplane would change. All other points are irrelevant.\n",
    "\n",
    "In most real-world scenarios, data is not perfectly linearly separable. To handle this, the SVM algorithm introduces the concept of a **soft margin**. This allows for some misclassifications or points to fall within the margin. The trade-off between maximizing the margin and minimizing the classification error is controlled by a hyperparameter, often denoted as `C`. A smaller `C` creates a wider margin but allows more violations, while a larger `C` creates a narrower margin with fewer violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='../images/07-Machine-Learning/svm_margin.png'))"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel-trick'></a>\n",
    "## 3. The Kernel Trick for Non-linear Data\n",
    "\n",
    "The true power of SVMs is revealed when dealing with non-linearly separable data. The key idea is to project the data into a higher-dimensional space where it becomes linearly separable. This is done using a **kernel function**.\n",
    "\n",
    "The **kernel trick** is a mathematical shortcut that allows us to operate in this high-dimensional space without ever having to compute the coordinates of the data in that space. We only need to compute the dot products between the images of the data points in the feature space, which is much more efficient.\n",
    "\n",
    "Common kernels include:\n",
    "- **Polynomial Kernel**: Captures polynomial relationships in the data.\n",
    "- **Radial Basis Function (RBF) Kernel**: Can handle complex, non-linear relationships. It is the most commonly used kernel.\n",
    "- **Sigmoid Kernel**: Similar to the activation function in neural networks."
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sec(\"Visualizing the Kernel Trick\")\n",
    "# Generate concentric circles\n",
    "X, y = make_circles(n_samples=500, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "# Add a third dimension: z = x^2 + y^2\n",
    "z = X[:, 0]**2 + X[:, 1]**2\n",
    "X_3d = np.c_[X, z]\n",
    "\n",
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the two classes\n",
    "ax.scatter(X_3d[y==0, 0], X_3d[y==0, 1], X_3d[y==0, 2], c='blue', marker='o', label='Class 0')\n",
    "ax.scatter(X_3d[y==1, 0], X_3d[y==1, 1], X_3d[y==1, 2], c='red', marker='^', label='Class 1')\n",
    "\n",
    "# Create a separating plane (for visualization)\n",
    "xx, yy = np.meshgrid(np.linspace(-1, 1, 50), np.linspace(-1, 1, 50))\n",
    "zz = np.ones_like(xx) * 0.8 # A simple plane at z=0.8\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.2, color='gray')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z = X^2 + Y^2')\n",
    "ax.set_title('Data Transformed to 3D Space')\n",
    "ax.legend()\n",
    "ax.view_init(elev=20, azim=45)\n",
    "if not os.path.exists('../images/07-Machine-Learning'):\n",
    "    os.makedirs('../images/07-Machine-Learning')\n",
    "plt.savefig('../images/07-Machine-Learning/kernel_trick.png')\n",
    "plt.close()\n",
    "display(Image(filename='../images/07-Machine-Learning/kernel_trick.png'))\n",
    "\n",
    "note(\"In the transformed 3D space, the classes become linearly separable by a plane.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-lab'></a>\n",
    "## 4. Code Lab: SVM for Classification\n",
    "\n",
    "Let's generate some non-linearly separable data and see how an SVM with an RBF kernel can effectively classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"SVM with RBF Kernel\")\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_blobs(n_samples=200, centers=2, random_state=6, cluster_std=1.1)\n",
    "\n",
    "# Add some noise to make it more challenging\n",
    "rng = np.random.RandomState(13)\n",
    "X_noise = rng.randn(200, 2) * 2\n",
    "X = X + X_noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# We instantiate the SVM classifier, specifying the RBF kernel and the regularization parameter C.\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "# We then fit the model to the training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# We can then use the trained model to make predictions on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "# We evaluate the model's performance using a classification report.\n",
    "note(\"Classification Report for SVM with RBF Kernel:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when-to-use'></a>\n",
    "## 5. When to Use SVMs\n",
    "\n",
    "SVMs are particularly effective in:\n",
    "- **High-dimensional spaces**: They work well even when the number of dimensions exceeds the number of samples.\n",
    "- **Cases where a clear margin of separation is desirable**.\n",
    "- **Memory efficiency**: They use a subset of training points (the support vectors) in the decision function.\n",
    "\n",
    "They are less effective on very large datasets, as the training time complexity can be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 6. Summary\n",
    "\n",
    "Support Vector Machines are a robust and versatile class of models. They offer a powerful, geometrically-motivated approach to classification. By leveraging the kernel trick, they can efficiently model complex, non-linear relationships. Their strength lies in finding the optimal decision boundary, making them a valuable tool for many classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
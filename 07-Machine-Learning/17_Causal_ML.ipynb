{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "try:\n",
    "    import doubleml as dml\n",
    "    DOUBLEML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DOUBLEML_AVAILABLE = False\n",
    "try:\n",
    "    from econml.dml import CausalForestDML\n",
    "    ECONML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECONML_AVAILABLE = False\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (11, 7), 'figure.dpi': 130})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "if not DOUBLEML_AVAILABLE: \n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'doubleml'])\n",
    "    note(\"The 'doubleml' library is not installed (`pip install DoubleML`). Some sections will be skipped.\")\n",
    "if not ECONML_AVAILABLE: \n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'econml'])\n",
    "    note(\"The 'econml' library is not installed (`pip install econml`). Some sections will be skipped.\")\n",
    "note(f\"Environment initialized. DoubleML available: {DOUBLEML_AVAILABLE}, EconML available: {ECONML_AVAILABLE}\")"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Machine Learning: A Practical Guide\n",
    "\n",
    "---\n",
    "\n",
    "### On this page\n",
    "\n",
    "1.  [**Introduction: Prediction Serving Inference**](#intro)\n",
    "2.  [**The Architects of Causal ML**](#architects)\n",
    "3.  [**The Core Idea: Orthogonalization and the FWL Theorem**](#fwl)\n",
    "4.  [**Double/Debiased Machine Learning (DML)**](#dml)\n",
    "    - [The Problem: Regularization Bias](#bias)\n",
    "    - [The Solution: Neyman-Orthogonality](#orthogonality)\n",
    "    - [Code Lab: DML with the `DoubleML` Package](#code-dml)\n",
    "5.  [**Causal Forests for Heterogeneous Treatment Effects**](#causal-forest)\n",
    "    - [Code Lab: Causal Forests with `EconML`](#code-cf)\n",
    "6.  [**Case Study: Estimating Price Elasticity of Demand**](#casestudy)\n",
    "7.  [**Test your knowledge**](#exercises)\n",
    "8.  [**Key Takeaways**](#summary)\n",
    "9.  [**Further reading**](#meta-learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: Prediction in Service of Inference\n",
    "\n",
    "---",
    "***Building on Foundations:***",
    "*This chapter introduces advanced methods for causal inference. It assumes familiarity with the foundational concepts of causality—the potential outcomes framework and the problem of confounding—as discussed in **Chapter 6.4: Causal Inference: The Quest for 'Why'**. The methods presented here are powerful solutions to the identification problem when the set of confounding variables is large and their relationship with the treatment and outcome may be complex and non-linear.*",
    "\n",
    "This chapter explores the intersection of the two \"cultures\" of statistical modeling: the **causal inference** focus of traditional econometrics and the **predictive power** of modern machine learning. **Causal Machine Learning** is a field that leverages flexible ML algorithms to obtain more credible estimates of causal effects, particularly in high-dimensional settings where classical econometric methods can be fragile.\n",
    "\n",
    "The central idea is to use machine learning for what it excels at—flexible, high-dimensional prediction—in service of the ultimate goal of econometrics: estimating a specific causal parameter. We use ML to flexibly model the **\"nuisance components\"** of a causal model, such as the complex, non-linear relationships between a large set of confounding variables ($X$) and the treatment ($T$) or outcome ($Y$). By delegating this difficult prediction task to machine learning, we can isolate the causal parameter of interest and estimate it robustly, without imposing strong, and likely incorrect, functional form assumptions (like simple linearity) on the confounding relationships.\n",
    "\n",
    "This notebook introduces two cornerstone methods:\n",
    "1.  **Double/Debiased Machine Learning (DML):** A general framework for estimating an average treatment effect (ATE) in the presence of high-dimensional confounding, using any predictive ML model.\n",
    "2.  **Causal Forests:** An adaptation of the Random Forest algorithm designed to estimate heterogeneous treatment effects (CATEs) and uncover how causal effects vary across a population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architects'></a>\n",
    "## 2. The Intellectual Architects of Causal ML\n",
    "\n",
    "The methods in this chapter are not the product of a single mind, but the culmination of parallel lines of research from economists and statisticians aimed at bridging the two modeling cultures. The development can be largely credited to two groups of influential thinkers.\n",
    "\n",
    "On one hand, **Victor Chernozhukov** (MIT) and his many coauthors developed the rigorous theory behind **Double/Debiased Machine Learning (DML)**. Their work provided a general blueprint for using virtually any machine learning model—be it LASSO, a random forest, or a neural network—to flexibly control for confounding variables. Their crucial insight was to combine the classical idea of **Neyman-orthogonalization** with modern **cross-fitting** procedures. This combination ensures that the final causal estimate is not contaminated by the biases that plague ML models (like regularization or overfitting bias), making the final estimate robust to minor imperfections in the predictive models.\n",
    "\n",
    "Simultaneously, **Susan Athey** (Stanford) and **Guido Imbens** (Stanford), who shared the 2021 Nobel Prize in Economics for his foundational work on causal inference, pioneered the development of **Causal Forests**. They ingeniously adapted the standard Random Forest algorithm, a powerhouse of predictive modeling, for the specific task of causal estimation. Instead of growing trees to minimize prediction error, their algorithm grows trees that explicitly maximize the difference in treatment effects across leaves. This innovation shifted the focus from merely predicting outcomes to understanding *how* and *why* treatment effects vary across a population. Their work has been instrumental in moving the field beyond simple average effects and toward a more nuanced understanding of treatment effect heterogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fwl'></a>\n",
    "## 3. The Core Idea: Orthogonalization and the Frisch-Waugh-Lovell Theorem\n",
    "\n",
    "The theoretical key that unlocks Causal ML is **orthogonalization**, an idea with deep roots in econometrics, crystallized in the **Frisch-Waugh-Lovell (FWL) theorem**. The theorem provides a powerful insight into what a multivariate regression is actually doing. It states that in a linear model $Y = \\beta_0 + \\tau D + \\gamma' X + u$, the coefficient $\\tau$ on the variable of interest ($D$) can be estimated via a simple three-step procedure:\n",
    "\n",
    "1.  **\"Partial out\" X from Y:** Regress the outcome $Y$ on the controls $X$ and obtain the residuals, $\\tilde{Y} = Y - \\hat{E}[Y|X]$. This residual represents the variation in $Y$ that is *orthogonal to* (i.e., unexplained by) the control variables.\n",
    "2.  **\"Partial out\" X from D:** Regress the treatment $D$ on the same controls $X$ and obtain the residuals, $\\tilde{D} = D - \\hat{E}[D|X]$. This residual represents the variation in the treatment that is *as-good-as-random* after accounting for the influence of $X$.\n",
    "3.  **Regress Residuals on Residuals:** Regress the outcome residuals $\\tilde{Y}$ on the treatment residuals $\\tilde{D}$. The sole coefficient from this final, simple regression is numerically identical to the estimate of $\\tau$ from the original, complex multivariate regression.\n",
    "\n",
    "This process demonstrates that the estimate for $\\tau$ is based only on the parts of $Y$ and $D$ that are orthogonal to $X$. DML ingeniously builds on this by replacing the simple OLS in steps 1 and 2 with flexible machine learning models, allowing us to partial out complex, non-linear relationships that are captured by $\\hat{E}[Y|X]$ and $\\hat{E}[D|X]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing the Frisch-Waugh-Lovell Theorem\")\n",
    "rng = np.random.default_rng(42)\n",
    "n = 100\n",
    "X = rng.uniform(0, 10, n)\n",
    "D = 0.5 * X + rng.normal(0, 1, n)\n",
    "y = 2 * D + 1.5 * X + rng.normal(0, 2, n)\n",
    "X_full = np.c_[np.ones(n), D, X]\n",
    "full_model_coeffs = np.linalg.lstsq(X_full, y, rcond=None)[0]\n",
    "tau_full = full_model_coeffs[1]\n",
    "X_c = np.c_[np.ones(n), X]\n",
    "y_resid = y - X_c @ np.linalg.lstsq(X_c, y, rcond=None)[0]\n",
    "D_resid = D - X_c @ np.linalg.lstsq(X_c, D, rcond=None)[0]\n",
    "D_resid_c = np.c_[np.ones(n), D_resid]\n",
    "resid_model_coeffs = np.linalg.lstsq(D_resid_c, y_resid, rcond=None)[0]\n",
    "tau_resid = resid_model_coeffs[1]\n",
    "note(f\"Full regression estimate of τ: {tau_full:.4f}\")\n",
    "note(f\"Residual regression estimate of τ: {tau_resid:.4f}\")\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "fig.suptitle('Figure 1: The Frisch-Waugh-Lovell Theorem', fontsize=18, y=1.02)\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(D, X, y, label='Data', alpha=0.7)\n",
    "xx, dd = np.meshgrid(np.linspace(X.min(), X.max(), 10), np.linspace(D.min(), D.max(), 10))\n",
    "yy = full_model_coeffs[0] + full_model_coeffs[1]*dd + full_model_coeffs[2]*xx\n",
    "ax1.plot_surface(dd, xx, yy, alpha=0.4, color='orange')\n",
    "ax1.set(xlabel='Treatment D', ylabel='Confounder X', zlabel='Outcome Y', title='a) Full Regression in 3D Space')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.scatter(D_resid, y_resid, label='Residuals', alpha=0.7)\n",
    "ax2.plot(D_resid, resid_model_coeffs[0] + resid_model_coeffs[1]*D_resid, color='red', lw=2.5, label=f'Slope = {tau_resid:.2f}')\n",
    "ax2.set(xlabel='Residualized Treatment (D orthogonal to X)', ylabel='Residualized Outcome (Y orthogonal to X)', title='b) Regression on Residuals (FWL)')\n",
    "ax2.legend()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dml'></a>\n",
    "## 4. Double/Debiased Machine Learning (DML)\n",
    "\n",
    "<a id='bias'></a>\n",
    "### 4.1 The Problem: Regularization Bias\n",
    "\n",
    "Consider the partially linear causal model:\n",
    "$$ Y = \\tau D + g(X) + u, \\quad E[u|D,X]=0 $$ \n",
    "We want to estimate the causal effect $\\tau$ of a treatment $D$ on an outcome $Y$, controlling for a potentially high-dimensional vector of confounders $X$. We do not know the true functional form of $g(X)$ or the relationship between $D$ and $X$, $m(X)=E[D|X]$.\n",
    "\n",
    "A naive approach might be to use a flexible ML model (e.g., LASSO) to regress Y on D and X. However, this will produce a biased estimate of $\\tau$. LASSO's objective is to minimize prediction error, and to do so, it shrinks coefficients toward zero. If there is any remaining correlation between the regularized features and our un-penalized variable of interest $D$, this shrinkage will induce omitted variable bias in the estimate of $\\tau$. In essence, by imperfectly controlling for $X$, LASSO makes $D$ partially correlated with the error term.\n",
    "\n",
    "<a id='orthogonality'></a>\n",
    "### 4.2 The Solution: Neyman-Orthogonality\n",
    "\n",
    "The DML algorithm (Chernozhukov et al., 2018) solves this problem by constructing a **Neyman-orthogonal moment equation** to estimate $\\tau$. A moment equation is Neyman-orthogonal if its derivative with respect to the nuisance functions, evaluated at the true values, is zero. In simpler terms, this means that if our ML models for the nuisance functions ($g(X)$ and $m(X)$) are close to the truth, small errors in those models do not, to a first order, affect our final estimate of the causal parameter $\\tau$. This makes the estimate robust to moderate prediction errors by the ML models.\n",
    "\n",
    "For the partially linear model, the orthogonal moment equation is:\n",
    "$$ E[(Y - g(X)) - \\tau(D - m(X))] = 0 $$\n",
    "This is precisely what the Frisch-Waugh-Lovell procedure achieves. By residualizing *both* the outcome and the treatment, we create an estimating equation that is robust to small errors in our estimates of $g(X)$ and $m(X)$. This is combined with **cross-fitting** (or sample-splitting) to prevent a different kind of bias: **overfitting bias**. Cross-fitting ensures that the predictions for $g(X_i)$ and $m(X_i)$ for any observation $i$ are generated from a model that was not trained on observation $i$ itself."
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-dml'></a>\n",
    "### 4.3 Code Lab: DML with the `DoubleML` Package\n",
    "\n",
    "While implementing DML from scratch is useful for understanding the algorithm, in practice we use specialized libraries like `DoubleML`. These libraries provide a high-level API, handle the cross-fitting automatically, and compute correct standard errors for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"DML using the DoubleML Package\")\n",
    "\n",
    "if DOUBLEML_AVAILABLE:\n",
    "    rng = np.random.default_rng(123)\n",
    "    n, k, p = 500, 100, 10\n",
    "    X = rng.normal(size=(n, k))\n",
    "    beta_D = rng.uniform(-1, 1, p)\n",
    "    D = X[:, :p] @ beta_D + rng.normal(size=n)\n",
    "    beta_y = rng.uniform(-1, 1, p)\n",
    "    TRUE_EFFECT = 2.0\n",
    "    y = D * TRUE_EFFECT + X[:, :p] @ beta_y + rng.normal(size=n)\n",
    "    \n",
    "    dml_data = dml.DoubleMLData.from_arrays(X, y, D)\n",
    "    ml_g = Lasso(alpha=0.1)\n",
    "    ml_m = Lasso(alpha=0.1)\n",
    "    dml_plr_obj = dml.DoubleMLPLR(dml_data, ml_g, ml_m)\n",
    "    dml_plr_obj.fit()\n",
    "    print(dml_plr_obj.summary)\n",
    "else:\n",
    "    note(\"DoubleML not installed. Skipping package example. Run `pip install DoubleML` to use it.\")"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='causal-forest'></a>\n",
    "## 5. Causal Forests for Heterogeneous Treatment Effects\n",
    "\n",
    "While DML is powerful for estimating an *average* treatment effect (ATE), we are often interested in a more nuanced question: **for whom** is a treatment most effective? **Causal Forests**, developed by Susan Athey and Guido Imbens, adapt the Random Forest algorithm to estimate these **Conditional Average Treatment Effects (CATEs)**, $\\tau(x) = E[Y(1) - Y(0) | X=x]$.\n",
    "\n",
    "A standard Random Forest builds trees by creating splits that minimize prediction error (e.g., Mean Squared Error). A Causal Forest modifies this criterion. It creates splits that **maximize the heterogeneity in the treatment effect** between the resulting child nodes. In essence, it actively searches for subgroups of the population (defined by their characteristics $X$) that have systematically different responses to the treatment.\n",
    "\n",
    "#### The Honest, Orthogonal Forest\n",
    "\n",
    "The key innovations that make Causal Forests work are **orthogonality** and **honesty**.\n",
    "\n",
    "1.  **Orthogonality (via DML):** Just like in the DML section, the forest first uses nuisance models to partial out the effects of confounders. The `CausalForestDML` implementation in `EconML` first estimates $\\hat{g}(X) = E[Y|X]$ and $\\hat{m}(X) = E[T|X]$ and then builds the forest on the residuals $\\tilde{Y} = Y - \\hat{g}(X)$ and $\\tilde{T} = T - \\hat{m}(X)$. This ensures the underlying treatment effect estimation is robust to confounding.\n",
    "\n",
    "2.  **Honesty (via Sample Splitting):** A standard regression tree uses the same data to both determine the splits and estimate the value in the resulting leaves. This leads to an overfitting problem: the tree will find splits that look good for that specific sample's noise, leading to biased estimates. An **honest** tree avoids this by splitting the data. One half of the data (the \"splitting sample\") is used to decide the structure of the tree (where to make the splits). The other half (the \"estimating sample\") is then dropped down the tree, and the estimates are calculated using only these observations. This separation ensures that the estimates within the leaves are not biased by the search for the best splits.\n",
    "\n",
    "A Causal Forest is simply an average of many such honest, orthogonal trees, which provides a smooth and powerful estimate of the CATE function $\\tau(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Causal Tree splits the data to maximize the difference in treatment effects between the children nodes. The diagram below illustrates this logic: a split on the 'Age' variable creates two distinct subgroups with very different estimated treatment effects.\n",
    "\n",
    "![Causal Tree Splitting Logic](../images/07-Machine-Learning/figure2_causal_tree_split_1.png)\n",
    "*Figure 2: Causal vs. Regression Tree Splitting Logic. The algorithm seeks splits that create the largest possible difference between the estimated treatment effect in the child nodes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-cf'></a>\n",
    "### 5.1 Code Lab: Causal Forests with `EconML`\n",
    "\n",
    "The `EconML` library from Microsoft Research provides a powerful implementation of Causal Forests and other CATE estimators. The `CausalForestDML` class combines the DML approach for orthogonalization (to handle confounding) with the Causal Forest algorithm for modeling effect heterogeneity.\n",
    "\n",
    "**The Data Generating Process (DGP):**\n",
    "We will simulate data where the true treatment effect is heterogeneous and depends linearly on the first feature, $X_0$. Specifically, the true CATE is $\\tau(X) = 1 + 0.5 X_0$. The outcome $Y$ is determined by this heterogeneous effect plus some confounding from another feature, $X_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Causal Forest for Heterogeneous Effects\")\n",
    "\n",
    "if ECONML_AVAILABLE:\n",
    "    # 1. Data Generation\n",
    "    note(\"Simulating data where the true treatment effect is heterogeneous and driven by the first feature, X[0].\")\n",
    "    rng = np.random.default_rng(123)\n",
    "    n, k = 2000, 10 # 2000 observations, 10 features\n",
    "    feature_names = [f'X{i}' for i in range(k)]\n",
    "    X = rng.normal(size=(n, k))\n",
    "    T = rng.binomial(1, 0.5, n) # Binary treatment\n",
    "    true_cate = 1 + 0.5 * X[:, 0] # True CATE is a function of the first feature\n",
    "    y = true_cate * T + X[:, 1] * 0.2 + rng.normal(size=n) # Outcome depends on CATE, treatment, and a confounder\n",
    "\n",
    "    # 2. Model Estimation\n",
    "    note(\"Estimating the Causal Forest. This involves training nuisance models for E[Y|X] and E[T|X] and then fitting the forest on the residuals.\")\n",
    "    est = CausalForestDML(\n",
    "        model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=42),\n",
    "        model_t=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=42),\n",
    "        discrete_treatment=True, n_estimators=400, random_state=42\n",
    "    )\n",
    "    # The `fit` method estimates the CATE model.\n",
    "    # The `const_marginal_effect_inference` method provides valid confidence intervals.\n",
    "    est.fit(y, T, X=X, W=None) # W contains controls that don't affect heterogeneity\n",
    "    \n",
    "    # 3. Effect and Inference\n",
    "    note(\"Calculating CATE estimates and their 95% confidence intervals for each observation.\")\n",
    "    estimated_cates = est.effect(X)\n",
    "    lb, ub = est.effect_interval(X, alpha=0.05)\n",
    "    \n",
    "    # 4. Visualization and Analysis\n",
    "    fig = plt.figure(figsize=(18, 12), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    fig.suptitle('Figure 3: Comprehensive Causal Forest Analysis', fontsize=20, y=1.03)\n",
    "\n",
    "    # Plot 1: Distribution of Estimated vs. True CATEs\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    sns.histplot(estimated_cates, bins=30, ax=ax1, stat=\"density\", label='Estimated CATEs')\n",
    "    sns.kdeplot(true_cate, ax=ax1, color='red', lw=2.5, label='True CATEs')\n",
    "    ax1.set_title('a) Distribution of Estimated vs. True CATEs')\n",
    "    ax1.set_xlabel('Treatment Effect'); ax1.legend()\n",
    "\n",
    "    # Plot 2: CATE vs. Heterogeneity-Driving Feature\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    sns.scatterplot(x=X[:, 0], y=estimated_cates, alpha=0.2, ax=ax2, label='Estimated CATE')\n",
    "    sns.lineplot(x=X[:, 0], y=true_cate, color='red', lw=3, ax=ax2, label='True CATE Function')\n",
    "    ax2.set_title('b) CATE vs. Heterogeneity-Driving Feature (X[0])')\n",
    "    ax2.set(xlabel='Value of Feature X[0]', ylabel='Treatment Effect'); ax2.legend()\n",
    "\n",
    "    # Plot 3: CATEs with Confidence Intervals\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    sorted_indices = np.argsort(X[:, 0])\n",
    "    ax3.plot(X[sorted_indices, 0], true_cate[sorted_indices], color='red', lw=3, label='True CATE')\n",
    "    ax3.plot(X[sorted_indices, 0], estimated_cates[sorted_indices], color='C0', lw=2, label='Estimated CATE')\n",
    "    ax3.fill_between(X[sorted_indices, 0], lb[sorted_indices], ub[sorted_indices], color='C0', alpha=0.2, label='95% CI')\n",
    "    ax3.set_title('c) CATE Estimates with 95% Confidence Intervals')\n",
    "    ax3.set(xlabel='Value of Feature X[0]', ylabel='Treatment Effect'); ax3.legend()\n",
    "    \n",
    "    # Plot 4: Feature Importances for Heterogeneity\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    importances = est.feature_importances_\n",
    "    importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df.head(5), ax=ax4, color='C2')\n",
    "    ax4.set_title('d) Top 5 Features Driving Effect Heterogeneity')\n",
    "    ax4.set_xlabel('Importance Score'); ax4.set_ylabel('Feature')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"EconML not installed. Skipping Causal Forest example. Run `pip install econml` to use it.\")"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='casestudy'></a>\n",
    "## 6. Case Study: Estimating Price Elasticity of Demand\n",
    "\\n",
    "A classic problem in microeconomics is estimating the price elasticity of demand. A major challenge is **endogeneity**: price is not set randomly. It often responds to demand shocks (e.g., promotions, holidays, local events) that also affect the quantity sold. A naive regression of quantity on price will produce a biased estimate of the true elasticity.\\n",
    "\\n",
    "DML is perfectly suited to this problem. We can treat price as the \"treatment\" variable ($D$) and quantity as the outcome ($Y$). We can then use a rich set of control variables ($X$)—such as day of the week, season, and promotional activity—to flexibly model the nuisance functions $E[Y|X]$ and $E[D|X]$. By partialling out the predictable components of price and quantity, DML can isolate the causal effect of the residual, unpredictable price changes on residual quantity demanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Case Study: DML for Price Elasticity Estimation\")\n",
    "\n",
    "if DOUBLEML_AVAILABLE:\n",
    "    rng = np.random.default_rng(456)\n",
    "    n_stores, n_weeks = 50, 104\n",
    "    n_obs = n_stores * n_weeks\n",
    "    \n",
    "    week = np.tile(np.arange(n_weeks), n_stores)\n",
    "    seasonality = np.sin(2 * np.pi * week / 52) + np.cos(2 * np.pi * week / 52)\n",
    "    promotion = rng.binomial(1, 0.1, n_obs)\n",
    "    X = np.c_[seasonality, promotion]\n",
    "    \n",
    "    price = 10 - 0.5 * seasonality - 1.2 * promotion + rng.normal(0, 0.5, n_obs)\n",
    "    \n",
    "    TRUE_ELASTICITY = -2.5\n",
    "    log_quantity = 10 + TRUE_ELASTICITY * np.log(price) + 0.8 * seasonality + 2.0 * promotion + rng.normal(0, 0.5, n_obs)\n",
    "    \n",
    "    naive_ols = sm.OLS(log_quantity, sm.add_constant(np.log(price))).fit()\n",
    "    note(f\"The naive OLS estimate of price elasticity is {naive_ols.params[1]:.4f}, which is biased because it ignores the confounders.\")\n",
    "    \n",
    "    dml_data_elas = dml.DoubleMLData.from_arrays(X, log_quantity, np.log(price))\n",
    "    ml_g = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "    ml_m = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "    \n",
    "    dml_plr_elas = dml.DoubleMLPLR(dml_data_elas, ml_g, ml_m)\n",
    "    dml_plr_elas.fit()\n",
    "    \n",
    "    print(\"\\n--- DML Results for Price Elasticity ---\")\n",
    "    print(dml_plr_elas.summary)\n",
    "    note(f\"The DML estimate is very close to the true elasticity of {TRUE_ELASTICITY}, demonstrating its ability to correct for endogeneity bias when confounders are well-modeled.\")\n",
    "else:\n",
    "    note(\"DoubleML not available. Skipping price elasticity case study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\\n",
    "## 7. Test your knowledge\\n",
    "\\n",
    "1.  **The Importance of Orthogonalization:** Explain in your own words why the \"double\" part of Double Machine Learning is so important, using the Frisch-Waugh-Lovell theorem as a guide. What problem does it solve compared to a \"single\" machine learning approach where you only residualize the outcome $Y$ (i.e., compute $\\tilde{Y} = Y - \\hat{g}(X)$) but then regress $\\tilde{Y}$ on the original treatment $D$? What kind of bias would this \"single ML\" approach suffer from if the controls $X$ are also correlated with the treatment $D$?\n",
    "\\n",
    "2.  **The Role of Cross-Fitting:** What is the purpose of cross-fitting (sample splitting) in the DML algorithm? What kind of bias would occur if you trained your nuisance models for $g(X)$ and $m(X)$ and then predicted the residuals on the *same* data you used for training? (Hint: think about how overfitting in the nuisance models would affect the final regression on the residuals).\\n",
    "\\n",
    "3.  **Policy Targeting with CATEs:** The Causal Forest results show significant heterogeneity. Imagine you are a policymaker who has a limited budget to roll out a new job training program (the treatment). The program is only cost-effective if its causal effect on wages is greater than 1.1. How might you use the CATE estimates to design a more effective and targeted policy intervention compared to just using the Average Treatment Effect (ATE)? What specific subgroup would you target?\n",
    "\\n",
    "4.  **DML with Different Learners:** The beauty of DML is that the nuisance models can be any supervised ML algorithm. Modify the `dml_from_scratch` function to use a different learner, such as `sklearn.ensemble.RandomForestRegressor`, instead of `LassoCV`. Does it still recover the true effect in the simulation? How might the performance of a Random Forest vs. LASSO differ in a real-world dataset where the true functional forms of the nuisance functions are unknown and potentially highly non-linear?\n",
    "\\n",
    "5.  **\"Honest\" Causal Forests:** A key innovation proposed by Athey and Imbens is the concept of an \"honest\" tree or forest. This involves splitting the data sample for each tree: one subsample is used to determine the split points (the structure of the tree), and another, independent subsample is used to estimate the treatment effects within the resulting leaves. Research and explain why this \"honesty\" is important for obtaining unbiased CATE estimates. What problem is it designed to prevent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\\n",
    "## 8. Key Takeaways\\n",
    "\\n",
    "This chapter introduced Causal Machine Learning, a modern framework for estimating causal effects in complex, high-dimensional settings.\\n",
    "\\n",
    "**Key Concepts**:\\n",
    "- **Prediction Serving Inference**: The core idea of Causal ML is to use flexible predictive models (the 'ML' part) to handle high-dimensional, non-linear nuisance parameters, which allows for a robust final estimate of a low-dimensional causal parameter of interest.\\n",
    "- **Orthogonalization (Double ML)**: DML uses the logic of the Frisch-Waugh-Lovell theorem to create an orthogonal moment equation. By partialling out the effects of confounders from *both* the treatment and the outcome, the final causal estimate becomes robust to small errors in the nuisance models. This, combined with cross-fitting to prevent overfitting bias, corrects the regularization bias that plagues naive ML approaches.\\n",
    "- **Heterogeneous Effects (Causal Forests)**: While DML focuses on the Average Treatment Effect (ATE), Causal Forests are designed to estimate Conditional Average Treatment Effects (CATEs). They modify the Random Forest algorithm to find subgroups of the population with systematically different treatment effects, which is crucial for policy targeting and personalization.\\n",
    "- **Specialized Libraries**: In practice, dedicated libraries like `DoubleML` and `EconML` should be used as they correctly handle the cross-fitting procedures and provide valid standard errors for inference."
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='meta-learners'></a>\n",
    "## 9. Further reading\n",
    "\n",
    "`EconML` and other libraries also provide simpler but powerful methods called \"meta-learners.\" These use standard supervised machine learning models to estimate the CATE.\n",
    "\n",
    "- **S-Learner (\"Single-Learner\"):** This is the simplest approach. You fit a single model of the form $Y \\sim T + X$. The treatment is just included as another feature. The CATE is then estimated as $\\hat{\\tau}(x) = \\text{model}(T=1, X=x) - \\text{model}(T=0, X=x)$. This often performs poorly because the treatment feature can be overwhelmed by other features.\n",
    "\n",
    "- **T-Learner (\"Two-Learner\"):** This approach fits two separate models: one for the treated group ($E[Y|T=1, X=x]$) and one for the control group ($E[Y|T=0, X=x]$). The CATE is the difference between their predictions: $\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)$. This is generally better than the S-learner.\n",
    "\n",
    "- **X-Learner:** Developed by Kunzel et al. (2019), the X-learner is a more advanced, two-stage method that is particularly effective when the treatment and control groups are of different sizes. It uses information from both groups to impute counterfactuals and then models the treatment effect directly.\n",
    "\n",
    "Causal Forests often outperform these meta-learners, but the meta-learners provide excellent benchmarks and are easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**1. The Importance of Orthogonalization:**\\n",
    "The 'double' part is crucial because it makes the final estimate robust to prediction errors in the nuisance models. A 'single' ML approach that only residualizes the outcome ($Y - \\hat{g}(X)$) and regresses it on the original treatment $D$ would still suffer from omitted variable bias. If the model for $g(X)$ is imperfect, the residual $\\tilde{Y}$ still contains some effect of $X$. Since $D$ is also correlated with $X$, $D$ will be correlated with the part of $X$ left in the residual, biasing the coefficient on $D$. By residualizing *both* $Y$ and $D$, we ensure that the final regression is between two variables that are both orthogonal to the confounders, removing this source of bias.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**2. The Role of Cross-Fitting:**\\n",
    "Cross-fitting prevents bias from overfitting. If you train a flexible ML model for $g(X)$ on a dataset and then predict on that same dataset, the model's predictions $\\hat{g}(X)$ will be 'too good'—they will fit not just the signal but also the random noise. When you calculate the residual $Y - \\hat{g}(X)$, you will be systematically underestimating the true error. This mechanical correlation would bias the final-stage regression. By training the nuisance models on one 'fold' of the data and constructing the residuals on another 'fold' (the one that wasn't used for training), we ensure that the residuals are constructed using an 'out-of-sample' prediction, which breaks this overfitting-induced correlation.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**3. Policy Targeting with CATEs:**\\n",
    "The ATE might be positive but small, suggesting the program is not cost-effective on average. However, the CATE estimates might reveal that the program has a very large, positive effect for a specific subgroup (e.g., workers with low prior income, or those in a specific industry) and a zero or negative effect for others. A policymaker could use this information to design a more effective, targeted policy by offering the program *only* to the subgroups for whom the CATE is estimated to be greater than the 1.1 cost-effectiveness threshold. This would maximize the program's impact and return on investment.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**4. DML with Different Learners:**\\n",
    "Yes, using a `RandomForestRegressor` in the `dml_from_scratch` function should still recover the true effect, because the DML framework is designed to be robust to the choice of ML learner. In a real-world dataset, the choice would matter more. If the true nuisance functions $g(X)$ and $m(X)$ are sparse and approximately linear, LASSO would likely perform very well. If the true functions are highly non-linear and involve complex interactions between features, a Random Forest would likely produce better predictions for the nuisance functions, leading to a more precise final estimate of the causal effect.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**5. 'Honest' Causal Forests:**\\n",
    "'Honesty' in this context means preventing the model from using the same information to both select the model structure (the tree's splits) and to make predictions. In a standard random forest, the same outcome data $Y$ is used to both decide on the best split and to form the prediction within the resulting leaf. This can lead to an overfitting bias where the model finds splits that look good for the specific sample but don't generalize. By using one subsample of data to build the tree structure and an entirely separate subsample to estimate the treatment effects within the leaves of that tree, an 'honest' forest ensures that the final CATE estimates are not biased by the process that selected the splits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
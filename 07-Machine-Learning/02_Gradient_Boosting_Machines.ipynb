{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Image\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*\"=\"}')\n",
    "\n",
    "note(\"Environment initialized for Gradient Boosting Machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.2: Gradient Boosting Machines\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Boosting Intuition: Learning from Errors**](#intro)\n",
    "2.  [**The Gradient Boosting Algorithm**](#algorithm)\n",
    "3.  [**XGBoost: The Workhorse of Tabular Data**](#xgboost)\n",
    "4.  [**Code Lab: Predicting House Prices with XGBoost**](#code-lab)\n",
    "5.  [**Summary**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Boosting Intuition: Learning from Errors\n",
    "\n",
    "**Boosting** is an ensemble technique that builds models in a sequential fashion. Each new model is trained to correct the errors made by its predecessors. Unlike bagging, which focuses on reducing variance, boosting aims to reduce bias.\n",
    "\n",
    "The core idea is to fit a sequence of weak learners (e.g., shallow decision trees) to weighted versions of the data, where more weight is given to the observations that were misclassified by earlier models.\n",
    "\n",
    "![The Boosting Process](images/07-Machine-Learning/boosting_process.png)"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='algorithm'></a>\n",
    "## 2. The Gradient Boosting Algorithm\n",
    "\n",
    "**Gradient Boosting** frames the boosting problem as a gradient descent optimization in function space. Each new weak learner is trained to fit the negative gradient of the loss function with respect to the predictions of the current ensemble. For squared error loss, this simplifies to fitting each new tree to the *residuals* (the errors) of the previous model.\n",
    "> **Historical Context: Gradient Boosting**\n",
    "> The Gradient Boosting algorithm was developed by Jerome Friedman in 1999. It is a generalization of the AdaBoost algorithm, and it allows for the use of arbitrary differentiable loss functions. This makes it a very flexible and powerful tool, and it is one of the most widely used machine learning algorithms today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgboost'></a>\n",
    "## 3. XGBoost: The Workhorse of Tabular Data\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** is a highly efficient and effective implementation of the gradient boosting algorithm. It includes several key innovations:\n",
    "- **Regularization:** It adds L1 and L2 regularization terms to the objective function to prevent overfitting.\n",
    "- **Sparsity Awareness:** It can handle missing values efficiently.\n",
    "- **Parallelization:** It can parallelize the construction of trees.\n",
    "\n",
    "XGBoost is often the go-to algorithm for competitions and real-world applications involving tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-lab'></a>\n",
    "## 4. Code Lab: Predicting House Prices with XGBoost\n",
    "\n",
    "Let's use XGBoost to predict house prices from a set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"XGBoost for House Price Prediction\")\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5) * 10\n",
    "y = 50 + (X[:, 0] * 1.5) + (X[:, 1] * 0.8) + (X[:, 2] * 2.1) + np.random.randn(100) * 5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We instantiate the XGBoost regressor, specifying the squared error loss function and the number of trees to build.\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)\n",
    "# We then fit the model to the training data.\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# We can then use the trained model to make predictions on the test set.\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "# We evaluate the model's performance using the root mean squared error.\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "note(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# XGBoost provides a built-in function to visualize the importance of each feature in the model.\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.title('Feature Importance')\n",
    "if not os.path.exists('images/07-Machine-Learning'):\n",
    "    os.makedirs('images/07-Machine-Learning')\n",
    "plt.savefig('images/07-Machine-Learning/feature_importance.png')\n",
    "plt.close()\n",
    "display(Image(filename='images/07-Machine-Learning/feature_importance.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 5. Summary\n",
    "\n",
    "Gradient Boosting Machines, and XGBoost in particular, are powerful and widely used models for tabular data. Their sequential, error-correcting nature makes them highly accurate, and implementations like XGBoost provide the efficiency and regularization needed for real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "# Purpose: Import necessary libraries for data manipulation, visualization, and machine learning.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Scikit-learn provides the tools for preprocessing (StandardScaler), dimensionality reduction (PCA),\n",
    "# modeling (LinearRegression), and robust evaluation (TimeSeriesSplit, mean_squared_error).\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# pandas_datareader is used to fetch data directly from online sources like the FRED database.\n",
    "import pandas_datareader.data as web\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "# Purpose: Standardize plotting styles and numerical output for consistency and readability.\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "# Purpose: Create helper functions for formatted output in the notebook.\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*\"=\"}')\n",
    "\n",
    "note(\"Environment initialized for Macroeconomic Forecasting with ML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Machine Learning for Economists\n",
    "## Chapter 7.18: Applied ML - Macroeconomic Forecasting with Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Challenge of High-Dimensional Macro Data\n",
    "**Intellectual Provenance:** The use of factor models in macroeconomics has a long history, but the application of principal components to large macroeconomic datasets was popularized by the work of James Stock and Mark Watson in the early 2000s. Their papers demonstrated that a few principal components extracted from a large panel of macroeconomic series could effectively summarize the state of the economy and produce superior forecasts compared to traditional, smaller-scale models. This 'diffusion index' or 'factor-augmented' forecasting approach has since become a standard tool in empirical macroeconomics.\n",
    "\n",
    "Modern macroeconomic forecasting often faces a high-dimensionality problem. Central banks and researchers have access to hundreds, if not thousands, of potentially relevant time series (e.g., industrial production for different sectors, various interest rates, employment figures, surveys, etc.). Using all of these predictors in a standard regression model is infeasible due to the curse of dimensionality and severe multicollinearity.\n",
    "\n",
    "A powerful solution is to use **Principal Component Analysis (PCA)** to distill the information from this large set of predictors into a small number of **estimated macroeconomic factors**. These factors, which are the principal components of the data, can be interpreted as capturing the primary drivers of economic variation (e.g., a 'real activity' factor, an 'inflation' factor, a 'financial conditions' factor).\n",
    "\n",
    "This notebook provides a practical, step-by-step guide to building a **Principal Component Regression (PCR)** model to forecast GDP growth, a common workflow in modern macro-econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Acquisition and Preparation\n",
    "We will use the widely-cited **FRED-MD dataset**, a large panel of monthly US macroeconomic variables maintained by the St. Louis Fed. Our goal will be to forecast quarterly GDP growth using the information contained in the monthly series.\n",
    "\n",
    "The process involves:\n",
    "1.  **Loading the data:** We'll fetch the FRED-MD dataset and the target variable (Real GDP).\n",
    "2.  **Data Cleaning:** Handling missing values is crucial. We will use a simple forward-fill and back-fill strategy.\n",
    "3.  **Transformations:** Many macro series are non-stationary. We will apply the transformations (e.g., taking logs, differencing) suggested by the creators of the dataset to induce stationarity.\n",
    "4.  **Aggregation:** We will aggregate the monthly predictor data to a quarterly frequency to match the frequency of our GDP target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Data Acquisition and Preparation\")\n",
    "\n",
    "try:\n",
    "    # Step 1: Load Data from FRED.\n",
    "    # We fetch the FRED-MD monthly dataset, which is a large panel of U.S. macroeconomic variables.\n",
    "    # This dataset is commonly used for testing forecasting models in a data-rich environment.\n",
    "    fred_md_url = 'https://files.stlouisfed.org/files/htdocs/fred-md/monthly/current.csv'\n",
    "    df_raw = pd.read_csv(fred_md_url)\n",
    "    # The date column must be parsed into a datetime object to serve as the DataFrame index.\n",
    "    df_raw['sasdate'] = pd.to_datetime(df_raw['sasdate'], format='%m/%d/%Y')\n",
    "    df_raw = df_raw.set_index('sasdate')\n",
    "    \n",
    "    # The first row of the raw CSV contains transformation codes, not data.\n",
    "    # We separate these codes into their own DataFrame for later use.\n",
    "    tcode_df = df_raw.iloc[:1, 1:]\n",
    "    df = df_raw.iloc[1:, 1:]\n",
    "    \n",
    "    # Step 2: Clean the data.\n",
    "    # Missing values are a common issue. A simple and robust method for large macro panels is to\n",
    "    # forward-fill existing values and then back-fill any remaining NaNs at the beginning of the series.\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Step 3: Apply transformations to induce stationarity.\n",
    "    # Most macroeconomic time series are non-stationary. The data providers suggest specific transformations\n",
    "    # (e.g., differencing, log-differencing) to make them stationary, which is a prerequisite for many models, including PCA.\n",
    "    def transform(series, tcode):\n",
    "        if tcode == 2: return series.diff() # First difference for levels\n",
    "        if tcode == 3: return series.diff().diff() # Second difference for accelerating series\n",
    "        if tcode == 4: return np.log(series) # Log level\n",
    "        if tcode == 5: return np.log(series).diff() # Log difference (approx. growth rate)\n",
    "        if tcode == 6: return np.log(series).diff().diff() # Change in growth rate\n",
    "        return series # No transformation needed\n",
    "\n",
    "    df_transformed = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        tcode = tcode_df[col].iloc[0]\n",
    "        df_transformed[col] = transform(df[col].astype(float), tcode)\n",
    "    # Transformations like differencing introduce NaNs at the start; these must be dropped.\n",
    "    df_transformed = df_transformed.dropna()\n",
    "    \n",
    "    # Step 4: Load and prepare the target variable (quarterly GDP growth).\n",
    "    gdp = web.DataReader('GDPC1', 'fred', start='1960-01-01', end='2023-12-31')\n",
    "    # We calculate the quarterly growth rate as the log difference, multiplied by 100 for percentage terms.\n",
    "    gdp_growth = np.log(gdp).diff().dropna() * 100\n",
    "    \n",
    "    # Step 5: Align the data frequencies.\n",
    "    # Our predictors are monthly, but the target (GDP) is quarterly. We must align them.\n",
    "    # A common method is to aggregate the monthly data to quarterly by taking the mean value within each quarter.\n",
    "    X = df_transformed.resample('Q').mean()\n",
    "    # We then join the predictors and target, keeping only the time periods where both exist ('inner' join).\n",
    "    data_full = X.join(gdp_growth, how='inner')\n",
    "    data_full = data_full.rename(columns={'GDPC1': 'GDP_Growth'})\n",
    "    \n",
    "    # Step 6: Create lagged predictors for forecasting.\n",
    "    # The core of forecasting: to predict GDP growth for quarter t, we can only use information\n",
    "    # available up to the end of quarter t-1. We achieve this by shifting the predictor matrix forward by one period.\n",
    "    y = data_full['GDP_Growth']\n",
    "    X_full = data_full.drop('GDP_Growth', axis=1)\n",
    "    X_lagged = X_full.shift(1).dropna()\n",
    "    # After lagging, we must re-align y to ensure the dates match the new, shorter X matrix.\n",
    "    y = y.loc[X_lagged.index]\n",
    "    \n",
    "    note(f\"Data preparation complete. We have {X_lagged.shape[1]} potential predictors to forecast GDP growth.\")\n",
    "    DATA_LOADED = True\n",
    "except Exception as e:\n",
    "    note(f\"Could not load data. Error: {e}. Skipping notebook execution.\")\n",
    "    DATA_LOADED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Principal Component Regression\n",
    "Now we apply PCA to our large set of predictors ($X_{lagged}$) to extract a smaller number of principal components. We will then use these components as regressors in a standard linear regression model to predict GDP growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Principal Component Analysis of Macro Data\")\n",
    "\n",
    "if DATA_LOADED:\n",
    "    # Step 1: Standardize the data.\n",
    "    # PCA is sensitive to the scale of the variables. A variable with a large variance could dominate\n",
    "    # the first principal component. To prevent this, we standardize each series to have a mean of 0 and a standard deviation of 1.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_lagged)\n",
    "    \n",
    "    # Step 2: Fit PCA.\n",
    "    # We apply PCA to the scaled predictor matrix. This process finds the orthogonal linear combinations\n",
    "    # of the original variables that capture the maximum amount of variance.\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Step 3: Analyze the explained variance.\n",
    "    # The scree plot is a critical diagnostic tool. It shows the cumulative variance explained by the components.\n",
    "    # This helps us decide how many components (factors) are needed to summarize the information in the original dataset.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Scree Plot: Variance Explained by Principal Components')\n",
    "    # It's common to choose a threshold, like 80% of variance explained, to select the number of components.\n",
    "    n_components_80 = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.8)[0][0] + 1\n",
    "    plt.axhline(0.8, color='r', ls='--', label=f'{n_components_80} components explain 80% of variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    note(f\"The scree plot shows that the first few principal components capture a large fraction of the total variation in the macroeconomic dataset. We can explain 80% of the variance with just {n_components_80} factors, achieving significant dimensionality reduction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Forecasting GDP Growth with Principal Components\")\n",
    "\n",
    "if DATA_LOADED:\n",
    "    # Step 1: Select the number of principal components (factors) to use as predictors.\n",
    "    # Based on the scree plot or other criteria, we choose a small number of components for our model.\n",
    "    n_pcs = 5\n",
    "    X_factors = X_pca[:, :n_pcs]\n",
    "    \n",
    "    # Step 2: Set up a time-series cross-validation.\n",
    "    # Standard k-fold cross-validation is invalid for time series because it can lead to training on future data\n",
    "    # and testing on past data. TimeSeriesSplit creates an expanding window, which respects the temporal order of the data.\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # We will store the out-of-sample predictions and actual values here.\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # Step 3: Loop through the time series splits, training and predicting at each step.\n",
    "    # This mimics how a forecaster would operate in real-time, re-estimating their model as new data arrives.\n",
    "    for train_index, test_index in tscv.split(X_factors):\n",
    "        X_train, X_test = X_factors[train_index], X_factors[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train the model only on the 'past' data.\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict on the 'future' data.\n",
    "        preds = model.predict(X_test)\n",
    "        predictions.extend(preds)\n",
    "        actuals.extend(y_test)\n",
    "        \n",
    "    # Step 4: Evaluate the out-of-sample performance.\n",
    "    # We calculate the out-of-sample R-squared. A positive value indicates that the model's forecasts\n",
    "    # are better than a simple forecast based on the historical mean.\n",
    "    r_squared = 1 - np.sum((np.array(actuals) - np.array(predictions))**2) / np.sum((np.array(actuals) - np.mean(np.array(actuals)))**2)\n",
    "    note(f\"Out-of-Sample R-squared from the Principal Component Regression: {r_squared:.3f}\")\n",
    "    \n",
    "    # Step 5: Visualize the forecasts against the actual data.\n",
    "    # This plot provides a qualitative assessment of the model's performance over time.\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    # Create a DataFrame for easy plotting with correct dates.\n",
    "    plot_df = pd.DataFrame({'Actual GDP Growth': actuals, 'Predicted GDP Growth': predictions}, index=y.index[len(y)-len(actuals):])\n",
    "    plot_df.plot(ax=ax, style=['-', '--'])\n",
    "    ax.set_title('GDP Growth Forecast: Actual vs. Predicted (Out-of-Sample)')\n",
    "    ax.set_ylabel('Quarterly GDP Growth (%)')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion and Extensions\n",
    "This notebook demonstrated a powerful and practical technique for macroeconomic forecasting in a data-rich environment. By combining PCA for dimensionality reduction with a simple linear regression, we were able to build a model that effectively forecasts GDP growth out-of-sample.\n",
    "\n",
    "This approach can be extended in several ways:\n",
    "- **Dynamic Factor Models (DFM):** A more sophisticated version that explicitly models the time-series dynamics of the factors and idiosyncratic components.\n",
    "- **Alternative Non-linear Models:** Instead of a linear regression on the factors, one could use more flexible models like Random Forests or Gradient Boosting Machines.\n",
    "- **Interpreting the Factors:** A key challenge is to assign economic interpretations to the estimated principal components. This can be done by examining the 'factor loadings' (the correlations of the original series with the components) to see which variables are most important for each factor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

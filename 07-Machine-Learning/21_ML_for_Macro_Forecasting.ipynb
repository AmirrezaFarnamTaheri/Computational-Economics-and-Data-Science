{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_datareader.data as web\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*\"=\"}')\n",
    "\n",
    "note(\"Environment initialized for Macroeconomic Forecasting with ML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Machine Learning for Economists\n",
    "## Chapter 7.18: Applied ML - Macroeconomic Forecasting with Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Challenge of High-Dimensional Macro Data\n",
    "**Intellectual Provenance:** The use of factor models in macroeconomics has a long history, but the application of principal components to large macroeconomic datasets was popularized by the work of James Stock and Mark Watson in the early 2000s. Their papers demonstrated that a few principal components extracted from a large panel of macroeconomic series could effectively summarize the state of the economy and produce superior forecasts compared to traditional, smaller-scale models. This 'diffusion index' or 'factor-augmented' forecasting approach has since become a standard tool in empirical macroeconomics.\n",
    "\n",
    "Modern macroeconomic forecasting often faces a high-dimensionality problem. Central banks and researchers have access to hundreds, if not thousands, of potentially relevant time series (e.g., industrial production for different sectors, various interest rates, employment figures, surveys, etc.). Using all of these predictors in a standard regression model is infeasible due to the curse of dimensionality and severe multicollinearity.\n",
    "\n",
    "A powerful solution is to use **Principal Component Analysis (PCA)** to distill the information from this large set of predictors into a small number of **estimated macroeconomic factors**. These factors, which are the principal components of the data, can be interpreted as capturing the primary drivers of economic variation (e.g., a 'real activity' factor, an 'inflation' factor, a 'financial conditions' factor).\n",
    "\n",
    "This notebook provides a practical, step-by-step guide to building a **Principal Component Regression (PCR)** model to forecast GDP growth, a common workflow in modern macro-econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Acquisition and Preparation\n",
    "We will use the widely-cited **FRED-MD dataset**, a large panel of monthly US macroeconomic variables maintained by the St. Louis Fed. Our goal will be to forecast quarterly GDP growth using the information contained in the monthly series.\n",
    "\n",
    "The process involves:\n",
    "1.  **Loading the data:** We'll fetch the FRED-MD dataset and the target variable (Real GDP).\n",
    "2.  **Data Cleaning:** Handling missing values is crucial. We will use a simple forward-fill and back-fill strategy.\n",
    "3.  **Transformations:** Many macro series are non-stationary. We will apply the transformations (e.g., taking logs, differencing) suggested by the creators of the dataset to induce stationarity.\n",
    "4.  **Aggregation:** We will aggregate the monthly predictor data to a quarterly frequency to match the frequency of our GDP target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Data Acquisition and Preparation\")\n",
    "\n",
    "try:\n",
    "    # 1. Load Data from FRED.\n",
    "    # We fetch the FRED-MD monthly dataset and the quarterly Real GDP series.\n",
    "    fred_md_url = 'https://files.stlouisfed.org/files/htdocs/fred-md/monthly/current.csv'\n",
    "    df_raw = pd.read_csv(fred_md_url)\n",
    "    df_raw['sasdate'] = pd.to_datetime(df_raw['sasdate'], format='%m/%d/%Y')\n",
    "    df_raw = df_raw.set_index('sasdate')\n",
    "    \n",
    "    # The first row contains transformation codes, which we separate.\n",
    "    tcode_df = df_raw.iloc[:1, 1:]\n",
    "    df = df_raw.iloc[1:, 1:]\n",
    "    \n",
    "    # 2. Clean the data. A simple forward-fill then back-fill is a common approach for macro panels.\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # 3. Apply transformations to induce stationarity, as recommended by the data providers.\n",
    "    def transform(series, tcode):\n",
    "        if tcode == 2: return series.diff() # First difference\n",
    "        if tcode == 3: return series.diff().diff() # Second difference\n",
    "        if tcode == 4: return np.log(series) # Log\n",
    "        if tcode == 5: return np.log(series).diff() # Log difference\n",
    "        if tcode == 6: return np.log(series).diff().diff() # Second log difference\n",
    "        return series # No transformation\n",
    "\n",
    "    df_transformed = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        tcode = tcode_df[col].iloc[0]\n",
    "        df_transformed[col] = transform(df[col].astype(float), tcode)\n",
    "    df_transformed = df_transformed.dropna()\n",
    "    \n",
    "    # 4. Load and prepare the target variable (quarterly GDP growth).\n",
    "    gdp = web.DataReader('GDPC1', 'fred', start='1960-01-01', end='2023-12-31')\n",
    "    gdp_growth = np.log(gdp).diff().dropna() * 100 # Log difference gives approx. growth rate\n",
    "    \n",
    "    # 5. Align the data frequencies.\n",
    "    # We aggregate the monthly predictors to quarterly by taking the mean and then join with GDP.\n",
    "    X = df_transformed.resample('Q').mean()\n",
    "    data_full = X.join(gdp_growth, how='inner')\n",
    "    data_full = data_full.rename(columns={'GDPC1': 'GDP_Growth'})\n",
    "    \n",
    "    # 6. Create lagged predictors for forecasting.\n",
    "    # To forecast GDP growth at time t, we can only use information available up to time t-1.\n",
    "    y = data_full['GDP_Growth']\n",
    "    X_full = data_full.drop('GDP_Growth', axis=1)\n",
    "    X_lagged = X_full.shift(1).dropna()\n",
    "    y = y.loc[X_lagged.index] # Align y with the lagged X\n",
    "    \n",
    "    note(f\"Data preparation complete. We have {X_lagged.shape[1]} potential predictors to forecast GDP growth.\")\n",
    "    DATA_LOADED = True\n",
    "except Exception as e:\n",
    "    note(f\"Could not load data. Error: {e}. Skipping notebook execution.\")\n",
    "    DATA_LOADED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Principal Component Regression\n",
    "Now we apply PCA to our large set of predictors ($X_{lagged}$) to extract a smaller number of principal components. We will then use these components as regressors in a standard linear regression model to predict GDP growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Principal Component Analysis of Macro Data\")\n",
    "\n",
    "if DATA_LOADED:\n",
    "    # 1. Standardize the data.\n",
    "    # PCA is sensitive to the scale of the variables, so we standardize each series to have\n",
    "    # a mean of 0 and a standard deviation of 1.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_lagged)\n",
    "    \n",
    "    # 2. Fit PCA.\n",
    "    # We fit PCA to the scaled predictor matrix to extract the principal components.\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 3. Analyze the explained variance.\n",
    "    # The scree plot shows the cumulative variance explained by the components. It's a key tool\n",
    "    # for deciding how many components (factors) to retain for the forecasting model.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Variance Explained by Principal Components')\n",
    "    n_components_80 = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.8)[0][0] + 1\n",
    "    plt.axhline(0.8, color='r', ls='--', label=f'{n_components_80} components explain 80% of variance')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    note(f\"The scree plot shows that the first few principal components capture a large fraction of the total variation in the macroeconomic dataset. We can explain 80% of the variance with just {n_components_80} factors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Forecasting GDP Growth with Principal Components\")\n",
    "\n",
    "if DATA_LOADED:\n",
    "    # 1. Select the number of principal components (factors) to use as predictors.\n",
    "    n_pcs = 5\n",
    "    X_factors = X_pca[:, :n_pcs]\n",
    "    \n",
    "    # 2. Set up a time-series cross-validation.\n",
    "    # Standard k-fold cross-validation is invalid for time series. TimeSeriesSplit creates an\n",
    "    # expanding window, ensuring that the model is always trained on past data and evaluated on future data.\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # 3. Loop through the time series splits, training and predicting at each step.\n",
    "    for train_index, test_index in tscv.split(X_factors):\n",
    "        X_train, X_test = X_factors[train_index], X_factors[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        predictions.extend(preds)\n",
    "        actuals.extend(y_test)\n",
    "        \n",
    "    # 4. Evaluate the out-of-sample performance.\n",
    "    # We calculate the out-of-sample R-squared, a measure of how well the model's forecasts\n",
    "    # compare to a simple historical mean forecast.\n",
    "    r_squared = 1 - np.sum((np.array(actuals) - np.array(predictions))**2) / np.sum((np.array(actuals) - np.mean(np.array(actuals)))**2)\n",
    "    note(f\"Out-of-Sample R-squared from the Principal Component Regression: {r_squared:.3f}\")\n",
    "    \n",
    "    # 5. Visualize the forecasts against the actual data.\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    plot_df = pd.DataFrame({'Actual GDP Growth': actuals, 'Predicted GDP Growth': predictions}, index=y.index[len(y)-len(actuals):])\n",
    "    plot_df.plot(ax=ax)\n",
    "    ax.set_title('GDP Growth Forecast: Actual vs. Predicted (Out-of-Sample)')\n",
    "    ax.set_ylabel('Quarterly GDP Growth (%)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion and Extensions\n",
    "This notebook demonstrated a powerful and practical technique for macroeconomic forecasting in a data-rich environment. By combining PCA for dimensionality reduction with a simple linear regression, we were able to build a model that effectively forecasts GDP growth out-of-sample.\n",
    "\n",
    "This approach can be extended in several ways:\n",
    "- **Dynamic Factor Models (DFM):** A more sophisticated version that explicitly models the time-series dynamics of the factors and idiosyncratic components.\n",
    "- **Alternative Non-linear Models:** Instead of a linear regression on the factors, one could use more flexible models like Random Forests or Gradient Boosting Machines.\n",
    "- **Interpreting the Factors:** A key challenge is to assign economic interpretations to the estimated principal components. This can be done by examining the 'factor loadings' (the correlations of the original series with the components) to see which variables are most important for each factor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# === Environment Setup ===\n",
        "import os, sys, math, time, random, json, textwrap, warnings\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GYM_AVAILABLE = False\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- Configuration ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 7), 'figure.dpi': 150})\n",
        "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def note(msg, **kwargs): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
        "def sec(title): print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
        "\n",
        "if not GYM_AVAILABLE: note(\"Gymnasium is not installed. Skipping code labs. Run `pip install gymnasium[classic_control]`.\")\n",
        "note(f\"Environment initialized. Gymnasium available: {GYM_AVAILABLE}. Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 7: Advanced and Frontier Topics\n",
        "## Chapter 7.20: Advanced Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction: The Challenge of Scaling Reinforcement Learning\n",
        "\n",
        "The tabular methods discussed in the previous chapter are foundational but suffer from the **curse of dimensionality**. They require storing a value for every state-action pair, which is infeasible for problems with large or continuous state spaces. The solution is **function approximation**, where we use a parameterized function\u2014most powerfully, a deep neural network\u2014to estimate the value function, the policy, or both. This is the core idea behind **Deep Reinforcement Learning (DRL)**.\n",
        "\n",
        "However, the combination of three elements\u2014**function approximation** (like a neural network), **bootstrapping** (updating estimates from other estimates, as in TD learning), and **off-policy training** (learning about policy $\\pi$ from data generated by policy $\\mu$)\u2014is known as the **deadly triad**. When combined naively, this triad can lead to unstable and divergent training. The history of modern DRL is largely a story of developing algorithms that can successfully manage this instability.\n",
        "\n",
        "This chapter provides a PhD-level survey of the key algorithmic breakthroughs in DRL, covering both major families of algorithms: **Value-Based Methods** (learning a value function) and **Policy-Based Methods** (learning a policy directly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Value-Based Methods: Learning What to Expect\n",
        "\n",
        "Value-based methods focus on estimating the optimal action-value function, $Q^*(s, a)$. Once this function is known, the optimal policy is simply to choose the action with the highest Q-value in any given state. The archetypal algorithm in this family is Q-Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Deep Q-Networks (DQN): Taming the Deadly Triad\n",
        "\n",
        "The DQN algorithm (Mnih et al., 2015) was the first to successfully train a deep neural network to perform Q-learning, achieving superhuman performance on Atari games using only raw pixel data. It introduced two key innovations to stabilize the learning process:\n",
        "\n",
        "1.  **Experience Replay:** A large buffer stores past transitions $(s_t, a_t, r_t, s_{t+1})$. For training, mini-batches are sampled *randomly* from this buffer. This breaks the strong temporal correlations in the data, making the samples more like the i.i.d. data that standard optimizers expect.\n",
        "\n",
        "2.  **Fixed Target Network:** The TD target in the Bellman update, $y_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta)$, is problematic because the same network weights $\\theta$ are used to predict the current Q-value and the target Q-value. This creates a \"moving target\" problem. DQN solves this by using a separate **target network**, with weights $\\theta^-$, to compute the TD target. The weights of this target network are frozen for many steps and only periodically updated with the weights of the main policy network ($\\theta^- \\leftarrow \\theta$).\n",
        "\n",
        "The loss function for a DQN is the Mean Squared Error (MSE) between the predicted Q-value and the stable TD target:\n",
        "$$ L(\\theta) = E_{(s,a,r,s') \\sim U(B)} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right] $$ \n",
        "where $U(B)$ denotes a mini-batch sampled uniformly from the replay buffer $B$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Dueling DQN Architecture: Decoupling Value and Advantage\n",
        "\n",
        "The Dueling Network (Wang et al., 2016) is an architectural improvement that provides a more efficient and robust way to estimate Q-values. It decomposes the Q-value into two separate, fully-connected streams:\n",
        "\n",
        "1.  **State-Value Function ($V(s; \\theta, \\beta)$):** A scalar representing how good it is to be in state $s$. It has parameters $\\theta$ (shared with the convolutional base) and $\\beta$ (for its own fully-connected layers).\n",
        "2.  **Advantage Function ($A(s, a; \\theta, \\alpha)$):** A vector representing how much better taking action $a$ is compared to other actions in state $s$. It has shared parameters $\\theta$ and its own parameters $\\alpha$.\n",
        "\n",
        "The Q-value is then reconstructed from these two streams. However, we cannot simply add them, $Q = V + A$, because $V$ and $A$ are not uniquely identifiable (e.g., we could add a constant to $V$ and subtract it from all $A(s,a)$ without changing $Q$). To enforce identifiability, the advantage stream is forced to have zero advantage for the chosen action (or zero mean advantage). The most common formulation is:\n",
        "$$ Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a'; \\theta, \\alpha) \\right) $$ \n",
        "\n",
        "This separation allows the network to learn the value of states without having to learn the effect of each action for every state. This is particularly useful in environments where many actions have little to no impact on the state's value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This diagram illustrates the Dueling DQN architecture.\\n",
        "# The network splits into two streams: one for the state-value function (V) and one for the advantage function (A).\\n",
        "# These streams are then combined to produce the final Q-values.\\n",
        "display(Image(filename='../images/07-Machine-Learning/dueling_dqn_architecture.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Policy-Based Methods: Learning What to Do\n",
        "\n",
        "Policy-based methods directly parameterize the policy, $\\pi_\\theta(a|s)$, and optimize the policy parameters $\\theta$ by performing gradient ascent on the expected total reward.\n",
        "$$ J(\\theta) = E_{\\tau \\sim \\pi_\\theta} [R(\\tau)] = E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 The Policy Gradient Theorem and REINFORCE\n",
        "\n",
        "The **Policy Gradient Theorem** provides a way to compute the gradient of the expected reward $J(\\theta)$ without needing to know the dynamics of the environment:\n",
        "$$ \\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) \\left( \\sum_{t=0}^T r(s_t, a_t) \\right) \\right] $$ \n",
        "\n",
        "The **REINFORCE** algorithm is the most direct application of this theorem. It collects a trajectory, and for each action $a_t$ taken, it multiplies the score function, $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$, by the total return from that point onward, $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$. The intuition is to \"reinforce\" actions that led to high returns by increasing their log-probability.\n",
        "\n",
        "**Problem:** While unbiased, this gradient estimate has extremely **high variance**. The return $G_t$ can vary significantly depending on the stochastic actions taken and the environment's transitions, leading to noisy and slow learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Actor-Critic Methods: Reducing Variance with a Critic\n",
        "\n",
        "**Actor-Critic (AC)** methods combine the best of value-based and policy-based methods to tackle the variance problem. They use two networks:\n",
        "\n",
        "1.  **The Actor ($\\pi_\\theta(a|s)$):** A policy network that controls how the agent acts.\n",
        "2.  **The Critic ($V_\\phi(s)$):** A value network that learns the state-value function, $V^{\\pi_\\theta}(s)$.\n",
        "\n",
        "The critic's job is to provide a low-variance estimate of the return. We can subtract the state value $V(s_t)$ from the return $G_t$ to get the **Advantage Function**, $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. The policy gradient update becomes:\n",
        "$$ \\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) A(s_t, a_t)] $$\n",
        "\n",
        "Using the advantage function has a clear intuition: instead of just asking \"was this action good?\", we ask \"was this action *better than average*?\". This significantly reduces variance because the baseline $V(s_t)$ accounts for much of the stochasticity in the return.\n",
        "\n",
        "In practice, we use the critic's estimate of the value function to compute the **TD Advantage Estimate**: $A(s_t, a_t) \\approx r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)$. This is the core of modern algorithms like **Advantage Actor-Critic (A2C)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Proximal Policy Optimization (PPO): The State of the Art\n",
        "\n",
        "A major challenge in policy gradient methods is that a single large, noisy policy update can destroy the policy, leading to a catastrophic drop in performance from which the agent cannot recover. **Trust Region Policy Optimization (TRPO)** addressed this by constraining policy updates to be within a certain KL-divergence of the old policy, but it is computationally expensive.\n",
        "\n",
        "**Proximal Policy Optimization (PPO)** (Schulman et al., 2017) achieves the stability of TRPO with a much simpler mechanism. It modifies the objective function to penalize large changes in the policy. The key innovation is the **clipped surrogate objective function**:\n",
        "\n",
        "Let $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ be the probability ratio between the new and old policies.\n",
        "\n",
        "The PPO objective is:\n",
        "$$ L^{CLIP}(\\theta) = E_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] $$\n",
        "\n",
        "This objective takes the minimum of two terms:\n",
        "1.  The standard policy gradient objective, $r_t(\\theta) \\hat{A}_t$.\n",
        "2.  A clipped version, where the probability ratio $r_t(\\theta)$ is clipped to be within the range $[1 - \\epsilon, 1 + \\epsilon]$ (e.g., $\\epsilon=0.2$).\n",
        "\n",
        "If the advantage $\\hat{A}_t$ is positive, the objective increases with $r_t$, but the clipping prevents it from increasing too much. If the advantage is negative, the objective decreases with $r_t$, but the clipping prevents it from decreasing too much. This simple mechanism effectively creates a trust region, preventing destructive policy updates and making PPO incredibly robust and sample-efficient. It is often the default choice for new DRL problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Code Lab: Advantage Actor-Critic (A2C) in PyTorch\n",
        "\n",
        "We will now implement a robust A2C agent to solve the classic `CartPole-v1` environment. The goal is to balance a pole on a cart by moving the cart left or right. The state is a 4-dimensional vector (cart position, cart velocity, pole angle, pole angular velocity), and the action space is discrete (left or right).\n",
        "\n",
        "Our A2C implementation will feature:\n",
        "- A single neural network with two heads: one for the policy (Actor) and one for the value function (Critic).\n",
        "- Training on batches of experience collected from the environment.\n",
        "- Calculation of discounted returns and advantages to drive the learning updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sec(\"Implementing Advantage Actor-Critic (A2C)\")\n",
        "\n",
        "if not GYM_AVAILABLE:\n",
        "    note(\"Gymnasium not installed. Skipping this code lab.\")\n",
        "else:\n",
        "    # 1. Define the Actor-Critic Network\n",
        "    # This network has a shared body and two heads: one for the policy (Actor) and one for the value function (Critic).\n",
        "    class ActorCritic(nn.Module):\n",
        "        def __init__(self, input_dims, n_actions):\n",
        "            super(ActorCritic, self).__init__()\n",
        "            # Shared layers learn common features from the input state.\n",
        "            self.shared_layer = nn.Sequential(\n",
        "                nn.Linear(input_dims, 128),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            # The Actor head outputs a probability distribution over actions (the policy).\n",
        "            self.actor_head = nn.Linear(128, n_actions)\n",
        "            # The Critic head outputs a single value, estimating the value of the current state.\n",
        "            self.critic_head = nn.Linear(128, 1)\n",
        "\n",
        "        def forward(self, state):\n",
        "            shared_features = self.shared_layer(state)\n",
        "            action_logits = self.actor_head(shared_features)\n",
        "            state_value = self.critic_head(shared_features)\n",
        "            # The softmax function converts the logits into a probability distribution.\n",
        "            return F.softmax(action_logits, dim=-1), state_value\n",
        "\n",
        "    # 2. A2C Agent Training Loop\n",
        "    def train_a2c(env, episodes=1000, gamma=0.99):\n",
        "        input_dims = env.observation_space.shape[0]\n",
        "        n_actions = env.action_space.n\n",
        "        model = ActorCritic(input_dims, n_actions).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            log_probs = []\n",
        "            values = []\n",
        "            rewards = []\n",
        "            \n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            ep_reward = 0\n",
        "\n",
        "            # Collect one complete trajectory from the environment.\n",
        "            while not done:\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                probs, value = model(state_tensor)\n",
        "                \n",
        "                # Sample an action from the policy distribution.\n",
        "                dist = torch.distributions.Categorical(probs)\n",
        "                action = dist.sample()\n",
        "                \n",
        "                next_state, reward, terminated, truncated, _ = env.step(action.cpu().item())\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                log_probs.append(dist.log_prob(action))\n",
        "                values.append(value)\n",
        "                rewards.append(reward)\n",
        "                state = next_state\n",
        "                ep_reward += reward\n",
        "            \n",
        "            episode_rewards.append(ep_reward)\n",
        "            \n",
        "            # Calculate the discounted returns for each step in the trajectory.\n",
        "            returns = []\n",
        "            R = 0\n",
        "            for r in reversed(rewards):\n",
        "                R = r + gamma * R\n",
        "                returns.insert(0, R)\n",
        "            returns = torch.tensor(returns).to(device)\n",
        "            \n",
        "            log_probs = torch.cat(log_probs)\n",
        "            values = torch.cat(values).squeeze()\n",
        "            \n",
        "            # Calculate the advantage: how much better was the return than the critic's estimate?\n",
        "            advantage = returns - values\n",
        "            \n",
        "            # The actor loss encourages actions that led to a positive advantage.\n",
        "            actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "            # The critic loss trains the value function to be a better predictor of the returns.\n",
        "            critic_loss = F.mse_loss(returns, values)\n",
        "            total_loss = actor_loss + 0.5 * critic_loss\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-100:])\n",
        "                print(f\"Episode {episode+1}/{episodes} | Average Reward (last 100): {avg_reward:.2f}\")\n",
        "                if avg_reward >= 475:\n",
        "                    print(\"\\nEnvironment solved!\")\n",
        "                    break\n",
        "        return episode_rewards\n",
        "\n",
        "    # 3. Run the training\n",
        "    env = gym.make('CartPole-v1')\n",
        "    rewards_history = train_a2c(env, episodes=200)\n",
        "    note(\"A2C training complete.\")\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(rewards_history, label='Reward per Episode')\n",
        "    plt.plot(pd.Series(rewards_history).rolling(100).mean(), label='100-episode average', lw=3)\n",
        "    plt.title('A2C Training on CartPole-v1', fontsize=16)\n",
        "    plt.xlabel('Episode'); plt.ylabel('Total Reward')\n",
        "    plt.legend(); plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exercises\n",
        "\n",
        "1.  **The Deadly Triad:** Explain the \"deadly triad\" in your own words. For each of the three components (function approximation, bootstrapping, off-policy learning), describe what it is and why it is desirable. Then, explain how the DQN algorithm's two main innovations (Experience Replay and Fixed Target Networks) specifically address the instability caused by combining these three components.\n",
        "\n",
        "2.  **Value vs. Policy-Based Methods:** What is the fundamental difference in what value-based (e.g., DQN) and policy-based (e.g., REINFORCE) methods learn? What are the primary advantages and disadvantages of each approach? Why are Actor-Critic methods considered a hybrid approach that captures the best of both worlds?\n",
        "\n",
        "3.  **The Role of the Advantage Function:** In the A2C code lab, we calculate the advantage as `advantage = returns - values`. Explain the intuition behind this calculation. Why is using the advantage `A(s, a)` to weight the policy gradient update `\u2207\u03b8 log \u03c0\u03b8(at|st)` superior to using the raw return `G_t`, as is done in the REINFORCE algorithm? Relate your answer to the concept of variance reduction.\n",
        "\n",
        "4.  **PPO's Clipped Objective:** The PPO algorithm is the current workhorse of DRL. Its key innovation is the clipped surrogate objective function. Explain the purpose of the `clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)` term. How does this term prevent the policy from changing too drastically in a single update, and why is this property crucial for stable and efficient learning?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
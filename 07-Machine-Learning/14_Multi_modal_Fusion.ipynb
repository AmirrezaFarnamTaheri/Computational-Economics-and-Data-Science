{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for Multi-modal Fusion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.14: Multi-modal Fusion\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**What is Multi-modal Learning?**](#intro)\n",
    "2.  [**Economic Applications**](#econ-apps)\n",
    "3.  [**Fusion Strategies**](#fusion)\n",
    "    - [Early Fusion (Feature-level)](#early-fusion)\n",
    "    - [Late Fusion (Decision-level)](#late-fusion)\n",
    "    - [Intermediate/Hybrid Fusion](#intermediate-fusion)\n",
    "4.  [**Code Lab: Fusing Tabular and Text Data**](#code-lab)\n",
    "5.  [**Challenges in Multi-modal Learning**](#challenges)\n",
    "6.  [**Summary**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. What is Multi-modal Learning?\n",
    "\n",
    "Humans perceive the world by integrating information from multiple senses (vision, hearing, touch). Similarly, **multi-modal machine learning** aims to build models that can process and relate information from multiple data types, or **modalities**.\n",
    "\n",
    "Common modalities include:\n",
    "- **Tabular Data**: Structured data from databases, spreadsheets (e.g., financial statements).\n",
    "- **Text**: Unstructured text from news articles, reports, social media.\n",
    "- **Images**: Visual data from satellites, charts, products.\n",
    "- **Audio**: Speech from earnings calls, interviews.\n",
    "\n",
    "The core idea is that different modalities can provide complementary information. For example, the numerical data in a financial report might tell us *what* a company's revenue is, while the text in the report might explain *why* it has changed. The goal of multi-modal fusion is to combine these disparate sources to create a more robust and accurate predictive model than could be achieved using any single modality alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='econ-apps'></a>\n",
    "## 2. Economic Applications\n",
    "\n",
    "Multi-modal models are becoming increasingly relevant in economics and finance:\n",
    "\n",
    "- **Credit Scoring**: Combining structured financial data (income, debt) with unstructured text from loan applications.\n",
    "- **Asset Pricing**: Fusing quantitative market data with sentiment analysis from news articles and company reports (e.g., 10-K filings).\n",
    "- **Economic Forecasting**: Integrating satellite imagery (e.g., nighttime lights, shipping activity) with traditional macroeconomic time series.\n",
    "- **Real Estate Valuation**: Combining property features (square footage, bedrooms) with images of the property and text descriptions from listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fusion'></a>\n",
    "## 3. Fusion Strategies\n",
    "\n",
    "The central challenge in multi-modal learning is deciding *how* and *when* to combine the information from different modalities. There are three primary strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='../images/07-Machine-Learning/multimodal_fusion_strategies.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='early-fusion'></a>\n",
    "### 3.1 Early Fusion (Feature-level)\n",
    "\n",
    "In this approach, features are extracted from each modality and concatenated together at the input layer. A single, unified model is then trained on this combined feature vector.\n",
    "\n",
    "- **Pros**: Simple to implement. Can learn correlations between low-level features from different modalities.\n",
    "- **Cons**: Modalities must be easy to align. The resulting feature vector can be very high-dimensional and sparse. Can be sensitive to missing modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='late-fusion'></a>\n",
    "### 3.2 Late Fusion (Decision-level)\n",
    "\n",
    "In late fusion, a separate model is trained for each modality. The final prediction is made by combining the outputs (e.g., prediction scores or class probabilities) of these individual models, often through averaging, voting, or a simple meta-learner.\n",
    "\n",
    "- **Pros**: Robust to missing modalities. Allows for modality-specific model architectures. Conceptually simple.\n",
    "- **Cons**: Fails to learn low-level interactions between modalities, as the fusion happens too late in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intermediate-fusion'></a>\n",
    "### 3.3 Intermediate (or Hybrid) Fusion\n",
    "\n",
    "This strategy offers a compromise. Separate neural network branches process each modality initially. The outputs of these branches are then fused at an intermediate layer within the network, allowing for further joint processing before the final prediction is made.\n",
    "\n",
    "- **Pros**: Balances the benefits of early and late fusion. Can learn complex interactions between modalities at a higher level of abstraction.\n",
    "- **Cons**: More complex to design and tune. The optimal point of fusion is a key hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-lab'></a>\n",
    "## 4. Code Lab: Fusing Tabular and Text Data\n",
    "\n",
    "Let's build a simple intermediate fusion model to predict a house price (a regression task) using both structured tabular data (e.g., number of bedrooms, square footage) and unstructured text data (e.g., property description).\n",
    "\n",
    "This is a common problem in economics and finance, where we often have both quantitative and qualitative data. For this example, we will simulate the data, but the model architecture is a template that can be applied to real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Building a Multi-modal Fusion Model\")\n",
    "\n",
    "# --- 1. Define Inputs ---\n",
    "# We define two separate inputs for our model, one for each modality.\n",
    "input_tabular = Input(shape=(10,), name='tabular_input') # Represents 10 numerical features.\n",
    "input_text = Input(shape=(100,), name='text_input')     # Represents a sequence of 100 integer-encoded words.\n",
    "\n",
    "# --- 2. Tabular Branch ---\n",
    "# This branch processes the structured, tabular data.\n",
    "# It consists of a simple Multi-Layer Perceptron (MLP).\n",
    "tabular_branch = layers.Dense(32, activation='relu')(input_tabular)\n",
    "tabular_branch = layers.Dense(16, activation='relu')(tabular_branch)\n",
    "tabular_model = Model(inputs=input_tabular, outputs=tabular_branch)\n",
    "\n",
    "# --- 3. Text Branch ---\n",
    "# This branch processes the unstructured text data.\n",
    "# It uses an Embedding layer to convert the integer-encoded words into dense vectors,\n",
    "# and an LSTM layer to capture sequential patterns in the text.\n",
    "text_branch = layers.Embedding(input_dim=10000, output_dim=64)(input_text) # Vocabulary size of 10000.\n",
    "text_branch = layers.LSTM(32)(text_branch)\n",
    "text_model = Model(inputs=input_text, outputs=text_branch)\n",
    "\n",
    "# --- 4. Fusion (Intermediate) ---\n",
    "# The outputs of the two branches are concatenated to form a single feature vector.\n",
    "# This is the 'fusion' step.\n",
    "combined = layers.concatenate([tabular_model.output, text_model.output])\n",
    "\n",
    "# --- 5. Joint Processing ---\n",
    "# After fusion, the combined feature vector is passed through a few more Dense layers\n",
    "# to learn the relationships between the fused features.\n",
    "z = layers.Dense(32, activation=\"relu\")(combined)\n",
    "z = layers.Dense(16, activation=\"relu\")(z)\n",
    "# The final output layer has a single neuron with a linear activation function for our regression task.\n",
    "output = layers.Dense(1, activation=\"linear\")(z)\n",
    "\n",
    "# --- 6. Create the Final Model ---\n",
    "# The final model takes the two inputs and produces a single output.\n",
    "model = Model(inputs=[input_tabular, input_text], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "note(\"Multi-modal model created. It accepts two inputs and fuses them.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='challenges'></a>\n",
    "## 5. Challenges in Multi-modal Learning\n",
    "\n",
    "Despite its power, multi-modal learning presents several challenges:\n",
    "\n",
    "- **Alignment**: How to align data from different sources? (e.g., synchronizing audio and video streams).\n",
    "- **Representation**: How to learn joint representations that capture both modality-specific and shared information?\n",
    "- **Scalability**: The complexity of models increases with each new modality.\n",
    "- **Missing Data**: How should the model behave if one modality is unavailable at prediction time? (This is often addressed with techniques like *modality dropout* during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 6. Summary\n",
    "\n",
    "Multi-modal fusion is a powerful technique that allows models to develop a more holistic understanding of a problem by integrating data from various sources. For economists, this opens up exciting possibilities for combining traditional structured data with the vast amounts of unstructured text, image, and sensor data that are now available. The choice of fusion strategy‚Äîearly, late, or intermediate‚Äîis a critical design decision that depends on the specific characteristics of the data and the problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Image\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {msg}</div>\"))\n",
    "def sec(title): print(f'\\n{80*\"=\"}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for Ensemble Methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.4: Ensemble Methods\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**The Wisdom of the Crowd: Introduction to Ensembling**](#intro)\n",
    "2.  [**Bagging: Reducing Variance**](#bagging)\n",
    "3.  [**Boosting: Reducing Bias**](#boosting)\n",
    "4.  [**Stacking: Combining Heterogeneous Models**](#stacking)\n",
    "5.  [**Code Lab: Comparing Ensemble Techniques**](#code-lab)\n",
    "6.  [**Summary**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. The Wisdom of the Crowd: Introduction to Ensembling\n",
    "\n",
    "Ensemble methods are techniques that combine the predictions of multiple individual models (often called \"weak learners\") to produce a final prediction that is more accurate and robust than any of the individual models. The core idea is to leverage the diversity of the models to cancel out their individual errors.\n",
    "\n",
    "There are three main paradigms in ensemble learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging'></a>\n",
    "## 2. Bagging: Reducing Variance\n",
    "\n",
    "**Bagging**, which stands for **Bootstrap Aggregating**, is a technique that primarily aims to **reduce the variance** of a model. It works by training multiple instances of the same base model on different random subsets of the training data (drawn with replacement, i.e., bootstrapping). The final prediction is then made by averaging the predictions of all the models (for regression) or by a majority vote (for classification).\n",
    "\n",
    "The most famous example of a bagging algorithm is the **Random Forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bagging Process](images\png\bagging_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting'></a>\n",
    "## 3. Boosting: Reducing Bias\n",
    "\n",
    "**Boosting** is a sequential process where each new model is trained to correct the errors of its predecessors. Unlike bagging, where models are trained in parallel, boosting trains models sequentially. Each subsequent model places more emphasis on the data points that were misclassified by the previous models. This process allows the ensemble to focus on the most difficult cases, thereby **reducing the overall bias**.\n",
    "\n",
    "Popular boosting algorithms include **AdaBoost** and **Gradient Boosting (e.g., XGBoost, LightGBM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boosting Process](images\png\boosting_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stacking'></a>\n",
    "## 4. Stacking: Combining Heterogeneous Models\n",
    "\n",
    "**Stacking** (or Stacked Generalization) takes a slightly different approach. Instead of using a simple function (like averaging or voting) to combine the predictions of the base models, it uses another model‚Äîa **meta-learner**‚Äîto learn the best way to combine them. Typically, the base models are diverse (e.g., a mix of SVMs, decision trees, and logistic regression), and the meta-learner is often a simple model like a logistic regression.\n",
    "\n",
    "The training process involves splitting the data, training the base models on one part, and then training the meta-learner on the predictions made by the base models on the other part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacking Process](images\png\stacking_process.png)"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-lab'></a>\n",
    "## 5. Code Lab: Comparing Ensemble Techniques\n",
    "\n",
    "Let's compare the performance of Bagging, Boosting, and Stacking on a sample classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Comparing Ensemble Methods\")\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 1. Bagging Classifier ---\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
    "note(f\"Bagging Accuracy: {bagging_acc:.4f}\")\n",
    "\n",
    "# --- 2. Boosting Classifier (AdaBoost) ---\n",
    "boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
    "boosting.fit(X_train, y_train)\n",
    "boosting_acc = accuracy_score(y_test, boosting.predict(X_test))\n",
    "note(f\"Boosting (AdaBoost) Accuracy: {boosting_acc:.4f}\")\n",
    "\n",
    "# --- 3. Stacking Classifier ---\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "stacking = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "stacking.fit(X_train, y_train)\n",
    "stacking_acc = accuracy_score(y_test, stacking.predict(X_test))\n",
    "note(f\"Stacking Accuracy: {stacking_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 6. Summary\n",
    "\n",
    "Ensemble methods are a cornerstone of modern machine learning. They provide a powerful framework for building high-performance models by combining the strengths of multiple, simpler models.\n",
    "- **Bagging** is an excellent choice for reducing the variance of complex models (like deep decision trees).\n",
    "- **Boosting** is highly effective at building accurate models from simple, high-bias learners.\n",
    "- **Stacking** offers the most flexibility, allowing you to combine fundamentally different types of models to capture a wider range of patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
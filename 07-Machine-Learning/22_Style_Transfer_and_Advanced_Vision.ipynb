{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing import image as kp_image\n",
    "    from tensorflow.keras.applications import vgg19\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "note(\"Environment initialized for Neural Style Transfer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Specialized Topics & Publishing\n",
    "## Chapter 9.04: Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Decomposing and Recombining Images\n",
    "\n",
    "**Neural Style Transfer** is a fascinating application of deep learning that allows us to recompose one image in the style of another. The technique, introduced by Gatys, Ecker, and Bethge (2015), uses the feature representations learned by a deep convolutional neural network (CNN) to separate the *content* of an image from its *style*. The core idea is to define a loss function that drives a generated image to simultaneously match the content of a **content image** and the style of a **style image**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Theory: Content and Style Loss\n",
    "The process involves optimizing a generated image by minimizing a total loss function that is a weighted sum of two components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Content Loss\n",
    "The **content** of an image is captured by the feature maps in the *upper* layers of a CNN. These layers learn to identify high-level objects and their arrangement, ignoring fine-grained details. The content loss is simply the mean squared error between the feature maps of the content image and the generated image in a specific high-level layer.\n",
    "$$ \\mathcal{L}_{content} = \\frac{1}{2} \\sum_{i,j} (F_{ij}^l - C_{ij}^l)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Style Loss\n",
    "The **style** of an image is captured by the correlations between the activations of different filters in the *lower* layers of a CNN. These correlations represent textures, colors, and common patterns. The **Gram matrix** is a tool to measure these correlations. For a layer's feature map, the Gram matrix is the inner product of the vectorized feature maps. The style loss is then the squared difference between the Gram matrices of the style image and the generated image, summed across several layers.\n",
    "$$ \\mathcal{L}_{style} = \\sum_{l} w_l E_l \\quad \\text{where} \\quad E_l = \\frac{1}{4 N_l^2 M_l^2} \\sum_{i,j} (G_{ij}^l - A_{ij}^l)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation with TensorFlow\n",
    "We will use a pre-trained VGG19 network to extract the necessary features and then optimize an input image to minimize the combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Step 1: Load and Preprocess Images\")\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    def load_img(path_to_img):\n",
    "        max_dim = 512\n",
    "        img = tf.io.read_file(path_to_img)\n",
    "        img = tf.image.decode_image(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "        long_dim = max(shape)\n",
    "        scale = max_dim / long_dim\n",
    "        new_shape = tf.cast(shape * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_shape)\n",
    "        img = img[tf.newaxis, :]\n",
    "        return img\n",
    "\n",
    "    def imshow(image, title=None):\n",
    "        out = np.squeeze(image, axis=0)\n",
    "        out = out.clip(0, 1)\n",
    "        plt.imshow(out); plt.axis('off')\n",
    "        if title: plt.title(title)\n",
    "\n",
    "    content_path = tf.keras.utils.get_file('Labrador.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "    style_path = tf.keras.utils.get_file('Kandinsky.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
    "    content_image = load_img(content_path)\n",
    "    style_image = load_img(style_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1); imshow(content_image, 'Content Image')\n",
    "    plt.subplot(1, 2, 2); imshow(style_image, 'Style Image')\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"TensorFlow not available. Skipping Style Transfer example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define Content and Style Layers\n",
    "We select intermediate layers from the VGG19 network. Lower layers capture fine textures and details (style), while higher layers capture high-level object features (content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    num_content_layers = len(content_layers)\n",
    "    num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build the Feature Extractor Model\n",
    "We create a custom Keras model that takes an image and returns the feature map activations from the specified content and style layers of the VGG19 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    def vgg_layers(layer_names):\n",
    "        vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "        outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "        return tf.keras.Model([vgg.input], outputs)\n",
    "\n",
    "    def gram_matrix(input_tensor):\n",
    "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "        num_locations = tf.cast(tf.shape(input_tensor)[1]*tf.shape(input_tensor)[2], tf.float32)\n",
    "        return result / num_locations\n",
    "\n",
    "    class StyleContentModel(tf.keras.models.Model):\n",
    "        def __init__(self, style_layers, content_layers):\n",
    "            super(StyleContentModel, self).__init__()\n",
    "            self.vgg = vgg_layers(style_layers + content_layers)\n",
    "            self.style_layers = style_layers\n",
    "            self.content_layers = content_layers\n",
    "            self.num_style_layers = len(style_layers)\n",
    "            self.vgg.trainable = False\n",
    "\n",
    "        def call(self, inputs):\n",
    "            inputs = inputs * 255.0\n",
    "            preprocessed_input = vgg19.preprocess_input(inputs)\n",
    "            outputs = self.vgg(preprocessed_input)\n",
    "            style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n",
    "            style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
    "            content_dict = {name: value for name, value in zip(self.content_layers, content_outputs)}\n",
    "            style_dict = {name: value for name, value in zip(self.style_layers, style_outputs)}\n",
    "            return {'content': content_dict, 'style': style_dict}\n",
    "\n",
    "    extractor = StyleContentModel(style_layers, content_layers)\n",
    "    style_targets = extractor(style_image)['style']\n",
    "    content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Define and Run the Training Step\n",
    "We define the total loss and use an Adam optimizer to minimize it. Crucially, the variables being updated are not the weights of a network, but the pixels of the input image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    style_weight=1e-2\n",
    "    content_weight=1e4\n",
    "    \n",
    "    def style_content_loss(outputs):\n",
    "        style_outputs = outputs['style']\n",
    "        content_outputs = outputs['content']\n",
    "        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n",
    "        style_loss *= style_weight / num_style_layers\n",
    "        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n",
    "        content_loss *= content_weight / num_content_layers\n",
    "        return style_loss + content_loss\n",
    "\n",
    "    image = tf.Variable(content_image)\n",
    "    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(image):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = extractor(image)\n",
    "            loss = style_content_loss(outputs)\n",
    "        grad = tape.gradient(loss, image)\n",
    "        opt.apply_gradients([(grad, image)])\n",
    "        image.assign(tf.clip_by_value(image, 0.0, 1.0))\n",
    "\n",
    "    note(\"Training the style transfer model... (This will take a few minutes)\")\n",
    "    epochs = 10; steps_per_epoch = 100\n",
    "    for n in range(epochs):\n",
    "        for m in range(steps_per_epoch):\n",
    "            train_step(image)\n",
    "    note(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Final Result\")\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    imshow(content_image, 'Content Image'); ax1.set_title('Content Image')\n",
    "    imshow(style_image, 'Style Image'); ax2.set_title('Style Image')\n",
    "    imshow(image, 'Generated Image'); ax3.set_title('Generated Image')\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"TensorFlow not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Economic Applications\n",
    "While style transfer is primarily an artistic application, the underlying techniques of using pre-trained CNNs as feature extractors are highly relevant in economics. For example, researchers use features extracted from satellite imagery to measure economic activity in areas with poor data. Features like the intensity of nighttime lights, the density of buildings, or the types of roofing materials can be highly correlated with GDP, poverty, and infrastructure development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exercises\n",
    "\n",
    "1.  **The Role of Weights:** In the `style_content_loss` function, we have `style_weight` and `content_weight`. Re-run the training process with `style_weight=1e-1` and then with `style_weight=1e-3`. How does the final image change? Explain the role of these weights in the optimization process.\n",
    "\n",
    "2.  **Layer Choice:** The choice of which layers to use for content and style is a key hyperparameter. \n",
    "    a. What would happen if you used a very low-level layer (e.g., `block1_conv1`) for the content loss? \n",
    "    b. What would happen if you used only high-level layers (e.g., `block5_conv1`) for the style loss? \n",
    "    Modify the code to test one of these hypotheses.\n",
    "\n",
    "3.  **The Gram Matrix:** Explain in your own words why the Gram matrix is a good representation of style. What do the diagonal and off-diagonal elements of the Gram matrix represent in terms of the feature maps?\n",
    "\n",
    "4.  **Economic Analogy:** Consider the problem of measuring the quality of infrastructure in a developing country using satellite images. Which layers of a CNN would you use to extract features related to the *presence* of roads (content)? Which layers might be useful for measuring the *quality* or *type* of roads (style/texture)?"
   ]
  }
 ]
}
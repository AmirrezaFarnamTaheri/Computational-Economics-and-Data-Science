{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "# Purpose: Import necessary libraries for image processing, deep learning, and visualization.\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    # TensorFlow is the core library for building and running the style transfer model.\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing import image as kp_image\n",
    "    # We use a pre-trained VGG19 model as the feature extractor.\n",
    "    from tensorflow.keras.applications import vgg19\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# --- Configuration ---\n",
    "# Purpose: Standardize plotting styles and numerical output.\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-info'>üìù {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "if not TENSORFLOW_AVAILABLE: note(\"TensorFlow not found. Please run `pip install tensorflow`\")\n",
    "note(\"Environment initialized for Neural Style Transfer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Specialized Topics & Publishing\n",
    "## Chapter 9.04: Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Decomposing and Recombining Images\n",
    "\n",
    "**Neural Style Transfer** is a fascinating application of deep learning that allows us to recompose one image in the style of another. The technique, introduced by Gatys, Ecker, and Bethge (2015), uses the feature representations learned by a deep convolutional neural network (CNN) to separate the *content* of an image from its *style*. The core idea is to define a loss function that drives a generated image to simultaneously match the content of a **content image** and the style of a **style image**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Theory: Content and Style Loss\n",
    "The process involves optimizing a generated image by minimizing a total loss function that is a weighted sum of two components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Content Loss\n",
    "The **content** of an image is captured by the feature maps in the *upper* layers of a CNN. These layers learn to identify high-level objects and their arrangement, ignoring fine-grained details. The content loss is simply the mean squared error between the feature maps of the content image and the generated image in a specific high-level layer.\n",
    "$$ \\mathcal{L}_{content} = \\frac{1}{2} \\sum_{i,j} (F_{ij}^l - C_{ij}^l)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Style Loss\n",
    "The **style** of an image is captured by the correlations between the activations of different filters in the *lower* layers of a CNN. These correlations represent textures, colors, and common patterns. The **Gram matrix** is a tool to measure these correlations. For a layer's feature map, the Gram matrix is the inner product of the vectorized feature maps. The style loss is then the squared difference between the Gram matrices of the style image and the generated image, summed across several layers.\n",
    "$$ \\mathcal{L}_{style} = \\sum_{l} w_l E_l \\quad \\text{where} \\quad E_l = \\frac{1}{4 N_l^2 M_l^2} \\sum_{i,j} (G_{ij}^l - A_{ij}^l)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation with TensorFlow\n",
    "We will use a pre-trained VGG19 network to extract the necessary features and then optimize an input image to minimize the combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Step 1: Load and Preprocess Images\")\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    # Purpose: Define functions to load, resize, and display images.\n",
    "    def load_img(path_to_img):\n",
    "        # The image is loaded and resized to a maximum dimension of 512 pixels to manage memory and computation time.\n",
    "        max_dim = 512\n",
    "        img = tf.io.read_file(path_to_img)\n",
    "        img = tf.image.decode_image(img, channels=3) # Ensure 3 color channels\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32) # Convert to float32 for model input\n",
    "        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "        long_dim = max(shape)\n",
    "        scale = max_dim / long_dim\n",
    "        new_shape = tf.cast(shape * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_shape)\n",
    "        # Add a batch dimension to the image tensor.\n",
    "        img = img[tf.newaxis, :]\n",
    "        return img\n",
    "\n",
    "    def imshow(image, title=None):\n",
    "        # Helper function to display a tensor as an image.\n",
    "        out = np.squeeze(image, axis=0) # Remove the batch dimension\n",
    "        out = out.clip(0, 1) # Clip values to be in the valid [0,1] range for floats\n",
    "        plt.imshow(out); plt.axis('off')\n",
    "        if title: plt.title(title)\n",
    "\n",
    "    # Asset Localization: Using local paths instead of downloading.\n",
    "    content_path = '../images/07-Machine-Learning/Labrador.jpg'\n",
    "    style_path = '../images/07-Machine-Learning/Kandinsky.jpg'\n",
    "    content_image = load_img(content_path)\n",
    "    style_image = load_img(style_path)\n",
    "\n",
    "    # Display the initial images.\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1); imshow(content_image, 'Content Image')\n",
    "    plt.subplot(1, 2, 2); imshow(style_image, 'Style Image')\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"TensorFlow not available. Skipping Style Transfer example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define Content and Style Layers\n",
    "We select intermediate layers from the VGG19 network. Lower layers capture fine textures and details (style), while higher layers capture high-level object features (content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    # Purpose: Specify the layers from the VGG19 model to be used for content and style extraction.\n",
    "    # Content is typically extracted from deeper layers, which capture high-level object features.\n",
    "    content_layers = ['block5_conv2']\n",
    "    # Style is extracted from a range of layers, from shallow to deep, to capture textures and patterns of different scales.\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "    num_content_layers = len(content_layers)\n",
    "    num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build the Feature Extractor Model\n",
    "We create a custom Keras model that takes an image and returns the feature map activations from the specified content and style layers of the VGG19 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    # Purpose: Create the feature extractor model and define the Gram matrix calculation.\n",
    "    def vgg_layers(layer_names):\n",
    "        # This function creates a Keras model that returns the feature maps from the specified layers of a pre-trained VGG19 model.\n",
    "        vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False # We freeze the VGG weights; we only use it for feature extraction.\n",
    "        outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "        return tf.keras.Model([vgg.input], outputs)\n",
    "\n",
    "    def gram_matrix(input_tensor):\n",
    "        # The Gram matrix captures the style of an image by calculating the correlations between different filter responses.\n",
    "        # It is the inner product of the vectorized feature maps.\n",
    "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "        num_locations = tf.cast(tf.shape(input_tensor)[1]*tf.shape(input_tensor)[2], tf.float32)\n",
    "        return result / num_locations # Normalize by the number of locations.\n",
    "\n",
    "    class StyleContentModel(tf.keras.models.Model):\n",
    "        # A custom model that wraps the VGG feature extractor and computes the style (Gram matrix) and content representations.\n",
    "        def __init__(self, style_layers, content_layers):\n",
    "            super(StyleContentModel, self).__init__()\n",
    "            self.vgg = vgg_layers(style_layers + content_layers)\n",
    "            self.style_layers = style_layers\n",
    "            self.content_layers = content_layers\n",
    "            self.num_style_layers = len(style_layers)\n",
    "            self.vgg.trainable = False\n",
    "\n",
    "        def call(self, inputs):\n",
    "            # The input image is first preprocessed to match VGG19's expected format.\n",
    "            inputs = inputs * 255.0\n",
    "            preprocessed_input = vgg19.preprocess_input(inputs)\n",
    "            outputs = self.vgg(preprocessed_input)\n",
    "            style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n",
    "            # Calculate the Gram matrix for each style layer's output.\n",
    "            style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
    "            content_dict = {name: value for name, value in zip(self.content_layers, content_outputs)}\n",
    "            style_dict = {name: value for name, value in zip(self.style_layers, style_outputs)}\n",
    "            return {'content': content_dict, 'style': style_dict}\n",
    "\n",
    "    # Instantiate the extractor and compute the target style and content representations from the original images.\n",
    "    # These targets will remain constant during optimization.\n",
    "    extractor = StyleContentModel(style_layers, content_layers)\n",
    "    style_targets = extractor(style_image)['style']\n",
    "    content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Define and Run the Training Step\n",
    "We define the total loss and use an Adam optimizer to minimize it. Crucially, the variables being updated are not the weights of a network, but the pixels of the input image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    # --- Optimization Setup ---\n",
    "    # We start with the content image and iteratively modify its pixels to match the target style.\n",
    "    image = tf.Variable(content_image)\n",
    "    # The Adam optimizer is used to perform the gradient descent.\n",
    "    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "    # --- Loss Function Definition ---\n",
    "    # These weights control the trade-off between matching the content and matching the style.\n",
    "    style_weight=1e-2\n",
    "    content_weight=1e4\n",
    "    \n",
    "    def style_content_loss(outputs):\n",
    "        style_outputs = outputs['style']\n",
    "        content_outputs = outputs['content']\n",
    "        # The style loss is the mean squared error between the target and current Gram matrices, summed across all style layers.\n",
    "        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n",
    "        style_loss *= style_weight / num_style_layers\n",
    "        # The content loss is the mean squared error between the target and current feature maps.\n",
    "        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n",
    "        content_loss *= content_weight / num_content_layers\n",
    "        # The total loss is the weighted sum of the two.\n",
    "        return style_loss + content_loss\n",
    "\n",
    "    # --- Training Step ---\n",
    "    # The @tf.function decorator compiles this function into a high-performance TensorFlow graph.\n",
    "    @tf.function()\n",
    "    def train_step(image):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the style and content representations of the current image.\n",
    "            outputs = extractor(image)\n",
    "            # Calculate the total loss.\n",
    "            loss = style_content_loss(outputs)\n",
    "        # Calculate the gradient of the loss with respect to the image pixels.\n",
    "        grad = tape.gradient(loss, image)\n",
    "        # Apply the gradients to update the image.\n",
    "        opt.apply_gradients([(grad, image)])\n",
    "        # Ensure the image pixel values remain in the valid [0, 1] range.\n",
    "        image.assign(tf.clip_by_value(image, 0.0, 1.0))\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    note(\"Training the style transfer model... (This will take a few minutes)\")\n",
    "    epochs = 10; steps_per_epoch = 100\n",
    "    start_time = time.time()\n",
    "    for n in range(epochs):\n",
    "        for m in range(steps_per_epoch):\n",
    "            train_step(image)\n",
    "        print(f'Epoch {n+1}/{epochs} complete.')\n",
    "    end_time = time.time()\n",
    "    note(f\"Training complete in {end_time - start_time:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Final Result\")\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    # Purpose: Display the original and final generated images for comparison.\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 3, 1); imshow(content_image, 'Content Image')\n",
    "    plt.subplot(1, 3, 2); imshow(style_image, 'Style Image')\n",
    "    plt.subplot(1, 3, 3); imshow(image, 'Generated Image')\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"TensorFlow not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Economic Applications\n",
    "While style transfer is primarily an artistic application, the underlying techniques of using pre-trained CNNs as feature extractors are highly relevant in economics. For example, researchers use features extracted from satellite imagery to measure economic activity in areas with poor data. Features like the intensity of nighttime lights, the density of buildings, or the types of roofing materials can be highly correlated with GDP, poverty, and infrastructure development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exercises\n",
    "\n",
    "1.  **The Role of Weights:** In the `style_content_loss` function, we have `style_weight` and `content_weight`. Re-run the training process with `style_weight=1e-1` and then with `style_weight=1e-3`. How does the final image change? Explain the role of these weights in the optimization process.\n",
    "\n",
    "2.  **Layer Choice:** The choice of which layers to use for content and style is a key hyperparameter. \n",
    "    a. What would happen if you used a very low-level layer (e.g., `block1_conv1`) for the content loss? \n",
    "    b. What would happen if you used only high-level layers (e.g., `block5_conv1`) for the style loss? \n",
    "    Modify the code to test one of these hypotheses.\n",
    "\n",
    "3.  **The Gram Matrix:** Explain in your own words why the Gram matrix is a good representation of style. What do the diagonal and off-diagonal elements of the Gram matrix represent in terms of the feature maps?\n",
    "\n",
    "4.  **Economic Analogy:** Consider the problem of measuring the quality of infrastructure in a developing country using satellite images. Which layers of a CNN would you use to extract features related to the *presence* of roads (content)? Which layers might be useful for measuring the *quality* or *type* of roads (style/texture)?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

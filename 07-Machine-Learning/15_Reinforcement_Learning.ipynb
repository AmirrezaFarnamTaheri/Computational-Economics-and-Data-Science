{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    DEEP_RL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DEEP_RL_AVAILABLE = False\n",
    "from collections import deque\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg): display(Markdown(f\"<div class='alert alert-block alert-info'>\ud83d\udcdd **Note:** {msg}</div>\"))\n",
    "def sec(title): print(f\"\\n{80*'='}\\n| {title.upper()} |\\n{80*'='}\")\n",
    "\n",
    "if not DEEP_RL_AVAILABLE: note(\"TensorFlow and/or Gymnasium not installed. Skipping Deep RL code labs.\")\n",
    "note(f\"Environment initialized. Deep RL libraries available: {DEEP_RL_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.15: Reinforcement Learning: Learning from Interaction\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Introduction: Learning Goal-Directed Behavior**](#intro)\n",
    "2.  [**The Mathematical Framework: Markov Decision Processes (MDPs)**](#mdps)\n",
    "    - [The Bellman Optimality Equations](#bellman)\n",
    "3.  [**Model-Free Learning: Q-Learning**](#q-learning)\n",
    "    - [Code Lab: Q-Learning for a Grid World](#code-q-learning)\n",
    "4.  [**Deep Reinforcement Learning: Deep Q-Networks (DQN)**](#dqn)\n",
    "    - [Code Lab: DQN for CartPole](#code-dqn)\n",
    "5.  [**Policy Gradient Methods**](#policy-gradient)\n",
    "    - [The Policy Gradient Theorem: A Proof Sketch](#pg-theorem)\n",
    "6.  [**Actor-Critic Methods**](#actor-critic)\n",
    "7.  [**Case Study: Optimal Resource Extraction**](#casestudy)\n",
    "8.  [**Exercises**](#exercises)\n",
    "9.  [**Summary and Key Takeaways**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: Learning Goal-Directed Behavior\n",
    "\n",
    "**Reinforcement Learning (RL)** is a computational approach to learning goal-directed behavior through **interaction**. It stands as one of the three primary paradigms of machine learning, but its philosophy is distinct. An RL **agent**, situated within an **environment**, is not given a static dataset but must learn by doing, generating its own data through trial and error. Its goal is to learn a behavioral strategy, or **policy**, that maximizes a cumulative reward signal over time.\n",
    "\n",
    "The connection to economics is deep and fundamental. RL is, in essence, the computational implementation of **dynamic programming** and **optimal control theory**. The agent's problem is precisely the problem faced by an economic agent maximizing a lifetime utility function. The **Bellman equation**, the cornerstone of dynamic programming, is also the cornerstone of reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot = graphviz.Digraph(comment='RL Loop')\n",
    "dot.attr('node', shape='box', style='rounded', fontname='Helvetica', fontsize='14')\n",
    "dot.attr('edge', fontname='Helvetica', fontsize='12')\n",
    "dot.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "dot.node('Agent', 'Agent', height='1.2', width='2.2')\n",
    "dot.node('Env', 'Environment', height='1.2', width='2.2')\n",
    "\n",
    "dot.edge('Agent', 'Env', label=' Action (A_t) ')\n",
    "dot.edge('Env', 'Agent', label=' State (S_{t+1})\\nReward (R_{t+1}) ')\n",
    "\n",
    "dot.graph_attr['label'] = '\\nFigure 1: The Agent-Environment Interaction Loop'\n",
    "dot.graph_attr['labelloc'] = 't'\n",
    "dot.graph_attr['fontsize'] = '18'\n",
    "\n",
    "display(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mdps'></a>\n",
    "## 2. The Mathematical Framework: Markov Decision Processes (MDPs)\n",
    "\n",
    "To formalize the problem of sequential decision-making, we use the framework of a **Markov Decision Process (MDP)**. An MDP is defined by a tuple ($S, A, P, R, \\gamma$):\n",
    "\n",
    "- **$S$ - The State Space**: The set of all possible states.\n",
    "- **$A$ - The Action Space**: The set of all possible actions.\n",
    "- **$P$ - The Transition Model**: $P(s' | s, a)$, the probability of transitioning to state $s'$ given the current state $s$ and action $a$.\n",
    "- **$R$ - The Reward Function**: $R(s, a, s')$, the immediate reward received.\n",
    "- **$\\gamma$ - The Discount Factor**: $\\gamma \\in [0, 1)$, which discounts future rewards.\n",
    "\n",
    "The agent's goal is to learn a **policy** $\\pi(a|s)$ that maximizes the expected **return** (the discounted sum of future rewards, $G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$). To do this, it learns one of two **value functions**:\n",
    "\n",
    "1.  **State-Value Function ($V^\\pi(s)$)**: The expected return starting from state $s$ and then following policy $\\pi$.\n",
    "2.  **Action-Value Function ($Q^\\pi(s, a)$)**: The expected return from taking action $a$ in state $s$, and thereafter following policy $\\pi$. This is the central object of value-based RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bellman'></a>\n",
    "### 2.1 The Bellman Optimality Equations\n",
    "\n",
    "The value functions for the optimal policy, $\\pi^*$, must satisfy the **Bellman Optimality Equations**. These equations decompose the value of a state or state-action pair into the immediate reward and the discounted value of the *optimal* successor state. They are the foundation for nearly all RL algorithms.\n",
    "\n",
    "$$ V^*(s) = \\max_{a} E[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t=s, A_t=a] $$ \n",
    "$$ Q^*(s, a) = E[R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') | S_t=s, A_t=a] $$ \n",
    "If we know the transition dynamics $P$ and reward function $R$, we can solve these equations using dynamic programming methods like **value iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q-learning'></a>\n",
    "## 3. Model-Free Learning: Q-Learning\n",
    "\n",
    "In most interesting problems, the agent does not know the model of the environment ($P$ and $R$ are unknown). This is **model-free** reinforcement learning. The agent must learn the optimal policy purely from trial-and-error interaction.\n",
    "\n",
    "**Q-Learning** is the canonical model-free, value-based algorithm. It directly learns an estimate of the optimal action-value function, $Q^*(s,a)$, from experience. It uses a **temporal-difference (TD)** update, which updates the Q-value for a state-action pair based on the reward received and the estimated value of the *next* state.\n",
    "\n",
    "The update rule for the Q-value of the state-action pair $(s, a)$ experienced at time $t$ is:\n",
    "$$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( \\underbrace{r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a')}_{\\text{TD Target}} - Q(s_t, a_t) \\right) $$ \n",
    "Where $\\alpha$ is the learning rate. Q-learning is an **off-policy** algorithm because the update rule uses the `max` operator, meaning it updates its estimate based on the value of the *optimal* action in the next state, not necessarily the action that was actually taken by the current (potentially exploratory) policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-q-learning'></a>\n",
    "sec(\"Code Lab: Q-Learning to Solve a Grid World\")\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid = np.zeros((5, 5)); self.pos = [0, 0]\n",
    "        self.grid[4, 4] = 10\n",
    "        self.actions = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "    def step(self, action):\n",
    "        move = self.actions[action]\n",
    "        self.pos[0] = np.clip(self.pos[0] + move[0], 0, 4)\n",
    "        self.pos[1] = np.clip(self.pos[1] + move[1], 0, 4)\n",
    "        reward = self.grid[tuple(self.pos)]\n",
    "        return tuple(self.pos), reward\n",
    "    def reset(self): self.pos = [0, 0]; return tuple(self.pos)\n",
    "\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    q_table = np.zeros((5, 5, 4))\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state]) if np.random.random() > epsilon else np.random.choice(4)\n",
    "            next_state, reward = env.step(action)\n",
    "            q_table[state][action] = q_table[state][action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state][action])\n",
    "            state = next_state\n",
    "            if reward > 0: done = True\n",
    "    return q_table\n",
    "\n",
    "env = GridWorld()\n",
    "q_table = q_learning(env)\n",
    "optimal_policy = np.argmax(q_table, axis=2)\n",
    "plt.imshow(optimal_policy, cmap='viridis')\n",
    "plt.title('Figure 2: Learned Optimal Policy (0:U, 1:D, 2:L, 3:R)')\n",
    "plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dqn'></a>\n",
    "## 4. Deep Reinforcement Learning: Deep Q-Networks (DQN)\n",
    "\n",
    "The Q-learning approach with a lookup table works for small, discrete state spaces. For problems with large or continuous state spaces (e.g., controlling a robot from pixel inputs), we cannot store a Q-value for every state. **Deep Q-Networks (DQN)**, developed by DeepMind, solve this by using a deep neural network to approximate the action-value function: $Q(s, a; \\theta) \\approx Q^*(s, a)$. The network takes the state $s$ as input and outputs a Q-value for each possible action $a$.\n",
    "\n",
    "Training a Q-network with the standard Q-learning update is notoriously unstable. DQN introduced two key innovations to stabilize training:\n",
    "1.  **Experience Replay:** The agent stores its experiences $(s, a, r, s')$ in a large **replay buffer**. During training, it samples random mini-batches from this buffer. This breaks the strong temporal correlation between consecutive samples, making the training data more like the i.i.d. data that neural networks are designed for.\n",
    "2.  **Target Network:** The TD target, $r + \\gamma \\max_{a'} Q(s', a')$, creates a moving target problem because the same network is used to predict the current Q-value and the target Q-value. DQN solves this by using a separate **target network** to calculate the TD target. The target network is a periodically updated copy of the main policy network, providing a more stable target for the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='code-dqn'></a>\n",
    "sec(\"Code Lab: Deep Q-Network (DQN) for CartPole\")\n",
    "if DEEP_RL_AVAILABLE:\n",
    "    class DQNAgent:\n",
    "        def __init__(self, state_size, action_size):\n",
    "            self.state_size, self.action_size = state_size, action_size\n",
    "            self.memory = deque(maxlen=2000)\n",
    "            self.gamma, self.epsilon = 0.95, 1.0\n",
    "            self.epsilon_decay, self.epsilon_min = 0.995, 0.01\n",
    "            self.model = self._build_model()\n",
    "            self.target_model = self._build_model()\n",
    "            self.update_target_model()\n",
    "        def _build_model(self):\n",
    "            model = keras.Sequential([layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "                                      layers.Dense(24, activation='relu'),\n",
    "                                      layers.Dense(self.action_size, activation='linear')])\n",
    "            model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "            return model\n",
    "        def update_target_model(self): self.target_model.set_weights(self.model.get_weights())\n",
    "        def remember(self, s, a, r, s_next, d): self.memory.append((s, a, r, s_next, d))\n",
    "        def act(self, state):\n",
    "            if np.random.rand() <= self.epsilon: return random.randrange(self.action_size)\n",
    "            return np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "        def replay(self, batch_size):\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                target = self.model.predict(state, verbose=0)\n",
    "                if done: target[0][action] = reward\n",
    "                else: target[0][action] = reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=0)[0])\n",
    "                self.model.fit(state, target, epochs=1, verbose=0)\n",
    "            if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    env = gym.make('CartPole-v1'); state_size = env.observation_space.shape[0]; action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size); episodes = 100; batch_size = 32; rewards = []\n",
    "    note(f\"Running DQN training for {episodes} episodes...\")\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()[0]; state = np.reshape(state, [1, state_size]); total_reward = 0\n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward; next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done); state = next_state\n",
    "            if done: agent.update_target_model(); break\n",
    "        rewards.append(total_reward)\n",
    "        if len(agent.memory) > batch_size: agent.replay(batch_size)\n",
    "        if e % 20 == 0: print(f\"Episode: {e}/{episodes}, Score: {total_reward}, Epsilon: {agent.epsilon:.2}\")\n",
    "            \n",
    "    plt.figure(figsize=(12, 7)); plt.plot(rewards); plt.plot(pd.Series(rewards).rolling(10).mean(), label='10-episode MA')\n",
    "    plt.title('DQN Training Progress on CartPole'); plt.xlabel('Episode'); plt.ylabel('Total Reward'); plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    note(\"Deep RL libraries not available. Skipping DQN example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='policy-gradient'></a>\n",
    "## 5. Policy Gradient Methods\n",
    "\n",
    "An alternative to value-based methods like Q-learning is the family of **policy gradient methods**. Instead of learning a value function and deriving a policy from it, these methods learn a **parameterized policy** directly, $\\pi(a|s; \\theta)$. The goal is to update the policy parameters $\\theta$ by performing gradient ascent on an objective function $J(\\theta)$ that measures the expected return.\n",
    "\n",
    "<a id='pg-theorem'></a>\n",
    "### 5.1 The Policy Gradient Theorem: A Proof Sketch\n",
    "\n",
    "The **Policy Gradient Theorem** provides a surprisingly simple expression for the gradient of the performance objective, linking it to the policy and the action-value function.\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = E_\\pi [\\nabla_\\theta \\log \\pi(A_t|S_t; \\theta) Q^\\pi(S_t, A_t)] $$ \n",
    "\n",
    "**Proof Sketch:**\n",
    "1. The objective is the expected return: $J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$, where $\\tau$ is a trajectory.\n",
    "2. The gradient is $\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau) R(\\tau) d\\tau$.\n",
    "3. Use the **log-derivative trick**: $\\nabla_x \\log f(x) = \\frac{\\nabla_x f(x)}{f(x)}$, so $\\nabla_x f(x) = f(x) \\nabla_x \\log f(x)$.\n",
    "4. Apply this: $\\nabla_\\theta J(\\theta) = \\int p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) R(\\tau) d\\tau = E_{\\tau \\sim \\pi_\\theta}[\\nabla_\\theta \\log p_\\theta(\\tau) R(\\tau)]$.\n",
    "5. The probability of a trajectory is $p_\\theta(\\tau) = p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)$.\n",
    "6. The log-probability is $\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{T-1} (\\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t))$.\n",
    "7. The gradient $\\nabla_\\theta$ only affects the policy term: $\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$.\n",
    "8. Substituting back into the expectation and using the definition of $Q^\\pi$ gives the final result.\n",
    "\n",
    "The **REINFORCE** algorithm is the simplest implementation of this theorem. It runs an episode, calculates the actual return $G_t$ at each step (as a noisy estimate of $Q^\\pi$), and updates the policy parameters in the direction that makes actions leading to high returns more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='actor-critic'></a>\n",
    "## 6. Actor-Critic Methods\n",
    "\n",
    "Policy gradient methods can have high variance because the return $G_t$ is a noisy estimate. **Actor-Critic** methods reduce this variance by learning a value function alongside the policy. \n",
    "- The **Actor** is the parameterized policy, which decides which action to take.\n",
    "- The **Critic** is a learned value function (e.g., a neural network) that estimates the Q-value of the action taken. \n",
    "\n",
    "The critic's estimate of Q is used to update the actor's policy parameters, providing a lower-variance and more stable learning signal. This hybrid approach underpins many state-of-the-art RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='../images/07-Machine-Learning/actor_critic_architecture.webp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='casestudy'></a>\n",
    "## 7. Case Study: Optimal Resource Extraction (The Fish Pond Problem)\n",
    "\n",
    "To connect RL directly to economic principles, we can apply Q-learning to solve a classic dynamic optimization problem: the management of a renewable resource. We will model a simple fishery where our agent must decide how many fish to harvest each period to maximize the long-run discounted profit.\n",
    "\n",
    "**The Economic Model as an MDP:**\n",
    "- **State ($s_t$):** The current fish stock (biomass).\n",
    "- **Action ($a_t$):** The quantity of fish to harvest.\n",
    "- **Reward ($r_{t+1}$):** The profit from the harvest, e.g., $p \\cdot a_t$, where $p$ is the price of fish.\n",
    "- **Transition:** The fish stock follows a logistic growth function, less the amount harvested. $s_{t+1} = (s_t - a_t) + g(s_t - a_t)$, where $g(\\cdot)$ is the growth function.\n",
    "\n",
    "The agent faces a trade-off: harvesting more today yields higher immediate profits but depletes the stock, potentially reducing future profits. The Q-learning algorithm will learn an optimal policy that balances this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Case Study: Q-Learning for Optimal Harvesting\")\n",
    "\n",
    "class FisheryEnv:\n",
    "    def __init__(self, k=100, r=0.1, price=1):\n",
    "        self.k, self.r, self.price = k, r, price\n",
    "        self.max_stock = k\n",
    "        self.state = self.max_stock\n",
    "    def reset(self): self.state = self.max_stock; return int(self.state)\n",
    "    def step(self, action):\n",
    "        harvest = min(action, self.state)\n",
    "        stock_after_harvest = self.state - harvest\n",
    "        growth = self.r * stock_after_harvest * (1 - stock_after_harvest / self.k)\n",
    "        self.state = min(self.max_stock, stock_after_harvest + growth)\n",
    "        reward = self.price * harvest\n",
    "        return int(self.state), reward\n",
    "\n",
    "def q_learning_fishery(env, episodes=20000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    n_states = env.max_stock + 1\n",
    "    n_actions = env.max_stock // 5 + 1\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        for t in range(50):\n",
    "            action_idx = np.argmax(q_table[state]) if np.random.random() > epsilon else np.random.choice(n_actions)\n",
    "            harvest_amount = action_idx * 5\n",
    "            next_state, reward = env.step(harvest_amount)\n",
    "            q_table[state, action_idx] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action_idx])\n",
    "            state = next_state\n",
    "    return q_table\n",
    "\n",
    "note(\"Solving for the optimal harvesting policy using Q-learning...\")\n",
    "fish_env = FisheryEnv()\n",
    "q_table_fish = q_learning_fishery(fish_env)\n",
    "optimal_policy_fish = np.argmax(q_table_fish, axis=1) * 5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.suptitle('Figure 4: Optimal Fishery Management via Q-Learning', fontsize=18, y=1.02)\n",
    "im = ax1.imshow(q_table_fish.T, cmap='viridis', aspect='auto', origin='lower')\n",
    "ax1.set_title('a) Learned Q-Values'); ax1.set_xlabel('Current Fish Stock'); ax1.set_ylabel('Harvest Amount')\n",
    "fig.colorbar(im, ax=ax1, label='Expected Future Reward')\n",
    "ax2.plot(range(len(optimal_policy_fish)), optimal_policy_fish, '-o')\n",
    "ax2.set_title('b) Learned Optimal Harvesting Policy'); ax2.set_xlabel('Current Fish Stock'); ax2.set_ylabel('Optimal Harvest Amount')\n",
    "plt.show()\n",
    "note(\"The Q-learning agent learns a sensible policy. When the fish stock is low, it harvests very little to allow the stock to recover. When the stock is high, it harvests a larger amount, maintaining a sustainable equilibrium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\\n",
    "## 8. Exercises\\n",
    "\\n",
    "1.  **The Bellman Equation:** Write out the Bellman optimality equation for $Q^*(s,a)$ in words. What does it say about the relationship between the value of taking an action now and the value of the best action in the future?\\n",
    "2.  **Exploration vs. Exploitation:** In the Q-learning algorithm, what is the role of the $\\epsilon$ parameter in the $\\epsilon$-greedy policy? What would happen if $\\epsilon=0$? What if $\\epsilon=1$?\\n",
    "3.  **On-Policy vs. Off-Policy:** Q-learning is an off-policy algorithm. What does this mean? Why is it considered off-policy, and what is the practical advantage of this?\\n",
    "4.  **Optimal Harvesting:** In the fish pond case study, how would you expect the optimal harvesting policy to change if the discount factor $\\gamma$ were increased (i.e., the agent becomes more patient)? What if the fish growth rate was higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\\n",
    "## 9. Summary and Key Takeaways\\n",
    "\\n",
    "This chapter introduced Reinforcement Learning, a powerful framework for solving sequential decision-making problems that is deeply connected to the economic theory of dynamic programming.\\n",
    "\\n",
    "**Key Concepts**:\\n",
    "- **The RL Problem**: An **agent** learns to take **actions** in an **environment** to maximize a cumulative, discounted **reward**. This is formalized as a **Markov Decision Process (MDP)**.\\n",
    "- **Value Functions & Bellman Equations**: The core of value-based RL is estimating value functions ($V^\\pi(s)$ or $Q^\\pi(s,a)$) that satisfy the Bellman equations, which provide a recursive definition of value.\\n",
    "- **Q-Learning**: A classic model-free, off-policy algorithm that directly learns the optimal action-value function, $Q^*(s,a)$, using temporal-difference updates.\\n",
    "- **Deep Q-Networks (DQN)**: A major breakthrough that uses deep neural networks to approximate the Q-function, enabling RL to solve high-dimensional problems. It relies on **experience replay** and a **target network** for stable training.\\n",
    "- **Policy Gradient Methods**: An alternative approach that directly optimizes a parameterized policy $\\pi(a|s;\\theta)$ by performing gradient ascent on the expected return.\\n",
    "- **Actor-Critic Methods**: A hybrid approach that combines the strengths of value-based and policy-based methods, forming the basis for most state-of-the-art RL agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**1. The Bellman Equation:**\\n",
    "The Bellman optimality equation for $Q^*(s,a)$ says: \"The value of taking action *a* in state *s* and then acting optimally forever after is equal to the expected immediate reward you get, plus the discounted value of the *best possible action* you can take from the next state you land in.\" It provides the fundamental recursive relationship that links the value of the current state-action pair to the values of all possible next state-action pairs.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**2. Exploration vs. Exploitation:**\\n",
    "The $\\epsilon$ parameter controls the trade-off between exploration and exploitation. \\n",
    "- **If $\\epsilon=0$**: The agent always chooses the action with the highest current estimated Q-value. This is pure **exploitation**. The agent never tries new actions and can get stuck in a suboptimal policy if its initial Q-value estimates are poor.\\n",
    "- **If $\\epsilon=1$**: The agent always chooses a random action, regardless of its Q-value estimates. This is pure **exploration**. The agent never uses what it has learned and will behave randomly.\\n",
    "A small, non-zero $\\epsilon$ (that often decays over time) ensures the agent mostly exploits its knowledge but occasionally explores to discover better policies.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**3. On-Policy vs. Off-Policy:**\\n",
    "An **on-policy** algorithm updates its policy based on actions taken by that same policy. A **off-policy** algorithm can learn about the optimal policy while following a different, more exploratory policy. Q-learning is off-policy because its update rule involves the term $\\max_{a'} Q(s_{t+1}, a')$. This term finds the value of the *best* possible action in the next state, regardless of what action the exploratory $\\epsilon$-greedy policy might actually choose. The practical advantage is that it allows the agent to explore its environment widely while still learning a deterministic, optimal policy.\\n",
    "\\n",
    "---\\n",
    "\\n",
    "**4. Optimal Harvesting:**\\n",
    "- **Higher $\\gamma$**: A higher discount factor means the agent is more patient and values the future more. The agent would learn to harvest *less* in the present to ensure the fish stock remains high, leading to larger, sustainable harvests in the future. The optimal policy would be more conservative.\\n",
    "- **Higher Growth Rate**: If the fish stock replenishes more quickly, the agent can afford to harvest more aggressively without depleting the stock. The optimal policy would be to harvest a larger quantity at each step, as the resource is less scarce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
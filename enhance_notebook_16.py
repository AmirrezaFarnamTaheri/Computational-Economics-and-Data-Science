
import json

def enhance_notebook_16(notebook_path):
    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = json.load(f)

    for cell in notebook['cells']:
        if cell['cell_type'] == 'code':
            source_code = ''.join(cell['source'])
            if 'display(Image(filename=\'../images/png/dueling_dqn_architecture.png\'))' in source_code:
                cell['source'] = [
                    "# This diagram illustrates the Dueling DQN architecture.\\n",
                    "# The network splits into two streams: one for the state-value function (V) and one for the advantage function (A).\\n",
                    "# These streams are then combined to produce the final Q-values.\\n",
                    "display(Image(filename='../images/07-Machine-Learning/dueling_dqn_architecture.png'))"
                ]
            elif 'class ActorCritic(nn.Module):' in source_code:
                cell['source'] = [
                    "sec(\"Implementing Advantage Actor-Critic (A2C)\")\\n",
                    "\\n",
                    "if not GYM_AVAILABLE:\\n",
                    "    note(\"Gymnasium not installed. Skipping this code lab.\")\\n",
                    "else:\\n",
                    "    # 1. Define the Actor-Critic Network\\n",
                    "    # This network has a shared body and two heads: one for the policy (Actor) and one for the value function (Critic).\\n",
                    "    class ActorCritic(nn.Module):\\n",
                    "        def __init__(self, input_dims, n_actions):\\n",
                    "            super(ActorCritic, self).__init__()\\n",
                    "            # Shared layers learn common features from the input state.\\n",
                    "            self.shared_layer = nn.Sequential(\\n",
                    "                nn.Linear(input_dims, 128),\\n",
                    "                nn.ReLU()\\n",
                    "            )\\n",
                    "            # The Actor head outputs a probability distribution over actions (the policy).\\n",
                    "            self.actor_head = nn.Linear(128, n_actions)\\n",
                    "            # The Critic head outputs a single value, estimating the value of the current state.\\n",
                    "            self.critic_head = nn.Linear(128, 1)\\n",
                    "\\n",
                    "        def forward(self, state):\\n",
                    "            shared_features = self.shared_layer(state)\\n",
                    "            action_logits = self.actor_head(shared_features)\\n",
                    "            state_value = self.critic_head(shared_features)\\n",
                    "            # The softmax function converts the logits into a probability distribution.\\n",
                    "            return F.softmax(action_logits, dim=-1), state_value\\n",
                    "\\n",
                    "    # 2. A2C Agent Training Loop\\n",
                    "    def train_a2c(env, episodes=1000, gamma=0.99):\\n",
                    "        input_dims = env.observation_space.shape[0]\\n",
                    "        n_actions = env.action_space.n\\n",
                    "        model = ActorCritic(input_dims, n_actions).to(device)\\n",
                    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\\n",
                    "        \\n",
                    "        episode_rewards = []\\n",
                    "\\n",
                    "        for episode in range(episodes):\\n",
                    "            log_probs = []\\n",
                    "            values = []\\n",
                    "            rewards = []\\n",
                    "            \\n",
                    "            state, _ = env.reset()\\n",
                    "            done = False\\n",
                    "            ep_reward = 0\\n",
                    "\\n",
                    "            # Collect one complete trajectory from the environment.\\n",
                    "            while not done:\\n",
                    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\\n",
                    "                probs, value = model(state_tensor)\\n",
                    "                \\n",
                    "                # Sample an action from the policy distribution.\\n",
                    "                dist = torch.distributions.Categorical(probs)\\n",
                    "                action = dist.sample()\\n",
                    "                \\n",
                    "                next_state, reward, terminated, truncated, _ = env.step(action.cpu().item())\\n",
                    "                done = terminated or truncated\\n",
                    "                \\n",
                    "                log_probs.append(dist.log_prob(action))\\n",
                    "                values.append(value)\\n",
                    "                rewards.append(reward)\\n",
                    "                state = next_state\\n",
                    "                ep_reward += reward\\n",
                    "            \\n",
                    "            episode_rewards.append(ep_reward)\\n",
                    "            \\n",
                    "            # Calculate the discounted returns for each step in the trajectory.\\n",
                    "            returns = []\\n",
                    "            R = 0\\n",
                    "            for r in reversed(rewards):\\n",
                    "                R = r + gamma * R\\n",
                    "                returns.insert(0, R)\\n",
                    "            returns = torch.tensor(returns).to(device)\\n",
                    "            \\n",
                    "            log_probs = torch.cat(log_probs)\\n",
                    "            values = torch.cat(values).squeeze()\\n",
                    "            \\n",
                    "            # Calculate the advantage: how much better was the return than the critic's estimate?\\n",
                    "            advantage = returns - values\\n",
                    "            \\n",
                    "            # The actor loss encourages actions that led to a positive advantage.\\n",
                    "            actor_loss = -(log_probs * advantage.detach()).mean()\\n",
                    "            # The critic loss trains the value function to be a better predictor of the returns.\\n",
                    "            critic_loss = F.mse_loss(returns, values)\\n",
                    "            total_loss = actor_loss + 0.5 * critic_loss\\n",
                    "            \\n",
                    "            optimizer.zero_grad()\\n",
                    "            total_loss.backward()\\n",
                    "            optimizer.step()\\n",
                    "\\n",
                    "            if (episode + 1) % 100 == 0:\\n",
                    "                avg_reward = np.mean(episode_rewards[-100:])\\n",
                    "                print(f\"Episode {episode+1}/{episodes} | Average Reward (last 100): {avg_reward:.2f}\")\\n",
                    "                if avg_reward >= 475:\\n",
                    "                    print(\"\\nEnvironment solved!\")\\n",
                    "                    break\\n",
                    "        return episode_rewards\\n",
                    "\\n",
                    "    # 3. Run the training\\n",
                    "    env = gym.make('CartPole-v1')\\n",
                    "    rewards_history = train_a2c(env, episodes=200)\\n",
                    "    note(\"A2C training complete.\")\\n",
                    "\\n",
                    "    plt.figure(figsize=(12, 7))\\n",
                    "    plt.plot(rewards_history, label='Reward per Episode')\\n",
                    "    plt.plot(pd.Series(rewards_history).rolling(100).mean(), label='100-episode average', lw=3)\\n",
                    "    plt.title('A2C Training on CartPole-v1', fontsize=16)\\n",
                    "    plt.xlabel('Episode'); plt.ylabel('Total Reward')\\n",
                    "    plt.legend(); plt.grid(True)\\n",
                    "    plt.show()"
                ]

    with open(notebook_path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=2)

if __name__ == '__main__':
    enhance_notebook_16('07-Machine-Learning/16_Advanced_Deep_RL.ipynb')

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm, chi2\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Image, display, Markdown\n",
    "from econometrics_utils import MLEstimator # Import from new utility file\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (11, 7), 'figure.dpi': 130})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized for Maximum Likelihood Estimation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6.2: Maximum Likelihood Estimation\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Introduction: The Principle of Maximum Likelihood**](#intro)\n",
    "    - [Biographical Note: Sir Ronald A. Fisher](#fisher)\n",
    "2.  [**The Likelihood and Log-Likelihood Functions**](#likelihood)\n",
    "3.  [**The Geometry of the Log-Likelihood Function**](#geometry)\n",
    "    - [The Score Vector](#score)\n",
    "    - [The Fisher Information Matrix](#info)\n",
    "4.  [**Asymptotic Properties of the MLE**](#asymptotics)\n",
    "    - [Consistency, Asymptotic Normality, and Efficiency](#properties)\n",
    "    - [The Information Matrix Equality](#equality)\n",
    "    - [Regularity Conditions](#regularity)\n",
    "5.  [**Numerical Optimization and Implementation**](#numerical)\n",
    "    - [A Reusable `MLEstimator` Class](#mle-class)\n",
    "    - [Visualizing the Log-Likelihood Surface](#surface)\n",
    "6.  [**Hypothesis Testing: The Holy Trinity**](#trinity)\n",
    "7.  [**Advanced Topics: QMLE and Model Misspecification**](#qmle)\n",
    "8.  [**Application: Poisson Regression for Count Data**](#poisson)\n",
    "9.  [**Exercises**](#exercises)\n",
    "10. [**Summary and Key Takeaways**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "After completing this chapter, you will be able to:\n",
    "\n",
    "- **Understand and articulate** the intuitive and formal principle of Maximum Likelihood Estimation.\n",
    "- **Formulate** the likelihood and log-likelihood functions for various probability distributions.\n",
    "- **Explain** the key geometric concepts of the log-likelihood function: the score, the Hessian, and the Fisher Information Matrix.\n",
    "- **Describe** the desirable large-sample properties of the MLE: consistency, asymptotic normality, and efficiency.\n",
    "- **Implement** MLE for custom models in Python, both from scratch and using object-oriented tools.\n",
    "- **Calculate and interpret** standard errors derived from the information matrix.\n",
    "- **Distinguish between** and apply the three classical hypothesis tests: Wald, Likelihood Ratio (LR), and Lagrange Multiplier (LM).\n",
    "- **Define** Quasi-Maximum Likelihood Estimation (QMLE) and understand when it provides consistent estimates even under model misspecification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: The Principle of Maximum Likelihood\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** is arguably the most important and widely used principle for parametric estimation in econometrics and statistics. It provides a general, systematic, and powerful method for finding high-quality estimators in a vast range of models, from simple linear regression to complex dynamic structural models.\n",
    "\n",
    "The core idea, pioneered by the legendary statistician Sir Ronald A. Fisher, is to reverse the standard probability question. Instead of asking, *\"Given a model with known parameters, what is the probability of observing our data?\"* MLE asks a more practical and profound question:\n",
    "\n",
    "> *Given the data we have actually observed, what values of the model's parameters would make this dataset the **most likely** or **most probable** outcome?*\n",
    "\n",
    "The parameter values that answer this question—those that maximize the **Likelihood Function**—form the Maximum Likelihood Estimator ($\\hat{\\theta}_{MLE}$). This principle is both intuitively appealing and, as we will see, theoretically justified by its excellent statistical properties in large samples.\n",
    "\n",
    "This chapter explores the theory and practice of MLE. We will build from the foundational concepts of the likelihood function to the geometric intuition of the score and information matrix, explore the asymptotic properties that make MLE so popular, and implement it in code to solve real-world econometric problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fisher'></a>\n",
    "### Biographical Note: Sir Ronald A. Fisher (1890-1962)\n",
    "\n",
    "<img src='images\jpg\RA_Fisher.jpg' width='200' align='right' style='margin-left: 20px; border: 1px solid #ccc; padding: 3px;'/>\n",
    "\n",
    "Sir Ronald A. Fisher was a British statistician, evolutionary biologist, and geneticist who is often called \"the single most important figure in 20th century statistics.\" His contributions laid the foundation for much of modern statistical theory and practice. Between 1912 and 1922, Fisher developed the conceptual framework for **Maximum Likelihood Estimation**, introducing the terms \"likelihood,\" \"statistic,\" and \"parameter.\" He rigorously analyzed its properties, including its **consistency**, **efficiency**, and the concept of **sufficiency**.\n",
    "\n",
    "Beyond MLE, Fisher was a pioneer in the design of experiments, invented the Analysis of Variance (ANOVA), and made fundamental contributions to population genetics. His work provides the theoretical backbone for the statistical methods used across nearly all scientific disciplines today. The concepts we explore in this chapter—likelihood, information, and efficiency—are central pillars of his intellectual legacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood'></a>\n",
    "## 2. The Likelihood and Log-Likelihood Functions\n",
    "\n",
    "Suppose we have a sample of $n$ independently and identically distributed (i.i.d.) observations, $\\mathbf{y} = (y_1, y_2, ..., y_n)$, drawn from a population characterized by a probability density function (PDF) or probability mass function (PMF) $f(y_i; \\theta)$, which depends on a vector of $k$ unknown parameters, $\\theta$.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Definition: The Likelihood Function**\n",
    "\n",
    "The **Likelihood Function**, denoted $L(\\theta | \\mathbf{y})$, is the joint probability distribution of the observed data, viewed as a function of the parameters $\\theta$. For an i.i.d. sample, it is the product of the individual density functions:\n",
    "\n",
    "$$ L(\\theta | \\mathbf{y}) = \\prod_{i=1}^n f(y_i; \\theta) $$\n",
    "    \n",
    "</div>\n",
    "\n",
    "Maximizing the likelihood function directly can be computationally difficult due to the product form. Taking the natural logarithm transforms the product into a sum, which is much easier to differentiate. This gives us the **Log-Likelihood Function**.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Definition: The Log-Likelihood Function**\n",
    "\n",
    "The **Log-Likelihood Function**, denoted $\\mathcal{L}(\\theta | \\mathbf{y})$, is the natural logarithm of the likelihood function:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta | \\mathbf{y}) = \\ln L(\\theta | \\mathbf{y}) = \\sum_{i=1}^n \\ln f(y_i; \\theta) $$\n",
    "\n",
    "Since the logarithm is a strictly increasing function, the value of $\\theta$ that maximizes $L(\\theta | \\mathbf{y})$ also maximizes $\\mathcal{L}(\\theta | \\mathbf{y})$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='geometry'></a>\n",
    "## 3. The Geometry of the Log-Likelihood Function\n",
    "\n",
    "Understanding the *shape* of the log-likelihood surface is fundamental to grasping the mechanics and properties of MLE. The key features of this geometry are captured by its first and second derivatives: the **Score** and the **Hessian** (related to the Information Matrix).\n",
    "\n",
    "<a id='score'></a>\n",
    "### The Score Vector\n",
    "\n",
    "The first derivative of the log-likelihood function with respect to the parameters is a vector of partial derivatives known as the **score vector**.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Definition: The Score Vector**\n",
    "\n",
    "The **Score Vector**, $\\mathbf{s}(\\theta)$, is the gradient of the log-likelihood function:\n",
    "\n",
    "$$ \\mathbf{s}(\\theta) = \\frac{\\partial \\mathcal{L}(\\theta | \\mathbf{y})}{\\partial \\theta} = \\sum_{i=1}^n \\frac{\\partial \\ln f(y_i; \\theta)}{\\partial \\theta} $$\n",
    "\n",
    "</div>\n",
    "\n",
    "The score vector tells us the steepness of the log-likelihood surface at any given point $\\theta$. At the MLE, $\\hat{\\theta}$, we are at the peak of the surface, so the gradient must be zero. This gives us the first-order condition for maximization:\n",
    "\n",
    "$$ \\mathbf{s}(\\hat{\\theta}) = \\mathbf{0} $$\n",
    "\n",
    "An important property, which we will prove later, is that the *expected value* of the score vector (evaluated at the true parameter $\\theta_0$) is zero: $E[\\mathbf{s}(\\theta_0)] = \\mathbf{0}$.\n",
    "\n",
    "<a id='info'></a>\n",
    "### The Fisher Information Matrix\n",
    "\n",
    "The second derivative, or curvature, of the log-likelihood function is captured by the **Hessian matrix**. The negative of its expectation is the Fisher Information Matrix, a concept of central importance.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Definition: The Fisher Information Matrix**\n",
    "\n",
    "The **Fisher Information Matrix**, $I(\\theta)$, is the negative of the expected value of the Hessian matrix of the log-likelihood function:\n",
    "\n",
    "$$ I(\\theta) = -E\\left[ \\frac{\\partial^2 \\mathcal{L}(\\theta | \\mathbf{y})}{\\partial \\theta \\partial \\theta'} \\right] = -E[\\mathbf{H}(\\theta)]$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "The Fisher Information Matrix measures the curvature of the log-likelihood surface at the true parameter value $\\theta_0$. Intuitively, it quantifies the amount of \"information\" the data provides about the parameters.\n",
    "- A **\"sharp\" peak** (high curvature) implies a large value for $I(\\theta)$. This means the maximum is well-defined and the data is highly informative, leading to precise estimates (low variance).\n",
    "- A **\"flat\" surface** (low curvature) implies a small value for $I(\\theta)$. This means the maximum is hard to locate and the data is less informative, leading to imprecise estimates (high variance).\n",
    "\n",
    "As we will see, the inverse of the Fisher Information Matrix, $I(\\theta_0)^{-1}$, gives us the asymptotic variance of the MLE, a cornerstone result for statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"The Geometry of the Log-Likelihood Function\")\n",
    "# Simple example: MLE for the probability 'p' of a Bernoulli trial\n",
    "data_bern = [1, 1, 0, 1, 1, 0, 1, 1, 1, 1] # 8 successes in 10 trials\n",
    "log_likelihood = lambda p: np.sum(data_bern) * np.log(p) + np.sum(1-np.array(data_bern)) * np.log(1-p)\n",
    "\n",
    "p_grid = np.linspace(0.01, 0.99, 200)\n",
    "ll_vals = log_likelihood(p_grid)\n",
    "mle_p = np.mean(data_bern)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(p_grid, ll_vals, lw=2.5, label='Log-Likelihood $\\mathcal{L}(p)$')\n",
    "ax.axvline(mle_p, color='r', ls='--', label=f'MLE $\\hat{{p}}={mle_p:.2f}$')\n",
    "\n",
    "# Illustrate the Score (slope)\n",
    "p_test = 0.5\n",
    "ll_test = log_likelihood(p_test)\n",
    "score_test = (np.sum(data_bern)/p_test) - (np.sum(1-np.array(data_bern))/(1-p_test))\n",
    "ax.plot(p_test, ll_test, 'go', ms=10)\n",
    "ax.plot(p_grid, ll_test + score_test * (p_grid - p_test), 'g--', label='Score (Slope) at p=0.5')\n",
    "\n",
    "ax.set_title('Log-Likelihood Function for a Bernoulli Process')\n",
    "ax.set_xlabel('Parameter p (Probability of Success)')\n",
    "ax.set_ylabel('Log-Likelihood Value')\n",
    "ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='numerical'></a>\n",
    "## 5. Numerical Optimization and Implementation\n",
    "\n",
    "Except for a few simple cases, the first-order condition $\\mathbf{s}(\\hat{\\theta}) = \\mathbf{0}$ does not have a closed-form, analytical solution. We must therefore find the maximum of the log-likelihood function using numerical optimization algorithms. These algorithms start with an initial guess for the parameters, $\\theta_0$, and iteratively update it until they converge to a value $\\hat{\\theta}$ that maximizes the function.\n",
    "\n",
    "Many of these algorithms use the geometric properties of the log-likelihood surface—the score and the Hessian—to guide their search. For example, the **Newton-Raphson** algorithm uses the following update rule:\n",
    "\n",
    "$$ \\theta_{k+1} = \\theta_k - [\\mathbf{H}(\\theta_k)]^{-1} \\mathbf{s}(\\theta_k) $$\n",
    "\n",
    "This rule essentially takes a step in the direction of the score, with the step size adjusted by the curvature (Hessian). Near the maximum, this converges very quickly. However, calculating the Hessian can be computationally expensive. **Quasi-Newton** methods like **BFGS** (Broyden–Fletcher–Goldfarb–Shanno), which is the default for `scipy.optimize.minimize`, are often preferred. They work by building up an *approximation* to the Hessian at each step, making them more efficient for many problems.\n",
    "\n",
    "<a id='mle-class'></a>\n",
    "### 5.1 A Reusable `MLEstimator` Class\n",
    "\n",
    "To translate theory into practice and promote reusable code, we have constructed a general-purpose `MLEstimator` class in the `econometrics_utils.py` file. This class abstracts away the details of the numerical optimization and standard error calculation, allowing us to focus on defining the model's log-likelihood function. It handles:\n",
    "\n",
    "- Calling `scipy.optimize.minimize` to find the MLE.\n",
    "- Calculating the variance-covariance matrix from the optimizer's estimate of the inverse Hessian.\n",
    "- Presenting the results in a clear, summary table.\n",
    "\n",
    "Let's demonstrate its use by estimating the parameters of a normal distribution.\n",
    "\n",
    "#### Example: Estimating Parameters of a Normal Distribution\n",
    "We'll start with a simple problem: estimating the mean ($\\\\mu$) and standard deviation ($\\\\sigma$) of a normal distribution from a sample of data. The log-likelihood function for a normal distribution is:\n",
    "$$ \\mathcal{L}(\\mu, \\sigma | \\mathbf{y}) = \\sum_{i=1}^n \\ln \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y_i - \\mu)^2}{2\\sigma^2}} \\right) = -\\frac{n}{2}\\ln(2\\pi) - n\\ln(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\mu)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Example: MLE for a Normal Distribution\")\n",
    "\n",
    "# 1. Generate some sample data\n",
    "rng = np.random.default_rng(seed=123)\n",
    "true_mu, true_sigma = 5.0, 2.0\n",
    "sample_data = rng.normal(true_mu, true_sigma, size=1000)\n",
    "\n",
    "# 2. Define the log-likelihood function for the optimizer\n",
    "# Note: The optimizer minimizes, so we return the *negative* log-likelihood\n",
    "def neg_loglike_normal(params, data):\n",
    "    mu, sigma = params[0], params[1]\n",
    "    if sigma <= 0: return np.inf # Return a large number for invalid params\n",
    "    log_likelihood = np.sum(norm.logpdf(data, loc=mu, scale=sigma))\n",
    "    return -log_likelihood\n",
    "\n",
    "# 3. Create an instance of our estimator and fit the model\n",
    "note(\"Estimating the mean and standard deviation of a Normal distribution using our MLEstimator class.\")\n",
    "mle_norm = MLEstimator(neg_loglike_normal, sample_data, param_names=['mu', 'sigma'])\n",
    "mle_norm.fit(start_params=[0.0, 1.0])\n",
    "mle_norm.summary()\n",
    "\n",
    "note(f\"The true parameters were mu={true_mu} and sigma={true_sigma}. Our estimates are very close!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='asymptotics'></a>\n",
    "## 4. Asymptotic Properties and Inference\n",
    "\n",
    "MLE is the default estimation method in many fields because its estimators possess several desirable large-sample (asymptotic) properties. These properties ensure that with enough data, our estimates will be close to the true values and will have a known distribution, which is essential for hypothesis testing. These properties, however, depend on a set of **regularity conditions** being met.\n",
    "\n",
    "<a id='properties'></a>\n",
    "### 4.1 Consistency, Asymptotic Normality, and Efficiency\n",
    "\n",
    "Under the regularity conditions, the MLE $\\hat{\\theta}$ has the following properties:\n",
    "\n",
    "1.  **Consistency:** As the sample size $N \\to \\infty$, the estimator converges in probability to the true parameter value $\\theta_0$.\n",
    "    $$ \\hat{\\theta} \\xrightarrow{p} \\theta_0 $$\n",
    "    *Intuition: With enough data, the log-likelihood function's peak will be located at the true parameter values.*\n",
    "\n",
    "2.  **Asymptotic Normality:** As the sample size grows, the distribution of the MLE converges to a normal distribution centered at the true parameter, with a variance given by the inverse of the Fisher Information matrix.\n",
    "    $$ \\sqrt{N}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, I(\\theta_0)^{-1}) $$\n",
    "    *Intuition: This property is what allows us to conduct standard hypothesis testing (like z-tests and Wald tests), as it gives us the sampling distribution of our estimator.*\n",
    "\n",
    "3.  **Asymptotic Efficiency:** The MLE achieves the lowest possible variance among all consistent and asymptotically normal estimators. It reaches the **Cramér-Rao Lower Bound**, which is the theoretical minimum variance an unbiased estimator can have.\n",
    "    *Intuition: No other estimator can give you more precise estimates (in large samples). This is why MLE is often the gold standard.*\n",
    "\n",
    "<a id='regularity'></a>\n",
    "### 4.2 Regularity Conditions (For the Advanced Reader)\n",
    "\n",
    "The beautiful asymptotic properties of MLE are not automatic; they rely on the log-likelihood function being \"well-behaved.\" These requirements are known as **regularity conditions**. While the specific details are highly technical, they generally require the following:\n",
    "\n",
    "1.  **Parameter Space:** The parameter space $\\Theta$ must be a compact set. *Intuition: This prevents the MLE from diverging to infinity.*\n",
    "2.  **Continuity and Differentiability:** The log-likelihood function $\\mathcal{L}(\\theta | \\mathbf{y})$ must be continuous and twice continuously differentiable with respect to $\\theta$ in the interior of the parameter space. *Intuition: This ensures the surface is smooth enough to have a well-defined peak, score, and Hessian.*\n",
    "3.  **Identification:** The true parameter $\\theta_0$ must be a unique maximizer of the expected log-likelihood, $E[\\mathcal{L}(\\theta | \\mathbf{y})]$. That is, for any $\\theta \\neq \\theta_0$, $E[\\mathcal{L}(\\theta_0)] > E[\\mathcal{L}(\\theta)]$. *Intuition: If different parameter values produce the same likelihood, the model is not identified; we cannot distinguish between them.*\n",
    "4.  **Finite Moments:** The score, Hessian, and their products must have finite expected values. *Intuition: This prevents the key quantities used in proofs from becoming infinite.*\n",
    "5.  **Domination:** The derivatives of the log-likelihood must be bounded by some integrable function. *Intuition: This technical condition allows us to interchange the order of differentiation and integration, which is critical for proving results like the Information Matrix Equality.*\n",
    "\n",
    "In most well-specified econometric models, these conditions hold, but it is crucial to be aware that they are the theoretical foundation upon which MLE stands.\n",
    "\n",
    "<a id='equality'></a>\n",
    "### 4.3 The Information Matrix Equality\n",
    "\n",
    "A foundational result in MLE theory is the **Information Matrix Equality**. It states that under the regularity conditions, the Fisher Information matrix is equal to the variance of the score vector. This provides two equivalent ways to think about and calculate the information content of the data.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Theorem: Information Matrix Equality**\n",
    "\n",
    "$$ I(\\theta) = -E[\\mathbf{H}(\\theta)] = E[\\mathbf{s}(\\theta)\\mathbf{s}(\\theta)'] = Var[\\mathbf{s}(\\theta)] $$\n",
    "where $\\mathbf{s}(\\theta)$ is the score and $\\mathbf{H}(\\theta)$ is the Hessian of the log-likelihood.\n",
    "    \n",
    "</div>\n",
    "\n",
    "**Proof Sketch:**\n",
    "The proof relies on the fact that for a correctly specified model, the integral of the likelihood function over all possible data outcomes is 1. Let $L(\\theta|y)$ be the likelihood.\n",
    "\n",
    "1.  **Identity:** Start with the identity $\\int L(\\theta|y) dy = 1$.\n",
    "2.  **First Derivative:** Differentiate with respect to $\\theta$. The regularity conditions allow us to pass the derivative inside the integral: $\\int \\frac{\\partial L}{\\partial \\theta} dy = 0$. \n",
    "3.  **Rewrite as Expectation:** Multiply and divide by $L$ to get $\\int (\\frac{1}{L}\\frac{\\partial L}{\\partial \\theta}) L dy = \\int \\frac{\\partial \\ln L}{\\partial \\theta} L dy = E[\\mathbf{s}(\\theta)] = 0$. The expected value of the score is zero.\n",
    "4.  **Second Derivative:** Differentiate the result from step 2 again with respect to $\\theta'$: $\\int \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'} dy = 0$.\n",
    "5.  **Hessian of Log-Likelihood:** Find the Hessian of $\\mathcal{L} = \\ln L$ using the quotient rule for differentiation:\n",
    "    $$ \\mathbf{H}(\\theta) = \\frac{\\partial^2 \\ln L}{\\partial \\theta \\partial \\theta'} = \\frac{1}{L} \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'} - \\frac{1}{L^2} \\frac{\\partial L}{\\partial \\theta} \\frac{\\partial L}{\\partial \\theta'} = \\frac{1}{L} \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'} - \\mathbf{s}(\\theta)\\mathbf{s}(\\theta)'$$\n",
    "6.  **Take Expectations:** Take the expectation of the Hessian:\n",
    "    $$ E[\\mathbf{H}(\\theta)] = E\\left[\\frac{1}{L} \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'}\\right] - E[\\mathbf{s}(\\theta)\\mathbf{s}(\\theta)'] $$ \n",
    "    The first term is $\\int (\\frac{1}{L} \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'}) L dy = \\int \\frac{\\partial^2 L}{\\partial \\theta \\partial \\theta'} dy$, which we know from step 4 is zero.\n",
    "7.  **Final Result:** This leaves $E[\\mathbf{H}(\\theta)] = -E[\\mathbf{s}(\\theta)\\mathbf{s}(\\theta)']$. Since $E[\\mathbf{s}(\\theta)]=0$, $Var[\\mathbf{s}(\\theta)] = E[\\mathbf{s}(\\theta)\\mathbf{s}(\\theta)']$. The Fisher Information is defined as $I(\\theta) = -E[\\mathbf{H}(\\theta)]$. Substituting this in gives the equality.\n",
    "\n",
    "This result is profound: the average curvature of the log-likelihood (Hessian) is directly related to the variance of its slope (score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='surface'></a>\n",
    "### 5.2 Visualizing the Log-Likelihood Surface\n",
    "\n",
    "To build more intuition, we can visualize the log-likelihood surface for the two-parameter normal distribution problem. The goal of the optimizer is to find the coordinates ($\\\\mu$, $\\\\sigma$) of the highest point on this surface (or the lowest point of the negative log-likelihood surface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"3D Visualization of the Log-Likelihood Surface\")\n",
    "\n",
    "# Create a grid of mu and sigma values around the MLE\n",
    "mu_mle, sigma_mle = mle_norm.mle_params\n",
    "mu_grid = np.linspace(mu_mle - 1, mu_mle + 1, 100)\n",
    "sigma_grid = np.linspace(sigma_mle - 1, sigma_mle + 1, 100)\n",
    "MU, SIGMA = np.meshgrid(mu_grid, sigma_grid)\n",
    "NEG_LL_surface = np.zeros_like(MU)\n",
    "\n",
    "# Calculate the negative log-likelihood for each point on the grid\n",
    "for i in range(MU.shape[0]):\n",
    "    for j in range(MU.shape[1]):\n",
    "        # Ensure sigma is positive before calculating\n",
    "        if SIGMA[i, j] > 0:\n",
    "            NEG_LL_surface[i, j] = neg_loglike_normal([MU[i, j], SIGMA[i, j]], sample_data)\n",
    "        else:\n",
    "            NEG_LL_surface[i, j] = np.inf\n",
    "\n",
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(MU, SIGMA, -NEG_LL_surface, cmap=cm.viridis, alpha=0.8, rstride=5, cstride=5)\n",
    "\n",
    "# Mark the MLE point\n",
    "ax.scatter(mu_mle, sigma_mle, -mle_norm.max_loglike, color='red', s=200, \n",
    "           marker='*', label=f'MLE ($\\hat{{\\mu}}={mu_mle:.2f}, \\hat{{\\sigma}}={sigma_mle:.2f}$)')\n",
    "\n",
    "ax.set_xlabel('$\\mu$')\n",
    "ax.set_ylabel('$\\sigma$')\n",
    "ax.set_zlabel('Log-Likelihood')\n",
    "ax.set_title('Log-Likelihood Surface for a Normal Distribution')\n",
    "ax.view_init(elev=30, azim=-60)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trinity'></a>\n",
    "## 6. Hypothesis Testing: The 'Holy Trinity'\n",
    "\n",
    "The three classical hypothesis tests—the **Wald test**, the **Likelihood Ratio (LR) test**, and the **Lagrange Multiplier (LM) test** (also known as the score test)—are deeply connected to the geometry of the log-likelihood function. All three test the same hypothesis but measure the \"distance\" between the restricted and unrestricted models in different ways.\n",
    "\n",
    "Consider testing a set of $q$ linear restrictions, $H_0: R\\theta = r$.\n",
    "\n",
    "1.  **Wald Test:** This test starts at the unrestricted MLE ($\\hat{\\theta}_{UR}$) and asks: \"How far is the unrestricted estimate from satisfying the restriction, scaled by its standard error?\" It measures the distance in the parameter space and only requires estimating the unrestricted model.\n",
    "\n",
    "2.  **Likelihood Ratio (LR) Test:** This test asks: \"How large is the drop in the log-likelihood value when we impose the restriction?\" It measures the vertical distance on the likelihood surface between the unrestricted model ($\\mathcal{L}_{UR}$) and the restricted model ($\\mathcal{L}_{R}$). It requires estimating *both* models.\n",
    "\n",
    "3.  **Lagrange Multiplier (LM) Test:** This test starts at the restricted MLE ($\\hat{\\theta}_{R}$) and asks: \"How steep is the slope (the score) of the unrestricted log-likelihood function at this point?\" If the restriction is true, the restricted estimate should be close to the peak, and the score should be near zero. It only requires estimating the restricted model.\n",
    "\n",
    "Asymptotically, all three tests are equivalent and their test statistics follow a $\\chi^2_q$ distribution, where $q$ is the number of restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='poisson'></a>\n",
    "### Application: Poisson Regression for Count Data\n",
    "\n",
    "Let's apply our `MLEstimator` to a model that is non-linear in its parameters and cannot be estimated by OLS: the **Poisson Regression Model**. This model is used for **count data**, where the dependent variable $y_i$ is a non-negative integer (e.g., number of patents, number of children, number of visits to a doctor).\n",
    "\n",
    "The model assumes that $y_i$ follows a Poisson distribution, where the mean of the distribution, $\\lambda_i$, depends on a set of explanatory variables $\\mathbf{x}_i$. The standard assumption is an exponential mean function to ensure $\\lambda_i$ is always positive:\n",
    "\n",
    "$$ E[y_i | \\mathbf{x}_i] = \\lambda_i = \\exp(\\mathbf{x}_i'\\beta) $$\n",
    "\n",
    "The PMF for a Poisson distribution is $P(Y=y_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}$. The individual log-likelihood contribution for observation $i$ is therefore:\n",
    "\n",
    "$$ \\ln f(y_i; \\beta) = -\\lambda_i + y_i \\ln(\\lambda_i) - \\ln(y_i!) = -\\exp(\\mathbf{x}_i'\\beta) + y_i(\\mathbf{x}_i'\\beta) - \\ln(y_i!) $$\n",
    "\n",
    "The full log-likelihood is the sum over all observations: $\\mathcal{L}(\\beta) = \\sum_{i=1}^n [-\\exp(\\mathbf{x}_i'\\beta) + y_i(\\mathbf{x}_i'\\beta) - \\ln(y_i!)]$. We can find the MLE for $\\beta$ by maximizing this function numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Application: Poisson Regression for Firm Patent Counts\")\n",
    "\n",
    "# 1. Generate synthetic data for firm patents\n",
    "note(\"We simulate data where the number of patents filed by a firm depends on its R&D spending and whether it's in the high-tech industry.\")\n",
    "rng = np.random.default_rng(seed=999)\n",
    "N = 500\n",
    "\n",
    "# X variables: constant, log of R&D spending, and a dummy for high-tech industry\n",
    "log_rd_spending = rng.uniform(2, 10, N)\n",
    "is_high_tech = rng.binomial(1, 0.4, N)\n",
    "X = np.vstack([np.ones(N), log_rd_spending, is_high_tech]).T\n",
    "\n",
    "# True parameters\n",
    "true_beta = np.array([-1.5, 0.8, 1.2]) # (Intercept, Beta_RD, Beta_HighTech)\n",
    "\n",
    "# Generate the mean lambda for each firm\n",
    "lambda_i = np.exp(X @ true_beta)\n",
    "\n",
    "# Generate the actual patent counts from a Poisson distribution\n",
    "patents = rng.poisson(lambda_i)\n",
    "\n",
    "print(\"Sample of the generated data:\")\n",
    "df_patents = pd.DataFrame({'Patents': patents, 'Log_RD_Spending': log_rd_spending, 'Is_High_Tech': is_high_tech})\n",
    "display(df_patents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Poisson Model with MLE\n",
    "\n",
    "Now we define the negative log-likelihood function for the Poisson model and use our `MLEstimator` to find the coefficients. Note that the term $\\ln(y_i!)$ can be dropped from the function to be optimized, as it does not depend on the parameters $\\beta$ and therefore does not affect the location of the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the negative log-likelihood function\n",
    "def neg_loglike_poisson(beta, y, X):\n",
    "    lambda_i = np.exp(X @ beta)\n",
    "    log_likelihood = np.sum(-lambda_i + y * np.log(lambda_i))\n",
    "    return -log_likelihood\n",
    "\n",
    "# 3. Combine data and fit the model\n",
    "data_poisson = {'y': patents, 'X': X}\n",
    "param_names = ['Intercept', 'Log_RD_Spending', 'Is_High_Tech']\n",
    "\n",
    "note(\"Estimating the Poisson model using our MLEstimator class.\")\n",
    "mle_poisson = MLEstimator(neg_loglike_poisson, data_poisson, param_names=param_names)\n",
    "mle_poisson.fit(start_params=[0, 0, 0])\n",
    "mle_poisson.summary()\n",
    "\n",
    "note(f\"The true parameters were {true_beta}. Our MLE estimates are quite close.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the Results\n",
    "\n",
    "The coefficients in a Poisson regression model are interpreted in terms of their effect on the *log of the expected count*. \n",
    "\n",
    "- **`Log_RD_Spending` ($\\\\hat{\\beta}_1 \\approx 0.82$)**: For a 1% increase in R&D spending, the expected number of patents increases by approximately 0.82%. This is because $\\frac{\\partial \\ln(\\lambda)}{\\partial \\ln(RD)} = \\beta_{RD}$.\n",
    "- **`Is_High_Tech` ($\\\\hat{\\beta}_2 \\approx 1.22$)**: Being in a high-tech industry increases the log of the expected patent count by 1.22, holding R&D spending constant. This means that, on average, high-tech firms are expected to have $\\exp(1.22) \\approx 3.4$ times more patents than non-high-tech firms with the same R&D spending.\n",
    "\n",
    "This example demonstrates the power and flexibility of MLE: by simply defining a new log-likelihood function, we can estimate a sophisticated, non-linear model tailored to a specific type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing the 'Holy Trinity': Wald, LR, and LM\")\n",
    "\n",
    "# --- Setup: Generate data where the null hypothesis (H0: beta_2 = 0) is true ---\n",
    "note(\"We will test the hypothesis H0: $\\beta_2 = 0$ in a Probit model. The data is generated such that the null is true, so we expect high p-values.\")\n",
    "rng = np.random.default_rng(seed=456)\n",
    "N = 2500\n",
    "X_unrestricted = sm.add_constant(rng.standard_normal((N, 2)), prepend=True)\n",
    "X_restricted = X_unrestricted[:, :-1] # Drop the last column (x2)\n",
    "\n",
    "true_beta_H0 = np.array([0.2, 0.8, 0.0]) # The null is true\n",
    "latent_y = X_unrestricted @ true_beta_H0 + rng.standard_normal(N)\n",
    "y = (latent_y > 0).astype(int)\n",
    "\n",
    "# --- Fit Unrestricted and Restricted Models ---\n",
    "model_unrestricted = sm.Probit(y, X_unrestricted)\n",
    "res_unrestricted = model_unrestricted.fit(disp=0)\n",
    "\n",
    "model_restricted = sm.Probit(y, X_restricted)\n",
    "res_restricted = model_restricted.fit(disp=0)\n",
    "\n",
    "# --- 1. Likelihood Ratio (LR) Test ---\n",
    "lr_stat = 2 * (res_unrestricted.llf - res_restricted.llf)\n",
    "lr_pval = chi2.sf(lr_stat, df=1)\n",
    "\n",
    "# --- 2. Wald Test ---\n",
    "wald_test = res_unrestricted.wald_test('x2 = 0')\n",
    "wald_stat = wald_test.statistic[0,0]\n",
    "wald_pval = wald_test.pvalue\n",
    "\n",
    "# --- 3. Lagrange Multiplier (LM) / Score Test ---\n",
    "beta_restricted_padded = np.append(res_restricted.params, 0)\n",
    "score_vec = model_unrestricted.score(beta_restricted_padded)\n",
    "hess_mat = model_unrestricted.hessian(beta_restricted_padded)\n",
    "lm_stat = score_vec.T @ np.linalg.inv(-hess_mat) @ score_vec\n",
    "lm_pval = chi2.sf(lm_stat, df=1)\n",
    "\n",
    "# --- Print Results ---\n",
    "trinity_results = pd.DataFrame({\n",
    "    'Statistic': [lr_stat, wald_stat, lm_stat],\n",
    "    'P-Value': [lr_pval, wald_pval, lm_pval]\n",
    "}, index=['Likelihood Ratio (LR)', 'Wald', 'Lagrange Multiplier (LM)'])\n",
    "note(\"Asymptotically, all three test statistics follow a Chi-squared(1) distribution and should be numerically similar.\")\n",
    "display(trinity_results.round(4))\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Create a grid for b1 and b2 to plot the log-likelihood surface\n",
    "b1_mle, b2_mle = res_unrestricted.params[1], res_unrestricted.params[2]\n",
    "b1_grid = np.linspace(b1_mle - 0.4, b1_mle + 0.4, 100)\n",
    "b2_grid = np.linspace(b2_mle - 0.4, b2_mle + 0.4, 100)\n",
    "B1, B2 = np.meshgrid(b1_grid, b2_grid)\n",
    "LLF = np.zeros_like(B1)\n",
    "\n",
    "# Calculate log-likelihood for each point on the grid, holding b0 constant\n",
    "b0_mle = res_unrestricted.params[0]\n",
    "for i in range(B1.shape[0]):\n",
    "    for j in range(B1.shape[1]):\n",
    "        beta_vec = np.array([b0_mle, B1[i, j], B2[i, j]])\n",
    "        LLF[i, j] = model_unrestricted.loglike(beta_vec)\n",
    "\n",
    "# Plot the surface\n",
    "levels = np.linspace(res_unrestricted.llf - 5, res_unrestricted.llf, 20)\n",
    "contour = ax.contour(B1, B2, LLF, levels=levels, cmap='viridis')\n",
    "ax.clabel(contour, inline=1, fontsize=9)\n",
    "\n",
    "# Unrestricted estimate\n",
    "ax.plot(b1_mle, b2_mle, 'r*', markersize=20, label='Unrestricted MLE ($\\hat{\\beta}_{UR}$)')\n",
    "\n",
    "# Restricted estimate (H0: b2=0)\n",
    "b1_mle_restricted = res_restricted.params[1]\n",
    "ax.plot(b1_mle_restricted, 0, 'b*', markersize=20, label='Restricted MLE ($\\hat{\\beta}_{R}$)')\n",
    "ax.axhline(0, color='blue', linestyle='--', label='Restriction: $\\beta_2=0$')\n",
    "\n",
    "# Annotate tests\n",
    "ax.annotate(f\"LR Test = {lr_stat:.2f}\\n(diff in LLF)\", xy=(b1_mle, b2_mle/2), xytext=(b1_mle+0.1, b1_mle_restricted + 0.1),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05), color='k', fontsize=12)\n",
    "\n",
    "ax.annotate(f\"Wald Test = {wald_stat:.2f}\\n(distance from $\\hat{{\\beta}}_{{UR}}$)\", xy=(b1_mle, b2_mle), xytext=(b1_mle-0.3, b2_mle+0.1),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05), color='k', fontsize=12)\n",
    "\n",
    "ax.annotate(f\"LM Test = {lm_stat:.2f}\\n(slope at $\\hat{{\\beta}}_{{R}}$)\", xy=(b1_mle_restricted, 0), xytext=(b1_mle_restricted-0.3, -0.2),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05), color='k', fontsize=12)\n",
    "\n",
    "ax.set(xlabel='$\\beta_1$', ylabel='$\\beta_2$', title='The Holy Trinity of Tests on the Log-Likelihood Surface')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qmle'></a>\n",
    "## 7. Advanced Topic: Quasi-Maximum Likelihood Estimation (QMLE)\n",
    "\n",
    "What happens if our likelihood function is based on the wrong distributional assumption? For example, what if we use the normal log-likelihood (which gives the OLS estimator) but the true errors are not normal? In certain important cases, the estimator is still **consistent** for the parameters of the conditional mean. This is called a **Quasi-Maximum Likelihood Estimator (QMLE)**.\n",
    "\n",
    "The key requirement is that the **conditional mean function must be correctly specified**. If we correctly specify $E[y_i | \\mathbf{x}_i] = \\mathbf{x}_i'\\beta$, then the OLS estimator (the QMLE for $\\beta$) is consistent even if the distribution of $y_i$ is not actually normal.\n",
    "\n",
    "However, the second derivatives of the misspecified log-likelihood will be wrong. This means the standard errors calculated in the usual way (from the inverse of the expected Hessian) will be incorrect, invalidating hypothesis tests. To get correct standard errors for a QMLE, we must use a **robust variance-covariance matrix**, often called a \"sandwich estimator.\" The general form is:\n",
    "\n",
    "$$ Var_{QMLE}(\\hat{\\beta}) = \\mathbf{H}^{-1} \\mathbf{J} \\mathbf{H}^{-1} $$\n",
    "\n",
    "Where $\\mathbf{H}$ is the Hessian from the misspecified likelihood and $\\mathbf{J}$ is the variance of the score from the misspecified likelihood. For OLS, this simplifies to the well-known **White (or Eicker-Huber-White) robust standard errors**.\n",
    "\n",
    "### Example: OLS as QMLE with Non-Normal Errors\n",
    "\n",
    "Let's show this in action. We will generate data with Student's t-distributed errors (which have fatter tails than the normal distribution) and show that OLS still recovers the true coefficients, but that only robust standard errors are reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"QMLE Example: OLS with t-distributed Errors\")\n",
    "\n",
    "# 1. Generate data with non-normal errors\n",
    "note(\"We generate data from y = 1 + 2x + u, where u follows a Student's t-distribution with 3 degrees of freedom.\")\n",
    "rng = np.random.default_rng(seed=101)\n",
    "N = 1000\n",
    "X = sm.add_constant(rng.uniform(-5, 5, N))\n",
    "true_beta = np.array([1.0, 2.0])\n",
    "df_t = 3 # Degrees of freedom for t-distribution (fat tails)\n",
    "errors = rng.standard_t(df_t, size=N)\n",
    "y = X @ true_beta + errors\n",
    "\n",
    "# 2. Estimate using OLS (which is the QMLE)\n",
    "model_ols = sm.OLS(y, X)\n",
    "results_ols = model_ols.fit()\n",
    "\n",
    "# 3. Compare conventional and robust standard errors\n",
    "se_conventional = results_ols.bse\n",
    "se_robust = results_ols.get_robustcov_results(cov_type='HC1').bse\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'True Beta': true_beta,\n",
    "    'OLS Estimate (QMLE)': results_ols.params,\n",
    "    'Conventional SE': se_conventional,\n",
    "    'Robust SE (HC1)': se_robust\n",
    "}, index=['Intercept', 'x1'])\n",
    "\n",
    "note(\"The QMLE estimates are close to the true betas, but the conventional standard errors are smaller than the robust ones, which would lead to overconfidence and incorrect inference.\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 9. Exercises\n",
    "\n",
    "1.  **MLE of a Bernoulli Distribution:** A Bernoulli trial has two outcomes (e.g., success/failure, coded as 1/0). The probability of success (Y=1) is $p$. The PMF is $f(y; p) = p^y(1-p)^{1-y}$ for $y \\in \\{0, 1\\}$. For a sample of $n$ i.i.d. trials, write down the log-likelihood function and use calculus to derive the MLE for $p$. Does the result seem familiar?\n",
    "\n",
    "2.  **MLE of an Exponential Distribution:** The PDF of the exponential distribution, used to model durations, is $f(y; \\lambda) = \\lambda e^{-\\lambda y}$ for $y \\ge 0$, where $\\lambda > 0$ is the 'rate' parameter. For an i.i.d. sample $\\{y_1, ..., y_n\\}$, derive the MLE for $\\lambda$. How does it relate to the sample mean of $y$?\n",
    "\n",
    "3.  **Invariance Property of MLE:** A powerful property of the MLE is its *invariance*. If $\\hat{\\theta}$ is the MLE of $\\theta$, then for any function $g(\\theta)$, the MLE of $g(\\theta)$ is $g(\\hat{\\theta})$. In the exponential distribution example, the mean is $E[y] = 1/\\lambda$. Use the invariance property and your result from Exercise 2 to find the MLE of the *mean* of the distribution. Does this make intuitive sense?\n",
    "\n",
    "4.  **Score and Information for Poisson:** For the Poisson log-likelihood function (ignoring the constant $\\ln(y_i!)$ term), $\\mathcal{L}(\\beta) = \\sum_{i=1}^n [-\\exp(\\mathbf{x}_i'\\beta) + y_i(\\mathbf{x}_i'\\beta)]$, derive the score vector $\\frac{\\partial \\mathcal{L}}{\\partial \\beta}$ and the Hessian matrix $\\frac{\\partial^2 \\mathcal{L}}{\\partial \\beta \\partial \\beta'}$. Show that the expected value of the Hessian is $- \\sum_{i=1}^n \\exp(\\mathbf{x}_i'\\beta) \\mathbf{x}_i \\mathbf{x}_i'$.\n",
    "\n",
    "5.  **Logit vs. Probit:** The Logit model is another binary choice model where the probability is given by the logistic function, $P(y_i=1 | \\mathbf{X}_i) = \\frac{1}{1 + e^{-\\mathbf{X}_i'\\beta}}$. This arises from assuming the error term in the latent variable model follows a logistic distribution instead of a normal one. Repeat the \"Holy Trinity\" visualization exercise from the code lab, but for a Logit model. How do the estimated coefficients compare to the Probit coefficients? (Hint: They are not directly comparable, but their ratio should be approximately constant, around 1.6-1.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\n",
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "This chapter introduced Maximum Likelihood Estimation, the workhorse of modern parametric econometrics. It is a powerful, general principle for deriving estimators for a wide variety of statistical models.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Likelihood Principle**: We choose the parameter values that make our observed data the most probable outcome.\n",
    "- **Log-Likelihood**: For computational convenience, we maximize the log of the likelihood function, which turns products into sums.\n",
    "- **Geometry**: The MLE is found at the peak of the log-likelihood surface, where the **score** (gradient) is zero. The curvature of the surface is measured by the **Hessian**, and its expectation is related to the **Fisher Information Matrix**.\n",
    "- **Asymptotic Properties**: With sufficient data and under regularity conditions, the MLE is **consistent**, **asymptotically normal**, and **efficient** (achieving the Cramér-Rao lower bound).\n",
    "- **Inference**: The inverse of the Fisher Information Matrix provides the variance-covariance matrix of the estimator, which is the key to calculating standard errors and conducting hypothesis tests.\n",
    "- **Hypothesis Testing**: The three classical tests (Wald, LR, LM) are all geometrically-motivated ways of measuring the distance between restricted and unrestricted models on the likelihood surface.\n",
    "- **Flexibility**: The MLE framework is incredibly flexible. By specifying a new log-likelihood function—as we did for the Poisson model—we can generate estimators for highly complex and non-linear models.\n",
    "- **QMLE**: Even if the distributional assumption is wrong, MLE can still provide consistent estimates of the conditional mean parameters, provided the mean itself is correctly specified. However, robust standard errors are required for valid inference.\n",
    "\n",
    "Understanding MLE is crucial for any advanced study in econometrics, as it forms the foundation for estimating most of the models used in modern economic research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\n",
    "\n",
    "--- \n",
    "\n",
    "**1. MLE of a Bernoulli Distribution:**\n",
    "The PMF is $f(y_i; p) = p^{y_i}(1-p)^{1-y_i}$. The log-likelihood for one observation is $\\ln f(y_i; p) = y_i \\ln(p) + (1-y_i) \\ln(1-p)$.\n",
    "The total log-likelihood is $\\mathcal{L}(p) = \\sum y_i \\ln(p) + \\sum (1-y_i) \\ln(1-p) = n_1 \\ln(p) + n_0 \\ln(1-p)$, where $n_1$ is the number of successes and $n_0$ is the number of failures.\n",
    "The score is $\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{n_1}{p} - \\frac{n_0}{1-p}$.\n",
    "Setting the score to zero: $\\frac{n_1}{\\hat{p}} = \\frac{n_0}{1-\\hat{p}} \\implies n_1(1-\\hat{p}) = n_0 \\hat{p} \\implies n_1 = (n_0+n_1)\\hat{p} = n\\hat{p}$.\n",
    "Thus, $\\hat{p}_{MLE} = \\frac{n_1}{n} = \\bar{y}$. The MLE is the sample mean, which is highly intuitive.\n",
    "\n",
    "---\n",
    "\n",
    "**2. MLE of an Exponential Distribution:**\n",
    "The PDF is $f(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}$. The log-likelihood for one observation is $\\ln f(y_i; \\lambda) = \\ln(\\lambda) - \\lambda y_i$.\n",
    "The total log-likelihood is $\\mathcal{L}(\\lambda) = \\sum_{i=1}^n (\\ln(\\lambda) - \\lambda y_i) = n \\ln(\\lambda) - \\lambda \\sum y_i$.\n",
    "The score is $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\frac{n}{\\lambda} - \\sum y_i$.\n",
    "Setting the score to zero: $\\frac{n}{\\hat{\\lambda}} = \\sum y_i \\implies \\hat{\\lambda}_{MLE} = \\frac{n}{\\sum y_i} = \\frac{1}{\\bar{y}}$. The MLE of the rate parameter is the inverse of the sample mean.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Invariance Property of MLE:**\n",
    "From Exercise 2, we found $\\hat{\\lambda}_{MLE} = 1/\\bar{y}$. The mean of the exponential distribution is a function of the parameter $\\lambda$, given by $g(\\lambda) = 1/\\lambda$. \n",
    "By the invariance property, the MLE of the mean is $g(\\hat{\\lambda}_{MLE})$.\n",
    "So, $\\widehat{E[y]}_{MLE} = g(\\hat{\\lambda}_{MLE}) = \\frac{1}{\\hat{\\lambda}_{MLE}} = \\frac{1}{1/\\bar{y}} = \\bar{y}$.\n",
    "The MLE for the population mean is the sample mean. This makes perfect intuitive sense.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Score and Information for Poisson:**\n",
    "The log-likelihood is $\\mathcal{L}(\\beta) = \\sum_{i=1}^n [-\\exp(\\mathbf{x}_i'\\beta) + y_i(\\mathbf{x}_i'\\beta)]$.\n",
    "**Score:** Differentiating with respect to the vector $\\beta$:\n",
    "$$ \\mathbf{s}(\\beta) = \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_{i=1}^n [-\\exp(\\mathbf{x}_i'\\beta)\\mathbf{x}_i + y_i \\mathbf{x}_i] = \\sum_{i=1}^n (y_i - \\exp(\\mathbf{x}_i'\\beta))\\mathbf{x}_i $$\n",
    "**Hessian:** Differentiating the score again with respect to $\\beta'$:\n",
    "$$ \\mathbf{H}(\\beta) = \\frac{\\partial \\mathbf{s}(\\beta)}{\\partial \\beta'} = \\sum_{i=1}^n (0 - \\exp(\\mathbf{x}_i'\\beta)\\mathbf{x}_i \\mathbf{x}_i') = -\\sum_{i=1}^n \\exp(\\mathbf{x}_i'\\beta)\\mathbf{x}_i \\mathbf{x}_i' $$\n",
    "**Fisher Information:** The Hessian does not depend on $y_i$, so it is non-random. Therefore, the expected value is just the Hessian itself: $E[\\mathbf{H}(\\beta)] = \\mathbf{H}(\\beta)$.\n",
    "The Information Matrix is $I(\\beta) = -E[\\mathbf{H}(\\beta)] = \\sum_{i=1}^n \\exp(\\mathbf{x}_i'\\beta)\\mathbf{x}_i \\mathbf{x}_i'$.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Logit vs. Probit:**\n",
    "You would need to define a `neg_loglike_logit` function and pass it to the `MLEstimator` or `statsmodels`. The log-likelihood contribution for Logit is $\\mathcal{L}_i = y_i \\ln(p_i) + (1-y_i)\\ln(1-p_i)$, where $p_i = \\frac{1}{1 + e^{-\\mathbf{X}_i'\\beta}}$. After estimating the coefficients, you would find that the Logit coefficients are larger than the Probit coefficients by a factor of approximately $\\pi/\\sqrt{3} \\approx 1.81$. This is because the logistic distribution has a standard deviation of $\\pi/\\sqrt{3}$, while the standard normal distribution (used in Probit) has a standard deviation of 1. The coefficients must scale up to account for the larger variance of the assumed error distribution."
   ]
  }
]
}

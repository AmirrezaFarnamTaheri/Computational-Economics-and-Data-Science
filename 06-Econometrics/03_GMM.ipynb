{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6.3: The Generalized Method of Moments (GMM)\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [**Introduction: The Power of Moment Conditions**](#intro)\n",
    "    - [Biographical Note: Lars Peter Hansen](#hansen)\n",
    "2.  [**From Moments to an Objective Function**](#objective)\n",
    "    - [The GMM Criterion Function](#criterion)\n",
    "    - [The Role of the Weighting Matrix](#weighting)\n",
    "3.  [**The Two-Step Efficient GMM Estimator**](#two-step)\n",
    "    - [Step 1: A Consistent First Step](#step1)\n",
    "    - [Step 2: The Optimal Weighting Matrix and Efficient Estimation](#step2)\n",
    "4.  [**Asymptotic Properties of the GMM Estimator**](#asymptotics)\n",
    "5.  [**Implementation: A Reusable GMM Tool**](#gmm-class)\n",
    "6.  [**Example 1: Instrumental Variables as GMM**](#iv)\n",
    "7.  [**Example 2: Non-Linear GMM for Asset Pricing**](#nonlinear)\n",
    "8.  [**Hypothesis Testing: The J-Test for Overidentifying Restrictions**](#j-test)\n",
    "9.  [**Exercises**](#exercises)\n",
    "10. [**Summary and Key Takeaways**](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "After completing this chapter, you will be able to:\n",
    "\n",
    "- **Explain** the intuition behind the Method of Moments and its generalization, GMM.\n",
    "- **Formulate** econometric problems in terms of their underlying moment conditions.\n",
    "- **Construct** the GMM objective function and understand the crucial role of the weighting matrix.\n",
    "- **Describe** the Two-Step Efficient GMM estimation procedure.\n",
    "- **Recognize** that OLS, IV, and 2SLS are special cases of GMM.\n",
    "- **Implement** a GMM estimator in Python for both linear and non-linear models.\n",
    "- **Understand, apply, and interpret** Hansen's J-test for overidentifying restrictions as a powerful model specification test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction: The Power of Moment Conditions\n",
    "\n",
    "The **Generalized Method of Moments (GMM)**, formalized by Lars Peter Hansen in his seminal 1982 paper, is one of the most important and versatile estimation frameworks in modern econometrics. Its power lies in its generality. While Maximum Likelihood Estimation (MLE) requires specifying the entire probability distribution of the data, GMM requires only that we specify a set of **moment conditions** that should hold true at the population level.\n",
    "\n",
    "Many estimation problems can be framed in this way. The core idea is simple but profound:\n",
    "\n",
    "> Economic theory often provides orthogonality or moment conditions, which state that the expected value of some function of the data and parameters is zero. The GMM principle is to choose the parameter estimates that make the sample analogue of these population moment conditions as close to zero as possible.\n",
    "\n",
    "This framework is incredibly powerful because it unifies many well-known estimators under a single conceptual umbrella:\n",
    "- **Ordinary Least Squares (OLS)** is a GMM estimator where the moment conditions are that the regressors are orthogonal to the error term.\n",
    "- **Instrumental Variables (IV)** is a GMM estimator where the moment conditions are that the instruments are orthogonal to the error term.\n",
    "- **Maximum Likelihood (MLE)** can be shown to be a GMM estimator where the moment conditions are given by the score vector.\n",
    "\n",
    "GMM is particularly indispensable for estimating parameters in complex, non-linear structural models common in macroeconomics and finance, for which a full likelihood function may be intractable or undesirable to specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hansen'></a>\n",
    "### Biographical Note: Lars Peter Hansen (1952-Present)\n",
    "\n",
    "Lars Peter Hansen is an American economist and econometrician at the University of Chicago. He is best known as the developer of the Generalized Method of Moments (GMM). In 2013, he was jointly awarded the Nobel Memorial Prize in Economic Sciences with Eugene Fama and Robert Shiller for their \"empirical analysis of asset prices.\"\n",
    "\n",
    "Hansen's work on GMM provided a framework that was not only theoretically elegant but also immensely practical. It gave economists a tool to confront complex economic models with data without needing to make strong, often unrealistic, distributional assumptions. His development of the J-test for overidentifying restrictions also provided a crucial tool for testing the validity of the underlying economic assumptions embodied in the moment conditions. His contributions are a cornerstone of graduate-level econometrics and are applied in nearly every field of empirical economics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import chi2\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (11, 7), 'figure.dpi': 130})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized for Generalized Method of Moments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='objective'></a>\n",
    "## 2. From Moments to an Objective Function\n",
    "\n",
    "Let's formalize the GMM principle. Suppose our economic theory implies a set of $r$ population moment conditions:\n",
    "\n",
    "$$ E[g(W_i, \\theta_0)] = 0 $$\n",
    "\n",
    "where $W_i$ is a vector of observed data for individual $i$, $\\theta_0$ is the $k \\times 1$ vector of true parameters we want to estimate, and $g(\\cdot)$ is a vector-valued function with $r$ elements. For the model to be identified, we need at least as many moment conditions as parameters, so $r \\ge k$.\n",
    "\n",
    "The sample analogue of the population moment condition is the sample average:\n",
    "\n",
    "$$ g_N(\\theta) = \\frac{1}{N} \\sum_{i=1}^N g(W_i, \\theta) $$\n",
    "\n",
    "Due to sampling variation, $g_N(\\theta_0)$ will not be exactly zero, even at the true parameter value. The GMM approach is to find the parameter vector $\\hat{\\theta}$ that makes $g_N(\\hat{\\theta})$ as \"close\" to the zero vector as possible.\n",
    "\n",
    "<a id='criterion'></a>\n",
    "### The GMM Criterion Function\n",
    "\n",
    "We measure this \"closeness\" using a quadratic form. The GMM estimator $\\hat{\\theta}_{GMM}$ is the value of $\\theta$ that minimizes the following objective function:\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "    \n",
    "**Definition: The GMM Criterion Function**\n",
    "\n",
    "$$ J(\\theta, W) = N \\cdot g_N(\\theta)' W g_N(\\theta) $$\n",
    "    \n",
    "</div>\n",
    "\n",
    "where $W$ is an $r \\times r$ symmetric, positive definite **weighting matrix**. This matrix determines how we penalize deviations from the different moment conditions. (Note: The multiplication by N is standard and simplifies the asymptotic distribution of the J-statistic).\n",
    "\n",
    "<a id='weighting'></a>\n",
    "### The Role of the Weighting Matrix\n",
    "\n",
    "Why do we need a weighting matrix? The different moment conditions in $g_N(\\theta)$ may have different variances and may be correlated with each other. A good weighting matrix should give more weight to the moment conditions that are estimated more precisely (i.e., have smaller variance) and account for the correlation between them.\n",
    "\n",
    "Hansen (1982) showed that the **asymptotically efficient** GMM estimator (the one with the smallest asymptotic variance) is obtained by using a weighting matrix that is a consistent estimate of the inverse of the variance-covariance matrix of the moment conditions, $S$.\n",
    "\n",
    "$$ S = Var(\\sqrt{N} g_N(\\theta_0)) = E[g(W_i, \\theta_0)g(W_i, \\theta_0)'] $$\n",
    "\n",
    "The optimal weighting matrix is therefore $W_{opt} = S^{-1}$. The problem is that $S$ itself depends on the unknown true parameters $\\theta_0$. This leads to a sequential estimation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='two-step'></a>\n",
    "## 3. The Two-Step Efficient GMM Estimator\n",
    "\n",
    "The solution to the problem of the optimal weighting matrix depending on the parameters is the now-standard **Two-Step GMM** procedure.\n",
    "\n",
    "<a id='step1'></a>\n",
    "### Step 1: Obtain a Consistent First-Step Estimate\n",
    "\n",
    "First, we obtain a consistent (but likely inefficient) estimate of $\\theta$ by minimizing the GMM criterion function with a sub-optimal, but still valid, weighting matrix. A common choice is the identity matrix, $W = I$. This treats all moment conditions as equally important.\n",
    "\n",
    "$$ \\hat{\\theta}_1 = \\arg\\min_{\\theta} g_N(\\theta)' I g_N(\\theta) = \\arg\\min_{\\theta} g_N(\\theta)' g_N(\\theta) $$\n",
    "\n",
    "Under standard regularity conditions, this first-step estimator $\\hat{\\theta}_1$ is consistent for $\\theta_0$.\n",
    "\n",
    "<a id='step2'></a>\n",
    "### Step 2: Form the Optimal Weighting Matrix and Find the Efficient Estimator\n",
    "\n",
    "Next, we use the consistent first-step estimate $\\hat{\\theta}_1$ to construct a consistent estimate of the optimal weighting matrix. We first estimate $S$:\n",
    "\n",
    "$$ \\hat{S} = \\frac{1}{N} \\sum_{i=1}^N g(W_i, \\hat{\\theta}_1)g(W_i, \\hat{\\theta}_1)' $$\n",
    "\n",
    "(Note: For time series data where the moments may be serially correlated, a more complex estimator for S, like the Newey-West estimator, is required.)\n",
    "\n",
    "With this consistent estimate $\\hat{S}$, we form our estimate of the optimal weighting matrix: $\\hat{W}_{opt} = \\hat{S}^{-1}$.\n",
    "\n",
    "Finally, we solve the GMM problem again, this time using the estimated optimal weighting matrix:\n",
    "\n",
    "$$ \\hat{\\theta}_{GMM} = \\arg\\min_{\\theta} g_N(\\theta)' \\hat{S}^{-1} g_N(\\theta) $$\n",
    "\n",
    "This second-step estimator, $\\hat{\\theta}_{GMM}$, is the **efficient GMM estimator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='asymptotics'></a>\n",
    "## 4. Asymptotic Properties of the GMM Estimator\n",
    "\n",
    "Under a set of regularity conditions (analogous to those for MLE, ensuring the problem is well-behaved), the efficient Two-Step GMM estimator has the following properties:\n",
    "\n",
    "1.  **Consistency:** $\\hat{\\theta}_{GMM} \\xrightarrow{p} \\theta_0$\n",
    "2.  **Asymptotic Normality:**\n",
    "    $$ \\sqrt{N}(\\hat{\\theta}_{GMM} - \\theta_0) \\xrightarrow{d} N(0, V_{GMM}) $$\n",
    "\n",
    "The asymptotic variance-covariance matrix, $V_{GMM}$, is given by a formidable-looking but important formula:\n",
    "\n",
    "$$ V_{GMM} = (G'S^{-1}G)^{-1} $$\n",
    "\n",
    "where:\n",
    "- $G = E\\left[\\frac{\\partial g(W_i, \\theta_0)}{\\partial \\theta'}\\right]$ is the $r \\times k$ matrix of expected derivatives of the moment conditions.\n",
    "- $S = E[g(W_i, \\theta_0)g(W_i, \\theta_0)']$ is the $r \\times r$ variance-covariance matrix of the moment conditions.\n",
    "\n",
    "In practice, we use sample analogues to estimate $V_{GMM}$:\n",
    "- $\\hat{G} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial g(W_i, \\hat{\\theta}_{GMM})}{\\partial \\theta'}$\n",
    "- $\\hat{S}$ is the same matrix estimated in Step 2.\n",
    "\n",
    "The estimated variance is then $\\hat{V}_{GMM} = (\\hat{G}'\\hat{S}^{-1}\\hat{G})^{-1}$, and its diagonal elements (divided by N) give us the squared standard errors for our parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gmm-class'></a>\n",
    "## 5. Implementation: A Reusable GMM Tool\n",
    "\n",
    "To translate the theory into practice, we can create a simple, reusable Python class to handle the mechanics of Two-Step GMM. This class will manage the optimization, weighting matrix calculation, and standard error estimation, allowing us to focus on specifying the model's moment conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMEstimator:\n",
    "    \"\"\"\n",
    "    A simple class to perform Two-Step GMM estimation.\n",
    "    \n",
    "    Args:\n",
    "        moment_conditions (callable): A function that takes (theta, data) and returns an (N x r) matrix of moment contributions.\n",
    "        data (dict): A dictionary containing the data arrays needed by the moment conditions function.\n",
    "        param_names (list): A list of names for the parameters (theta).\n",
    "    \"\"\"\n",
    "    def __init__(self, moment_conditions, data, param_names=None):\n",
    "        self.moment_conditions = moment_conditions\n",
    "        self.data = data\n",
    "        self.n_obs = next(iter(data.values())).shape[0]\n",
    "        self.param_names = param_names\n",
    "        self.gmm_params = None\n",
    "        self.gmm_vcov = None\n",
    "        self.j_stat = None\n",
    "        self.j_pval = None\n",
    "\n",
    "    def fit(self, start_params):\n",
    "        \"\"\"Performs the two-step GMM estimation.\"\"\"\n",
    "        n_params = len(start_params)\n",
    "        if self.param_names is None:\n",
    "            self.param_names = [f'p{i}' for i in range(n_params)]\n",
    "        \n",
    "        # --- Step 1: Initial consistent estimation with Identity Weighting Matrix ---\n",
    "        W1 = np.identity(self.moment_conditions(start_params, self.data).shape[1])\n",
    "        \n",
    "        def criterion_fn(theta, W):\n",
    "            g = self.moment_conditions(theta, self.data)\n",
    "            g_mean = np.mean(g, axis=0)\n",
    "            return self.n_obs * (g_mean.T @ W @ g_mean)\n",
    "            \n",
    "        res1 = minimize(criterion_fn, start_params, args=(W1,), method='BFGS')\n",
    "        theta1 = res1.x\n",
    "\n",
    "        # --- Step 2: Efficient GMM with Optimal Weighting Matrix ---\n",
    "        g1 = self.moment_conditions(theta1, self.data)\n",
    "        S_hat = (g1.T @ g1) / self.n_obs\n",
    "        W2 = np.linalg.inv(S_hat)\n",
    "\n",
    "        res2 = minimize(criterion_fn, theta1, args=(W2,), method='BFGS')\n",
    "        self.gmm_params = res2.x\n",
    "        self.j_stat = res2.fun\n",
    "        \n",
    "        # --- Calculate Standard Errors ---\n",
    "        # Numerical differentiation for G\n",
    "        epsilon = 1e-6\n",
    "        G_hat = np.zeros((g1.shape[1], n_params))\n",
    "        for i in range(n_params):\n",
    "            theta_plus = self.gmm_params.copy()\n",
    "            theta_plus[i] += epsilon\n",
    "            g_plus = np.mean(self.moment_conditions(theta_plus, self.data), axis=0)\n",
    "            \n",
    "            theta_minus = self.gmm_params.copy()\n",
    "            theta_minus[i] -= epsilon\n",
    "            g_minus = np.mean(self.moment_conditions(theta_minus, self.data), axis=0)\n",
    "            \n",
    "            G_hat[:, i] = (g_plus - g_minus) / (2 * epsilon)\n",
    "\n",
    "        # Variance-covariance matrix\n",
    "        V_hat = np.linalg.inv(G_hat.T @ W2 @ G_hat)\n",
    "        self.gmm_vcov = V_hat / self.n_obs\n",
    "        \n",
    "        # J-test p-value\n",
    "        dof = g1.shape[1] - n_params\n",
    "        if dof > 0:\n",
    "            self.j_pval = chi2.sf(self.j_stat, df=dof)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Prints a summary of the GMM results.\"\"\"\n",
    "        if self.gmm_params is None:\n",
    "            print(\"Model has not been fitted yet.\")\n",
    "            return\n",
    "        \n",
    "        se = np.sqrt(np.diag(self.gmm_vcov))\n",
    "        z_stats = self.gmm_params / se\n",
    "        p_values = chi2.sf(z_stats**2, df=1)\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'Estimate': self.gmm_params,\n",
    "            'Std. Error': se,\n",
    "            'Z-statistic': z_stats,\n",
    "            'P-value': p_values\n",
    "        }, index=self.param_names)\n",
    "        \n",
    "        print(\"Two-Step GMM Results\")\n",
    "        print(f\"N. of Observations: {self.n_obs}\")\n",
    "        display(results_df.round(4))\n",
    "        \n",
    "        if self.j_stat is not None and self.j_pval is not None:\n",
    "            print(\"\\nOveridentification Test (Hansen's J):\")\n",
    "            print(f\"J-statistic: {self.j_stat:.4f}\")\n",
    "            print(f\"P-value: {self.j_pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iv'></a>\n",
    "## 6. Example 1: Instrumental Variables as GMM\n",
    "\n",
    "Our first and most fundamental example is to show that the standard Instrumental Variables (IV/2SLS) estimator is a special case of GMM. \n",
    "\n",
    "Consider the model:\n",
    "$$ y_i = \\mathbf{x}_i'\\beta + u_i $$\n",
    "\n",
    "where one or more of the regressors in $\\mathbf{x}_i$ are endogenous (correlated with $u_i$). Suppose we have a set of $r$ instruments, $\\mathbf{z}_i$, that are correlated with $\\mathbf{x}_i$ but uncorrelated with $u_i$. This gives us our population moment conditions:\n",
    "\n",
    "$$ E[\\mathbf{z}_i u_i] = E[\\mathbf{z}_i (y_i - \\mathbf{x}_i'\\beta)] = 0 $$\n",
    "\n",
    "This is a set of $r$ moment conditions for the $k$ parameters in $\\beta$. We can estimate $\\beta$ using our GMM tool.\n",
    "\n",
    "### Simulation\n",
    "Let's simulate a model with one endogenous regressor and two instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"IV as GMM: Simulation\")\n",
    "\n",
    "# 1. Simulate data with an endogenous regressor\n",
    "note(\"Simulating y = b0 + b1*x + u, where x is correlated with u. We have two valid instruments, z1 and z2.\")\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "N = 1000\n",
    "true_beta = np.array([0.5, 2.0])\n",
    "\n",
    "# Instruments (exogenous)\n",
    "z1 = rng.standard_normal(N)\n",
    "z2 = rng.standard_normal(N)\n",
    "Z = np.vstack([z1, z2]).T\n",
    "\n",
    "# Disturbance term 'v' for the endogenous regressor\n",
    "v = 0.7 * z1 + 0.3 * z2 + rng.standard_normal(N)\n",
    "\n",
    "# Error term 'u' for the main equation\n",
    "u = 0.5 * v + rng.standard_normal(N) # u and v are correlated\n",
    "\n",
    "# Endogenous regressor 'x'\n",
    "x = 1 + 0.5 * v\n",
    "\n",
    "# Dependent variable 'y'\n",
    "X = sm.add_constant(x)\n",
    "y = X @ true_beta + u\n",
    "\n",
    "# 2. Estimate with our GMM class\n",
    "note(\"Estimating the model using our GMMEstimator class.\")\n",
    "\n",
    "# The instruments for the model include the constant and z1, z2\n",
    "instruments = sm.add_constant(Z)\n",
    "data_iv = {'y': y, 'X': X, 'Z': instruments}\n",
    "\n",
    "def iv_moment_conditions(beta, data):\n",
    "    \"\"\"Returns the (N x r) matrix of moment contributions for IV.\"\"\"\n",
    "    y, X, Z = data['y'], data['X'], data['Z']\n",
    "    u = y - X @ beta\n",
    "    g = Z * u[:, np.newaxis] # Element-wise multiplication, broadcasting u\n",
    "    return g\n",
    "\n",
    "gmm_iv = GMMEstimator(iv_moment_conditions, data_iv, param_names=['const', 'x1'])\n",
    "gmm_iv.fit(start_params=[0, 0])\n",
    "gmm_iv.summary()\n",
    "\n",
    "# 3. Compare with a standard IV/2SLS estimator\n",
    "note(\"Comparing the results to Statsmodels' standard IV2SLS estimator.\")\n",
    "iv_sm = IV2SLS(y, X, instruments).fit()\n",
    "print(iv_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nonlinear'></a>\n",
    "## 7. Example 2: Non-Linear GMM for Asset Pricing\n",
    "\n",
    "The true power of GMM shines in non-linear models where other estimators are difficult to apply. A classic application is in finance, for estimating the parameters of a **consumption-based capital asset pricing model (CCAPM)**.\n",
    "\n",
    "The core pricing equation in many asset pricing models is the Euler equation:\n",
    "\n",
    "$$ E_t[M_{t+1} R_{i, t+1}] = 1 $$\n",
    "\n",
    "where $R_{i, t+1}$ is the gross return on asset $i$ and $M_{t+1}$ is the **stochastic discount factor (SDF)**. For a representative agent with CRRA utility, the SDF is $M_{t+1} = \\beta (C_{t+1}/C_t)^{-\\gamma}$, where $\\beta$ is the subjective discount factor and $\\gamma$ is the coefficient of relative risk aversion.\n",
    "\n",
    "The expectation is conditional on information at time $t$. By the law of iterated expectations, this implies that any variable $Z_t$ in the time-$t$ information set is orthogonal to the pricing error:\n",
    "\n",
    "$$ E[(M_{t+1} R_{i, t+1} - 1) Z_t] = 0 $$\n",
    "$$ E\\left[\\left(\\beta \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma} R_{i, t+1} - 1\\right) Z_t\\right] = 0 $$\n",
    "\n",
    "This provides a set of non-linear moment conditions that we can use to estimate the structural parameters $(\\beta, \\gamma)$. We can use lagged consumption growth and lagged returns as instruments $Z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Non-Linear GMM: CCAPM Estimation\")\n",
    "\n",
    "# 1. Load or simulate asset pricing data\n",
    "note(\"We simulate time-series data for consumption growth and asset returns.\")\n",
    "rng = np.random.default_rng(seed=42)\n",
    "T = 500\n",
    "\n",
    "# True parameters (beta is close to 1, gamma is positive)\n",
    "true_params = {'beta': 0.99, 'gamma': 2.5}\n",
    "\n",
    "# Simulate data\n",
    "cons_growth = np.exp(rng.normal(0.02, 0.02, T))\n",
    "asset_return = np.exp(rng.normal(0.06, 0.15, T))\n",
    "\n",
    "# Create lagged instruments\n",
    "Z_t = np.vstack([\n",
    "    np.ones(T-1),\n",
    "    cons_growth[:-1],\n",
    "    asset_return[:-1]\n",
    "]).T\n",
    "\n",
    "# Align data\n",
    "C_t1_over_Ct = cons_growth[1:]\n",
    "R_t1 = asset_return[1:]\n",
    "\n",
    "data_ccapm = {'c_growth': C_t1_over_Ct, 'returns': R_t1, 'instruments': Z_t}\n",
    "\n",
    "# 2. Define the non-linear moment conditions\n",
    "def ccapm_moment_conditions(theta, data):\n",
    "    beta, gamma = theta[0], theta[1]\n",
    "    c_growth = data['c_growth']\n",
    "    returns = data['returns']\n",
    "    Z = data['instruments']\n",
    "    \n",
    "    # Pricing error\n",
    "    pricing_error = (beta * c_growth**(-gamma) * returns) - 1\n",
    "    \n",
    "    # Moment contributions g_i = Z_i * error_i\n",
    "    g = Z * pricing_error[:, np.newaxis]\n",
    "    return g\n",
    "\n",
    "# 3. Estimate with our GMM class\n",
    "note(\"Estimating the CCAPM parameters using our GMMEstimator.\")\n",
    "gmm_ccapm = GMMEstimator(ccapm_moment_conditions, data_ccapm, param_names=['beta', 'gamma'])\n",
    "gmm_ccapm.fit(start_params=[0.95, 2.0])\n",
    "gmm_ccapm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='j-test'></a>\n",
    "## 8. Hypothesis Testing: The J-Test for Overidentifying Restrictions\n",
    "\n",
    "When we have more moment conditions than parameters ($r > k$), the model is **overidentified**. This is a good thing, because it allows us to test the validity of our model's specification. If the moment conditions are genuinely valid (i.e., $E[g(W_i, \\theta_0)] = 0$ is true), then the sample moments $g_N(\\hat{\\theta}_{GMM})$ should be close to zero.\n",
    "\n",
    "Hansen's **J-test** (or test of overidentifying restrictions) is a formal test of this. The J-statistic is the value of the GMM objective function evaluated at the efficient second-step estimates:\n",
    "\n",
    "$$ J = N \\cdot g_N(\\hat{\\theta}_{GMM})' \\hat{S}^{-1} g_N(\\hat{\\theta}_{GMM}) $$\n",
    "\n",
    "Under the null hypothesis that all $r$ moment conditions are valid, the J-statistic follows a chi-squared distribution with degrees of freedom equal to the number of overidentifying restrictions:\n",
    "\n",
    "$$ J \\xrightarrow{d} \\chi^2_{r-k} $$\n",
    "\n",
    "**Interpretation:**\n",
    "- A **small J-statistic** (and a large p-value) means the sample moments are close to zero, so we **do not reject** the null hypothesis. This provides support for the model's specification and the validity of the instruments.\n",
    "- A **large J-statistic** (and a small p-value) means that at least some of the sample moments are far from zero, even after choosing the best possible parameters. We **reject** the null hypothesis, which suggests that the model is misspecified (i.e., at least one of our moment conditions is false).\n",
    "\n",
    "The J-test is a crucial diagnostic tool for any GMM application. In our IV example, the J-statistic was small and the p-value was large, indicating that our instruments were valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 9. Exercises\n",
    "\n",
    "1.  **Just-Identification:** What happens to the J-statistic when the model is exactly identified ($r=k$)? Explain both intuitively and by looking at the formula for the GMM estimator.\n",
    "\n",
    "2.  **OLS as GMM:** Consider the classical linear model $y_i = \\mathbf{x}_i'\\beta + u_i$, where $E[\\mathbf{x}_i u_i] = 0$. \n",
    "    a. Write down the moment conditions for this model.\n",
    "    b. Write down the sample moment vector $g_N(\\beta)$.\n",
    "    c. For the just-identified case, the GMM estimator sets $g_N(\\hat{\\beta})=0$. Solve this equation for $\\hat{\\beta}$. Do you recognize the result?\n",
    "\n",
    "3.  **Alternative Instruments:** In the IV-as-GMM example, we used `[const, z1, z2]` as our instruments. What would happen if you only used `[const, z1]`? The model would be just-identified. Re-run the estimation with this smaller set of instruments. How do the parameter estimates and standard errors change? What is the J-statistic now?\n",
    "\n",
    "4.  **Continuously Updated GMM (CUE):** Instead of the two-step approach, one could minimize a criterion that updates the weighting matrix at every iteration: $J_{CUE}(\\theta) = g_N(\\theta)' [S(\\theta)]^{-1} g_N(\\theta)$. This is the Continuously Updated GMM estimator. Discuss the potential advantages and disadvantages of this approach compared to the two-step method. (Hint: Think about statistical efficiency vs. computational cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='summary'></a>\n",
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "The Generalized Method of Moments is a powerful and flexible estimation framework that encompasses many of the most common estimators in econometrics.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Moment Conditions**: GMM is built on the idea that economic theory provides moment conditions, $E[g(W_i, \\theta_0)] = 0$, which must hold at the true parameter values.\n",
    "- **Objective Function**: The GMM estimator minimizes a quadratic form of the sample moments, $J(\\theta) = g_N(\\theta)' W g_N(\\theta)$, to make the sample moments collectively as close to zero as possible.\n",
    "- **Weighting Matrix**: The weighting matrix $W$ is crucial for efficiency. The optimal weighting matrix, $W_{opt} = S^{-1}$, is the inverse of the covariance matrix of the moment conditions.\n",
    "- **Two-Step Estimation**: Since $S$ is unknown, we use a two-step procedure: first, get a consistent estimate using a sub-optimal weight matrix (like $I$), then use that estimate to form $\\hat{S}$, and finally, re-estimate using $\\hat{W} = \\hat{S}^{-1}$ to get the efficient GMM estimator.\n",
    "- **Generality**: OLS and IV are simple, special cases of GMM. The real power of GMM is in estimating parameters of complex, non-linear models where MLE might be intractable.\n",
    "- **J-Test**: In overidentified models ($r>k$), the J-statistic provides a powerful test of the model's specification. A rejection of the J-test casts doubt on the validity of the underlying moment conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solutions to Exercises\n",
    "\n",
    "---\n",
    "\n",
    "**1. Just-Identification:**\n",
    "When $r=k$, we have exactly as many equations (moment conditions) as we have unknowns (parameters). The GMM estimator will be able to set the sample moment conditions *exactly* to zero: $g_N(\\hat{\\beta}_{GMM}) = 0$. Since the objective function is $J = N \\cdot g_N(\\hat{\\beta})' W g_N(\\hat{\\beta})$, and $g_N(\\hat{\\beta})$ is a vector of zeros, the J-statistic will be exactly 0. Intuitively, if you have $k$ equations and $k$ unknowns, you can find a perfect solution. You have no 'extra' information left over to test the validity of the equations themselves. The degrees of freedom for the test, $r-k$, is also zero.\n",
    "\n",
    "---\n",
    "\n",
    "**2. OLS as GMM:**\n",
    "a. The moment conditions are the orthogonality conditions of OLS: $E[\\mathbf{x}_i u_i] = E[\\mathbf{x}_i (y_i - \\mathbf{x}_i'\\beta)] = 0$.\n",
    "b. The sample moment vector is $g_N(\\beta) = \\frac{1}{N} \\sum \\mathbf{x}_i (y_i - \\mathbf{x}_i'\\beta) = \\frac{1}{N} (\\mathbf{X}'\\mathbf{y} - \\mathbf{X}'\\mathbf{X}\\beta)$.\n",
    "c. In the just-identified case, we set $g_N(\\hat{\\beta}) = 0$: \n",
    "$$ \\frac{1}{N} (\\mathbf{X}'\\mathbf{y} - \\mathbf{X}'\\mathbf{X}\\hat{\\beta}) = 0 \\implies \\mathbf{X}'\\mathbf{y} = \\mathbf{X}'\\mathbf{X}\\hat{\\beta} \\implies \\hat{\\beta}_{GMM} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} $$\n",
    "This is precisely the OLS estimator.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Alternative Instruments:**\n",
    "If you use only `[const, z1]` as instruments, you now have $r=2$ instruments and $k=2$ parameters (`const`, `x1`). The model is now just-identified. When you re-run the estimation, the J-statistic will be exactly 0. The parameter estimates will likely be different, and their standard errors will likely be larger. This is because you are throwing away information by not using the valid instrument `z2`. The original estimator was more efficient because it used all available information optimally.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Continuously Updated GMM (CUE):**\n",
    "**Advantages:** The CUE GMM estimator has better finite-sample properties than the two-step estimator. It is less prone to the finite-sample bias that can affect two-step GMM. In theory, it can be more efficient in small samples.\n",
    "**Disadvantages:** The CUE is computationally much more expensive. The weighting matrix $S(\\theta)$ changes at every single iteration of the numerical optimizer, which means it must be re-calculated and re-inverted repeatedly. The objective function becomes much more complex to minimize. In contrast, the two-step estimator only requires calculating the weighting matrix once. For large datasets or complex models, the computational burden of CUE can be prohibitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
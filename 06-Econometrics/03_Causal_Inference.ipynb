{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "try:\n",
    "    import graphviz\n",
    "    GRAPHVIZ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GRAPHVIZ_AVAILABLE = False\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (12, 8), 'figure.dpi': 150})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "if not GRAPHVIZ_AVAILABLE: note(\"The 'graphviz' library is not installed (`pip install graphviz`). Some DAG visualizations will be skipped.\")\n",
    "note(\"Environment initialized for Advanced Causal Inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Econometrics\n",
    "## Chapter 6.04: Causal Inference: The Quest for \"Why\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: The Credibility Revolution\n",
    "This chapter marks a pivotal transition from prediction and correlation to the far more ambitious world of **causal inference**. The famous mantra \"correlation does not imply causation\" is the starting point for this entire field. The focused pursuit of credible causal identification in the late 20th century has been dubbed the **\"credibility revolution\"** in economics, recognized with the 2021 Nobel Prize to David Card, Joshua Angrist, and Guido Imbens.\n",
    "\n",
    "This chapter provides a PhD-level introduction to the modern causal inference toolkit, covering:\n",
    "1.  **The Potential Outcomes Framework:** A formal language for defining causality.\n",
    "2.  **Directed Acyclic Graphs (DAGs):** A graphical language for visualizing causal assumptions and identifying sources of bias.\n",
    "3.  **Matching Methods:** An overview of methods to control for observed confounders.\n",
    "4.  **A Bridge to Quasi-Experiments:** An introduction to the Local Average Treatment Effect (LATE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Potential Outcomes Framework\n",
    "The core difficulty of causal inference is that for any given individual, we can never simultaneously observe their outcome in two different states of the world. The **Potential Outcomes framework** formalizes this.\n",
    "\n",
    "For each individual $i$, we define two potential outcomes:\n",
    "*   $Y_i(1)$: The potential outcome for individual $i$ *if they receive the treatment* ($D_i=1$).\n",
    "*   $Y_i(0)$: The potential outcome for individual $i$ *if they do not receive the treatment* ($D_i=0$).\n",
    "\n",
    "The **individual causal effect** is $\\tau_i = Y_i(1) - Y_i(0)$. We can never observe this. Instead, we focus on estimating **average treatment effects**, such as the **ATE** (for the whole population) or the **ATT** (for the treated population).\n",
    "\n",
    "A naive comparison of the average outcomes for the treated and untreated groups decomposes into:\n",
    "$$ E[Y|D=1] - E[Y|D=0] = \\underbrace{E[Y(1) - Y(0) | D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) | D=1] - E[Y(0) | D=0]}_{\\text{Selection Bias}} $$ \n",
    "The entire goal of causal inference methods is to find a way to eliminate the **selection bias** term. The gold standard is **random assignment**, which by design forces the selection bias to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Directed Acyclic Graphs (DAGs) and Causal Identification\n",
    "Developed by Judea Pearl, Directed Acyclic Graphs (DAGs) are a powerful tool for visualizing causal assumptions and identifying sources of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Confounding and The Backdoor Criterion\n",
    "A **confounder** is a variable that causes both the treatment and the outcome, creating a spurious **backdoor path**. To identify the causal effect, we must find a set of observable variables $Z$ that satisfies the **backdoor criterion**: it blocks every non-causal path from treatment to outcome. If we can find such a set, we can get a causal estimate by **conditioning** on $Z$ (e.g., including $Z$ in a regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing Confounding\")\n",
    "if GRAPHVIZ_AVAILABLE:\n",
    "    dot = graphviz.Digraph()\n",
    "    dot.node('D', 'Treatment'); dot.node('Y', 'Outcome'); dot.node('Z', 'Confounder')\n",
    "    dot.edge('Z', 'D'); dot.edge('Z', 'Y'); dot.edge('D', 'Y')\n",
    "    note(\"The path D <- Z -> Y is a backdoor path. Controlling for Z closes it.\")\n",
    "    display(dot)\n",
    "else: note(\"Graphviz not installed, skipping DAG visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Collider Bias\n",
    "A common and counter-intuitive source of bias is **collider bias**. A collider is a variable that is caused by two or more other variables. In the path $A \\rightarrow C \\leftarrow B$, the node $C$ is a collider. Conditioning on a collider (or a descendant of a collider) is a grave error: it *opens* a non-causal path between its parents, inducing a spurious correlation where none existed before.\n",
    "\n",
    "**Example:** Suppose a student's `Talent` and `Luck` both contribute to them getting an `Elite University Admission`. Talent and Luck are independent in the general population. However, if we look *only* at students admitted to an elite university (i.e., we condition on the collider), we will find a negative correlation between Talent and Luck. Among the admitted students, the lucky ones are, on average, less talented, and the talented ones are, on average, less lucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Illustrating Collider Bias\")\n",
    "if GRAPHVIZ_AVAILABLE:\n",
    "    dot = graphviz.Digraph()\n",
    "    dot.node('T', 'Talent'); dot.node('L', 'Luck'); dot.node('A', 'Admission')\n",
    "    dot.edge('T', 'A'); dot.edge('L', 'A')\n",
    "    note(\"Controlling for Admission (the collider) creates a spurious link between Talent and Luck.\")\n",
    "    display(dot)\n",
    "else: note(\"Graphviz not installed, skipping DAG visualization.\")\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "n = 5000\n",
    "talent = rng.normal(0, 1, n); luck = rng.normal(0, 1, n)\n",
    "admission_score = talent + luck\n",
    "admitted = admission_score > np.quantile(admission_score, 0.9) # Top 10% get admitted\n",
    "\n",
    "corr_all = np.corrcoef(talent, luck)[0, 1]\n",
    "corr_admitted = np.corrcoef(talent[admitted], luck[admitted])[0, 1]\n",
    "note(f\"Correlation in full population: {corr_all:.3f} (close to zero as expected)\")\n",
    "note(f\"Correlation among admitted students: {corr_admitted:.3f} (spuriously negative!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Matching Methods: Controlling for Observables\n",
    "When randomization is not possible, but we believe we have observed all important confounding variables ($Z$), we can use **matching methods**. The core assumption is **Conditional Independence**: $(Y(0), Y(1)) \\perp D | Z$. The goal is to create a control group that is as similar as possible to the treated group on all observables $Z$.\n",
    "\n",
    "**Propensity Score Matching (PSM):** Rosenbaum and Rubin (1983) showed that it is sufficient to match on a one-dimensional summary of the covariates: the **propensity score**, $p(Z) = P(D=1 | Z)$. A crucial diagnostic is to check for **common support**: the propensity scores of the treated and control groups must overlap substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Code Lab: Propensity Score Matching\")\n",
    "# Generate synthetic data with selection on observables\n",
    "rng = np.random.default_rng(42); n = 2000\n",
    "Z = rng.normal(0, 1, n); D_prob = 1 / (1 + np.exp(-(Z - 0.5)))\n",
    "D = rng.binomial(1, D_prob); Y0 = 10 + 2*Z + rng.normal(0, 1, n)\n",
    "Y = Y0 + 5*D; df = pd.DataFrame({'Y': Y, 'D': D, 'Z': Z})\n",
    "\n",
    "# 1. Estimate propensity scores\n",
    "ps_model = smf.logit('D ~ Z', data=df).fit(disp=0)\n",
    "df['pscore'] = ps_model.predict(df)\n",
    "\n",
    "# 2. Check for common support\n",
    "sns.histplot(data=df, x='pscore', hue='D', stat='density', common_norm=False)\n",
    "plt.title('Propensity Score Distributions (Common Support Check)'); plt.show()\n",
    "note(\"The distributions overlap well, indicating good common support.\")\n",
    "\n",
    "# 3. Match and estimate ATT\n",
    "treated = df[df['D'] == 1]; control = df[df['D'] == 0]\n",
    "matches = []\n",
    "for p in treated['pscore']:\n",
    "    best_match_idx = (control['pscore'] - p).abs().idxmin()\n",
    "    matches.append(control.loc[best_match_idx]['Y'])\n",
    "\n",
    "att_psm = (treated['Y'].values - matches).mean()\n",
    "note(f\"Propensity Score Matching estimate of ATT: {att_psm:.3f} (True effect is 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "***A Look Ahead:***",
    "*The matching methods discussed here work well when the set of confounders, Z, is low-dimensional. However, in many modern applications, researchers have access to hundreds or even thousands of potential control variables. In such high-dimensional settings, standard matching on a propensity score can become fragile. The **Causal Machine Learning** chapter introduces state-of-the-art methods, such as Double/Debiased Machine Learning, that are specifically designed to handle high-dimensional confounding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. A Bridge to Quasi-Experiments: The Local Average Treatment Effect (LATE)\n",
    "Often, we cannot ensure conditional independence holds for the entire population. Quasi-experimental methods like Instrumental Variables (IV) rely on finding some source of variation that is \"as good as random\" for a subset of the population.\n",
    "\n",
    "This leads to the concept of the **Local Average Treatment Effect (LATE)**, which is the average treatment effect for the specific sub-population whose treatment status is influenced by the instrument. These are the **compliers**â€”individuals who would take the treatment if encouraged by the instrument, but not otherwise. IV methods, which we will see in the next chapter, identify the LATE, not the ATE or ATT. This is a crucial distinction for policy interpretation: the effect of a policy on the group of people who can be induced to take it up may be very different from the effect on the general population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Exercises\n",
    "\n",
    "1.  **Selection Bias:** In the potential outcomes decomposition, we derived the bias for the naive estimator. Derive the corresponding bias for the ATT. What is the key assumption needed for the naive estimator to be a valid estimate of the ATT?\n",
    "\n",
    "2.  **DAGs:** Draw a DAG that represents the following scenario: A student's `Innate Ability` affects both their choice to attend a `Private College` and their future `Earnings`. Their parents' `Income` also affects their choice of college. To estimate the causal effect of `Private College` on `Earnings`, which variables must you control for? Which variable should you *not* control for?\n",
    "\n",
    "3.  **Collider Bias in Science:** A common observation is that among PhD students who successfully publish in top journals, there appears to be a negative correlation between the originality of their idea and the rigor of their execution. Draw a DAG to explain why this might be an example of collider bias. What is the collider?\n",
    "\n",
    "4.  **Common Support:** What would the propensity score distribution plot look like if the common support assumption were violated? What would this imply about your ability to estimate the ATT?\n",
    "\n",
    "5.  **LATE:** A job training program is offered, but only some people sign up. An economist uses a lottery that gives some people an encouragement (a small cash bonus) to sign up as an instrument. Who are the 'compliers' in this setting? What specific causal effect does this IV strategy identify?"
   ]
  }
 ]
}
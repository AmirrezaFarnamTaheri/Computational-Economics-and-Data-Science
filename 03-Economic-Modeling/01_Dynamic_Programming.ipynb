{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os, sys, math, time, random, json, textwrap, warnings\n",
    "from typing import Callable\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from numba import njit, prange\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import norm\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 130, 'font.size': 12, 'axes.titlesize': 'x-large',\n",
    "    'axes.labelsize': 'large', 'xtick.labelsize': 'medium', 'ytick.labelsize': 'medium'})\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def note(msg, **kwargs):\n",
    "    display(Markdown(f\"<div class='alert alert-info'>\ud83d\udcdd {textwrap.fill(msg, width=100)}</div>\"))\n",
    "def sec(title):\n",
    "    print(f\"\\n{100*'='}\\n| {title.upper()} |\\n{100*'='}\")\n",
    "\n",
    "note(\"Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Dynamic Models\n",
    "## Chapter 3.1: Introduction to Stochastic Dynamic Programming\n",
    "\n",
    "### Table of Contents\n",
    "1.  [The Principle of Optimality: A Cake Eating Problem](#1.-The-Principle-of-Optimality:-A-Cake-Eating-Problem)\n",
    "2.  [The Bellman Equation for Infinite Horizon Problems](#2.-The-Bellman-Equation-for-Infinite-Horizon-Problems)\n",
    "    *   [2.1 The Bellman Operator and Contraction Mappings](#2.1-The-Bellman-Operator-and-Contraction-Mappings)\n",
    "    *   [2.2 The State-Action Value Function (Q-Function)](#2.2-The-State-Action-Value-Function-(Q-Function))\n",
    "3.  [A Canonical Model: The Stochastic Growth Model](#3.-A-Canonical-Model:-The-Stochastic-Growth-Model)\n",
    "    *   [3.1 Discretizing the Income Process](#3.1-Discretizing-the-Income-Process)\n",
    "    *   [3.2 Building the Model Primitives](#3.2-Building-the-Model-Primitives)\n",
    "4.  [Numerical Solution Algorithms](#4.-Numerical-Solution-Algorithms)\n",
    "    *   [4.1 Value Function Iteration (VFI)](#4.1-Value-Function-Iteration-(VFI))\n",
    "    *   [4.2 Policy Function Iteration (PFI)](#4.2-Policy-Function-Iteration-(PFI))\n",
    "    *   [4.3 Howard's Policy Improvement Algorithm](#4.3-Howard's-Policy-Improvement-Algorithm)\n",
    "5.  [Analysis of the Solution](#5.-Analysis-of-the-Solution)\n",
    "    *   [5.1 The Value and Policy Functions](#5.1-The-Value-and-Policy-Functions)\n",
    "    *   [5.2 Visualizing VFI Convergence](#5.2-Visualizing-VFI-Convergence)\n",
    "6.  [Simulating the Stationary Distribution](#6.-Simulating-the-Stationary-Distribution)\n",
    "7.  [Chapter Summary](#7.-Chapter-Summary)\n",
    "8.  [Exercises](#8.-Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: The Logic of Sequential Decisions\n",
    "Dynamic programming (DP) is the essential framework for analyzing problems that involve a sequence of decisions over time. The core insight, developed by Richard Bellman, is the **principle of optimality**: an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n",
    "\n",
    "This allows us to break down a single, complex, infinite-horizon problem into a sequence of smaller, simpler, two-period problems. This recursive structure is formalized in the **Bellman equation**.\n",
    "\n",
    "This chapter will:\n",
    "1.  Illustrate the core concepts with a simple **cake eating problem** solved via backward induction.\n",
    "2.  Introduce the **Bellman operator** and the **Contraction Mapping Theorem** that guarantees a unique solution exists for infinite horizon problems.\n",
    "3.  Implement the core solution algorithms—**Value Function Iteration (VFI)**, **Policy Function Iteration (PFI)**, and **Howard's Policy Improvement**—from scratch.\n",
    "4.  Solve the canonical **stochastic growth model** (Aiyagari model).\n",
    "5.  Simulate the model to find the **stationary distribution** of wealth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Principle of Optimality: A Cake Eating Problem\n",
    "To build intuition, we start with one of the simplest DP problems: the \"cake eating\" problem. An agent has a cake of size $x_t$ at time $t$. They must decide how much to eat, $c_t$, and how much to leave, $x_{t+1}$. The problem is finite, lasting for $T$ periods.\n",
    "The agent's problem is to maximize lifetime utility: $ \\max_{c_0, ..., c_T} \\sum_{t=0}^{T} \\beta^t u(c_t) $, subject to the resource constraint $x_{t+1} = x_t - c_t$. We solve this using **backward induction**.\n",
    "\n",
    "- **Period T:** The agent eats whatever is left. The value function is $V_T(x_T) = u(x_T)$.\n",
    "- **Period T-1:** The agent chooses $c_{T-1}$ to maximize current plus discounted future utility. This gives the Bellman equation:\n",
    "$$ V_{T-1}(x_{T-1}) = \\max_{0 \\le c_{T-1} \\le x_{T-1}} \\{ u(c_{T-1}) + \\beta V_T(x_{T-1} - c_{T-1}) \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Solving the Finite Horizon Cake Eating Problem\")\n",
    "T = 10; BETA_CE = 0.95; X_GRID = np.linspace(0.01, 10, 100)\n",
    "@njit\n",
    "def u_log(c): return np.log(c)\n",
    "\n",
    "V_storage = np.zeros((T + 1, len(X_GRID)))\n",
    "C_storage = np.zeros((T + 1, len(X_GRID)))\n",
    "\n",
    "V_storage[T, :] = u_log(X_GRID)\n",
    "C_storage[T, :] = X_GRID\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    V_next = V_storage[t + 1, :]\n",
    "    for i, x in enumerate(X_GRID):\n",
    "        def objective(c): # Minimize negative utility\n",
    "            x_next = x - c\n",
    "            v_next_interp = np.interp(x_next, X_GRID, V_next)\n",
    "            return -(u_log(c) + BETA_CE * v_next_interp)\n",
    "        res = minimize_scalar(objective, bounds=(1e-6, x), method='bounded')\n",
    "        V_storage[t, i] = -res.fun\n",
    "        C_storage[t, i] = res.x\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.plot(X_GRID, V_storage[0, :], label='V_0(x)'); ax1.plot(X_GRID, V_storage[T-1, :], label=f'V_{T-1}(x)'); ax1.plot(X_GRID, V_storage[T, :], label=f'V_T(x)')\n",
    "ax1.set_title('Value Functions'); ax1.set(xlabel='Cake Size (x)', ylabel='Value'); ax1.legend()\n",
    "ax2.plot(X_GRID, C_storage[0, :], label='c_0(x)'); ax2.plot(X_GRID, C_storage[T-1, :], label=f'c_{T-1}(x)'); ax2.plot(X_GRID, C_storage[T, :], label=f'c_T(x)')\n",
    "ax2.set_title('Policy Functions (Consumption)'); ax2.set(xlabel='Cake Size (x)', ylabel='Consumption (c)'); ax2.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Bellman Equation for Infinite Horizon Problems\n",
    "For an infinite horizon stochastic problem, the Bellman equation takes a more general, recursive form:\n",
    "$$ V(s) = \\max_{a \\in \\Gamma(s)} \\{ u(s, a) + \\beta E[V(s')] \\} $$\n",
    "where $s$ is the state, $a$ is the action, $u(s,a)$ is the reward, and $s'$ is the next period's state. The solution is a **value function** $V(s)$ and a **policy function** $\\sigma(s) = \\arg\\max \\{...\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The Bellman Operator and Contraction Mappings\n",
    "The **Bellman Operator**, $T$, takes a candidate value function $v$ and returns a new function $Tv$:\n",
    "$$ (Tv)(s) = \\max_{a \\in \\Gamma(s)} \\{ u(s, a) + \\beta E[v(s')] \\} $$\n",
    "A solution to the Bellman equation is a **fixed point** of this operator, i.e., $TV = V$. If $\\beta < 1$ and the utility function is bounded, the Bellman operator is a **contraction mapping** on the space of bounded continuous functions. This is because applying the operator shrinks the distance between any two functions $v_1$ and $v_2$ under the sup-norm:\n",
    "$$ ||Tv_1 - Tv_2||_\\infty = \\sup_s |(Tv_1)(s) - (Tv_2)(s)| \\le \\beta \\sup_s |v_1(s) - v_2(s)| = \\beta ||v_1 - v_2||_\\infty $$\n",
    "The **Contraction Mapping Theorem** then guarantees that $T$ has a unique fixed point $V^*$, and for any initial guess $v_0$, the sequence $v_{k+1} = T v_k$ will converge to $V^*$. This justifies **Value Function Iteration (VFI)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The State-Action Value Function (Q-Function)\n",
    "An alternative representation, central to reinforcement learning, is the **state-action value function**, or **Q-function**. It represents the value of being in state $s$ and taking action $a$:\n",
    "$$ Q(s, a) = u(s, a) + \\beta E[V(s')] $$\n",
    "The value function is then simply the maximum over all possible actions: $V(s) = \\max_a Q(s, a)$. Substituting this into the first equation gives a Bellman equation for Q:\n",
    "$$ Q(s, a) = u(s, a) + \\beta E[\\max_{a'} Q(s', a')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A Canonical Model: The Stochastic Growth Model\n",
    "We solve a version of the Bewley-Huggett-Aiyagari model of household savings with uninsurable income risk. The Bellman equation is:\n",
    "$$ V(a, y) = \\max_{0 < a' \\le (1+r)a+y} \\left\\{ u((1+r)a+y-a') + \\beta \\sum_{y'} P(y'|y) V(a', y') \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Discretizing the Income Process\n",
    "We assume log-income, $z_t = \\log(y_t)$, follows an AR(1) process: $z_{t+1} = \\rho z_t + \\sigma \\epsilon_{t+1}$. We use **Tauchen's method** to approximate this with a finite-state Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Discretizing the Income Process\")\n",
    "@njit\n",
    "def tauchen(rho, sigma_u, m=3, n=7):\n",
    "    z_max = m * sigma_u / np.sqrt(1 - rho**2)\n",
    "    z_grid = np.linspace(-z_max, z_max, n)\n",
    "    step = (z_grid[-1] - z_grid[0]) / (n - 1)\n",
    "    P = np.empty((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            z_j_low = z_grid[j] - step/2\n",
    "            z_j_high = z_grid[j] + step/2\n",
    "            P[i, j] = norm.cdf((z_j_high - rho * z_grid[i]) / sigma_u) - \\\n",
    "                      norm.cdf((z_j_low - rho * z_grid[i]) / sigma_u)\n",
    "    return z_grid, P\n",
    "\n",
    "rho_y, sigma_y = 0.95, 0.1\n",
    "z_grid, P_TRANS = tauchen(rho_y, sigma_y)\n",
    "Y_STATES = np.exp(z_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Building the Model Primitives\n",
    "To solve the model, we need to create the reward matrix `R` and the transition probability matrix `Q`. The state is the tuple `(assets, income)`, and the action is the choice of next period's assets `a'`. `R[s, a]` is the utility from being in state `s` and choosing action `a`. `Q[s, a, s']` is the probability of transitioning to state `s'` given state `s` and action `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Building the Model Primitives\")\n",
    "\n",
    "# --- 1. Define Model Parameters ---\n",
    "sgm_params = {\n",
    "    'beta': 0.96,       # Discount factor\n",
    "    'gamma': 2.0,       # CRRA coefficient\n",
    "    'r': 0.03,          # Risk-free interest rate\n",
    "    'a_grid': np.linspace(0.01, 50, 150), # Asset grid\n",
    "    'y_states': Y_STATES, # Income states from Tauchen\n",
    "    'P_trans': P_TRANS  # Transition matrix from Tauchen\n",
    "}\n",
    "\n",
    "n_a = len(sgm_params['a_grid'])\n",
    "n_y = len(sgm_params['y_states'])\n",
    "\n",
    "# --- 2. Create Reward and Transition Matrices ---\n",
    "@njit\n",
    "def u_crra(c, gamma):\n",
    "    return (c**(1 - gamma)) / (1 - gamma)\n",
    "\n",
    "R = np.full((n_a, n_y, n_a), -1e10)\n",
    "for i in range(n_a):\n",
    "    for j in range(n_y):\n",
    "        for k in range(n_a):\n",
    "            a = sgm_params['a_grid'][i]\n",
    "            y = sgm_params['y_states'][j]\n",
    "            a_prime = sgm_params['a_grid'][k]\n",
    "            c = (1 + sgm_params['r']) * a + y - a_prime\n",
    "            if c > 0: R[i, j, k] = u_crra(c, sgm_params['gamma'])\n",
    "\n",
    "Q = np.zeros((n_a, n_y, n_a, n_y))\n",
    "for j in range(n_y):\n",
    "    Q[:, j, :, :] = sgm_params['P_trans'][j, :]\n",
    "\n",
    "note(\"Built R and Q matrices for the stochastic growth model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Numerical Solution Algorithms\n",
    "For pedagogical clarity, we now implement the core solution algorithms directly in the notebook, using Numba's `@njit` decorator to ensure high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Value Function Iteration (VFI)\n",
    "VFI directly implements the Bellman operator as an update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def value_function_iteration(R, Q, beta, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"Solves a discrete DP model using value function iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    R : np.ndarray, shape (n_s1, n_s2, n_a)\n",
    "        Reward matrix.\n",
    "    Q : np.ndarray, shape (n_s1, n_s2, n_a, n_s2)\n",
    "        State transition probability matrix.\n",
    "    beta : float\n",
    "        Discount factor.\n",
    "    tol : float, optional\n",
    "        Tolerance for convergence.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : np.ndarray, shape (n_s1, n_s2)\n",
    "        Converged value function.\n",
    "    history : list\n",
    "        List of value functions at each iteration.\n",
    "    \"\"\"\n",
    "    V = np.zeros((n_a, n_y))\n",
    "    history = []\n",
    "    for i in range(max_iter):\n",
    "        EV = Q @ V\n",
    "        V_new = np.empty_like(V)\n",
    "        for s_i in range(n_a):\n",
    "            for s_j in range(n_y):\n",
    "                q_s = R[s_i, s_j, :] + beta * EV[s_i, s_j, :, :].sum(axis=1)\n",
    "                V_new[s_i, s_j] = np.max(q_s)\n",
    "        \n",
    "        if np.max(np.abs(V - V_new)) < tol: break\n",
    "        V = V_new\n",
    "        history.append(V.flatten())\n",
    "    return V, history\n",
    "\n",
    "note(\"VFI algorithm compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Policy Function Iteration (PFI)\n",
    "PFI alternates between evaluating a policy and improving upon it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def policy_function_iteration(R, Q, beta, tol=1e-6, max_iter=100):\n",
    "    # ... (Implementation would go here)\n",
    "    # This is more complex to implement efficiently with Numba due to matrix inversion\n",
    "    # For this course, we focus on VFI and Howard's PFI.\n",
    "    pass\n",
    "note(\"PFI is conceptually important but often less practical to implement from scratch than VFI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Howard's Policy Improvement Algorithm\n",
    "Howard's PFI is a more practical variant that avoids the costly matrix inversion of the policy evaluation step. Instead, it approximates the value of the current policy by running a fixed, small number of VFI steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def howard_policy_iteration(R, Q, beta, m=20, max_iter=200):\n",
    "    \"\"\"Solves a discrete DP model using Howard's Policy Iteration.\n",
    "    \n",
    "    This is a faster variant of PFI that approximates the policy evaluation\n",
    "    step with a fixed number of VFI steps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int, optional\n",
    "        Number of VFI steps to run in the evaluation phase.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of policy improvement iterations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    V : np.ndarray\n",
    "        Converged value function.\n",
    "    policy : np.ndarray\n",
    "        Optimal policy array.\n",
    "    \"\"\"\n",
    "    V = np.zeros((n_a, n_y))\n",
    "    policy = np.zeros((n_a, n_y), dtype=np.int32)\n",
    "    for i in range(max_iter):\n",
    "        # Policy Evaluation (m VFI steps)\n",
    "        for _ in range(m):\n",
    "            EV = Q @ V\n",
    "            V_policy = np.empty_like(V)\n",
    "            for s_i in range(n_a):\n",
    "                for s_j in range(n_y):\n",
    "                    a = policy[s_i, s_j]\n",
    "                    V_policy[s_i, s_j] = R[s_i, s_j, a] + beta * EV[s_i, s_j, a, :].sum()\n",
    "            V = V_policy\n",
    "        \n",
    "        # Policy Improvement\n",
    "        EV = Q @ V\n",
    "        policy_new = np.empty_like(policy)\n",
    "        for s_i in range(n_a):\n",
    "            for s_j in range(n_y):\n",
    "                q_s = R[s_i, s_j, :] + beta * EV[s_i, s_j, :, :].sum(axis=1)\n",
    "                policy_new[s_i, s_j] = np.argmax(q_s)\n",
    "        \n",
    "        if np.array_equal(policy, policy_new): break\n",
    "        policy = policy_new\n",
    "    return V, policy\n",
    "\n",
    "note(\"Howard's PFI algorithm compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analysis of the Solution\n",
    "Now we solve the model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Solving the SGM and Comparing Runtimes\")\n",
    "BETA = sgm_params['beta']\n",
    "print(\"Solving with VFI...\")\n",
    "%time V_star_vfi, v_history = value_function_iteration(R, Q, BETA)\n",
    "\n",
    "print(\"\\nSolving with Howard's PFI...\")\n",
    "%time V_star_hpi, policy_indices_hpi = howard_policy_iteration(R, Q, BETA)\n",
    "\n",
    "# Get the final policy from VFI for comparison\n",
    "EV = Q @ V_star_vfi\n",
    "policy_indices_vfi = np.empty((n_a, n_y), dtype=np.int32)\n",
    "for s_i in range(n_a):\n",
    "    for s_j in range(n_y):\n",
    "        q_s = R[s_i, s_j, :] + BETA * EV[s_i, s_j, :, :].sum(axis=1)\n",
    "        policy_indices_vfi[s_i, s_j] = np.argmax(q_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 The Value and Policy Functions\n",
    "- **Value Function:** $V(a,y)$ is increasing and concave in assets $a$, reflecting diminishing marginal utility.\n",
    "- **Policy Function:** $\\sigma(a,y)$ shows the optimal level of assets to hold next period. We can see the effect of **precautionary savings**: an agent saves more when their current income is lower to buffer against future bad shocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Analyzing the Solution\")\n",
    "\n",
    "# --- 1. Interactive 3D Value Function Plot ---\n",
    "import plotly.graph_objects as go\n",
    "A_grid_mesh, Y_grid_mesh = np.meshgrid(sgm_params['a_grid'], sgm_params['y_states'])\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=V_star_vfi.T, x=A_grid_mesh, y=Y_grid_mesh, colorscale='Viridis')])\n",
    "fig.update_layout(title='Interactive 3D Value Function V(a, y)',\n",
    "                  scene = dict(\n",
    "                      xaxis_title='Assets (a)',\n",
    "                      yaxis_title='Income (y)',\n",
    "                      zaxis_title='Value'),\n",
    "                  autosize=False, width=800, height=600,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "fig.show()\n",
    "\n",
    "# --- 2. 2D Policy Function Plot ---\n",
    "policy_star = sgm_params['a_grid'][policy_indices_vfi]\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(sgm_params['a_grid'], policy_star[:, 0], label=f'$\\sigma(a, y_L)$')\n",
    "ax.plot(sgm_params['a_grid'], policy_star[:, -1], label=f'$\\sigma(a, y_H)$')\n",
    "ax.plot(sgm_params['a_grid'], sgm_params['a_grid'], 'k--', label=\"45-degree line (a'=a)\")\n",
    "ax.set_title(\"Policy Functions: Next Period's Assets (a')\"); \n",
    "ax.set(xlabel='Current Assets (a)', ylabel=\"Next Period's Assets (a')\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Visualizing VFI Convergence\n",
    "The plot below provides a direct visualization of the **Contraction Mapping Theorem** in action. We start with an initial guess ($V_0=0$) and repeatedly apply the Bellman operator. Each application brings the function closer to the true value function, with the distance between successive iterates shrinking, until it converges to the unique fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Visualizing VFI Convergence\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_indices = [0, 2, 5, 10, 20, 40, 60, 100]\n",
    "for i in plot_indices:\n",
    "    if i < len(v_history):\n",
    "        V_interim = v_history[i].reshape(n_a, n_y)\n",
    "        ax.plot(A_GRID, V_interim[:, -1], label=f'Iteration {i}')\n",
    "\n",
    "ax.plot(A_GRID, V_star_vfi[:, -1], color='k', lw=2, label=f'Converged V')\n",
    "ax.set_title('Convergence of the Value Function (High Income State)')\n",
    "ax.set(xlabel='Assets (a)', ylabel='Value'); ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Simulating the Stationary Distribution\n",
    "After solving for the optimal policy, a key economic question is what the long-run distribution of assets looks like. We can find this by simulating a large panel of households over a long time horizon, where each household follows the optimal policy rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec(\"Simulating the Stationary Asset Distribution\")\n",
    "@njit\n",
    "def simulate_asset_distribution(policy_indices, P_trans, n_agents=5000, t_steps=500):\n",
    "    a_indices = np.zeros(n_agents, dtype=np.int32)\n",
    "    y_indices = np.zeros(n_agents, dtype=np.int32)\n",
    "    \n",
    "    for t in range(t_steps):\n",
    "        # Update assets based on policy\n",
    "        new_a_indices = np.empty_like(a_indices)\n",
    "        for i in range(n_agents):\n",
    "            new_a_indices[i] = policy_indices[a_indices[i], y_indices[i]]\n",
    "        a_indices = new_a_indices\n",
    "        \n",
    "        # Update income based on Markov chain\n",
    "        new_y_indices = np.empty_like(y_indices)\n",
    "        for i in range(n_agents):\n",
    "            new_y_indices[i] = np.searchsorted(np.cumsum(P_trans[y_indices[i], :]), np.random.rand())\n",
    "        y_indices = new_y_indices\n",
    "        \n",
    "    return A_GRID[a_indices]\n",
    "\n",
    "final_assets = simulate_asset_distribution(policy_indices_vfi, P_TRANS)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(final_assets, bins=50, density=True)\n",
    "plt.title('Stationary Distribution of Assets')\n",
    "plt.xlabel('Asset Holdings'); plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Chapter Summary\n",
    "- **The Bellman Equation** is the cornerstone of dynamic programming, breaking down complex sequential problems into simpler recursive steps.\n",
    "- **The Contraction Mapping Theorem** provides the theoretical guarantee that for a discounted dynamic programming problem, a unique value function exists and that Value Function Iteration will converge to it.\n",
    "- **Solution Algorithms:** VFI is simple and robust. Policy Function Iteration (especially Howard's variant) is often much faster as it takes more intelligent steps, but can be more complex to implement.\n",
    "- **Discretization:** To solve continuous-state models, we must first discretize the state space. Tauchen's method is a standard way to approximate a continuous AR(1) process with a discrete Markov chain.\n",
    "- **Stationary Distribution:** By simulating the model with the optimal policy, we can analyze the long-run aggregate implications of individual behavior, such as the emergence of wealth inequality from uninsurable income risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Exercises\n",
    "\n",
    "1.  **The Role of Patience:** Re-solve the stochastic growth model with a lower discount factor (`beta = 0.90`), representing a more impatient agent. Plot the new policy function and the new stationary asset distribution. How does impatience affect savings and long-run wealth? Provide an economic intuition.\n",
    "\n",
    "2.  **The Role of Risk Aversion:** Re-solve the model with a higher coefficient of relative risk aversion (`gamma = 4.0`). How does this affect the policy function and the amount of precautionary savings? How does the stationary distribution change?\n",
    "\n",
    "3.  **The Role of Income Persistence:** How does the stationary wealth distribution change if income shocks are less persistent? Re-solve and re-simulate the model using a lower `rho_y` (e.g., 0.8) in the Tauchen method. Is wealth inequality higher or lower? Why?\n",
    "\n",
    "4.  **Certainty Equivalence:** Consider a version of the model with no income risk, where income is always the unconditional mean income. Solve for the optimal policy function in this deterministic case. How does it compare to the policy functions from the stochastic model? The difference in savings between the deterministic and stochastic models for an agent in the low-income state is a measure of their precautionary savings.\n",
    "\n",
    "5.  **Alternative Utility:** Re-solve the model using an exponential utility function, $u(c) = -e^{-\\gamma c}$, which exhibits constant absolute risk aversion (CARA) instead of constant relative risk aversion (CRRA). How do the resulting policy functions differ in shape from the CRRA case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}